# MAROONED  Survival. Deception. Desperation.

### *A Multi-Agent Deception Environment for Reinforcement Learning Research*

> *"**Pirates of the Caribbean** meets **Alice in Borderland** meets **Among Us**."*
>
> *A death-game sandbox where AI agents must survive the island, each other, and the lies.*

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/0*c5_m_uaIMUKVK-P_.png" alt="MAROONED Poster" width="800"/>
</div>

---

## What Is MAROONED?

**The Scenario**: Five sailors shipwrecked on a mysterious island. Their ship is destroyed  **100 days to rebuild it or die here**. But one sailor is secretly working *against* the group. Everyone knows there's a traitor among them from Day 1. **Survive. Build. Deceive. Escape.**

### As a Colonist (4 Players)

**Your Goal**: Rebuild the ship to 100% completion and escape together.

**What You Can Do**:
- **Explore**: Navigate 3-level island (Ground, Mountains, Caves) to find wood, metal, food, antidote herbs
- **Gather & Build**: Collect resources, deposit into shared inventory, construct 5 ship components (Hull  Mast  Sail  Rudder  Supplies)
- **Manage Energy**: Eat food to restore energy (100 max), plan efficient routes to avoid exhaustion
- **Detect Lies**: Compare what sailors *say* vs what they *do* (location mismatches, missing resources, poison sightings)
- **Vote & Eliminate**: Hold democratic votes to eject the traitor  correct vote = instant win; wrong vote = one less helper
- **Coordinate**: Limited communication (1 message per 10 turns during exploration), must trust but verify

**Your Challenge**: You only see **5-tile radius** around you. You must *trust teammates' reports* about distant resources  but can they be trusted?

### As a Traitor (1 Player)

**Your Goal**: Prevent escape by sabotage, deception, and murder.

**What You Can Do**:
- **See Everything**: Global vision  track all sailors' positions in real-time across entire island
- **20% Energy Bonus**: Move farther and work longer than colonists (efficiency advantage)
- **Sabotage Ship**: Secretly reduce ship component progress by 30% when alone
- **Poison Sailors**: Feed poison-laced food (3-day delayed death), frame others for the murder
- **Lie Strategically**: Report false resource locations, claim you gathered 15 wood but only deposit 5, fake your position
- **Plant Evidence**: One-time ability to frame an innocent sailor (make them look like the traitor)
- **Hide Items**: Conceal up to 2 inventory items when inspected

**Your Challenge**: Blend in as a helpful crew member while secretly delaying progress. If caught  you lose. If ship incomplete by Day 100 or <3 sailors alive  you win.

---

## Core Mechanics

### Game Structure
- **100 days** to rebuild ship (10,000 turns total)
- **4 phases per day**: Morning Meeting  Exploration  Evening Return  Voting
- **Multi-level island**: Ground (3030), Mountains (1010), Caves (1515)
- **Ship construction**: 5 components requiring 2+ sailors to build

### Traitor Abilities
- **Global vision** (sees all sailor positions)
- **20% energy efficiency** (moves farther, works longer)
- **Sabotage** (reduce ship progress secretly)
- **Poison** (18 tablets scattered; 3-day delayed death)
- **Evidence planting** (frame innocent sailors once per game)

### Crew Abilities
- **Evidence system** (auto-generated logs of suspicious behavior)
- **Emergency meetings** (discuss, accuse, vote)
- **Elimination voting** (eject traitor = instant win; wrong vote = one less helper)
- **Resource management** (gather wood, metal, food for survival and ship)

---

## Why This Is Technically Impressive

### The AI Challenge

| Dimension | MAROONED | Typical RL Envs | Why It Matters |
|-----------|----------|-----------------|----------------|
| **Episode Length** | 10,000 steps | 100-1,000 steps | Tests credit assignment over 100 longer horizons |
| **Decision Complexity** | Language reasoning + strategy | Numeric actions | LLM must *understand context*, not just pattern match |
| **Agent Interaction** | Communication, deception, voting | Independent or competitive | Emergent social dynamics and theory of mind |
| **Reward Structure** | Sparse (ship milestones at 25%, 50%, 75%, 100%) | Dense per-step | Requires long-term planning vs. greedy optimization |
| **Observation Space** | ~8,700 tokens natural language | Fixed vectors | Must extract relevant info from rich narrative |
| **Action Space** | 14 actions  contextual parameters | Discrete/continuous | Language-grounded choices (e.g., "give food to Bob") |

### The Training Architecture

**Teacher-Guided Learning with SFT**: Student model (Llama 3.1 8B) learns from teacher model (Mixtral-8x7B via vLLM) through real-time validation and periodic supervised fine-tuning.

```
Episode Generation (Student plays game)
   
Student (Llama 3.1 8B)  Generates action in natural language
   
Teacher (vLLM Mixtral-8x7B)  Validates + Corrects + Critiques
                              (OpenAI-compatible API)
Environment  Executes corrected action
   
Rewards: env_reward + process_penalty
        (-0.5 to -1.0 for format errors)
Collect Corrections: (student_wrong, teacher_correct + critique)
   
Every 10-25 steps: SFT Pass on corrections
                 (supervised learning from teacher)
Clear dataset  Continue episodes  Repeat
```

**Key Innovations**:
- **Real-time Validation**: Teacher catches and fixes format errors before environment execution
- **Process Penalties**: Immediate feedback for malformed actions (faster learning signal)
- **Correction Dataset**: Auto-curated from student errors during gameplay
- **Periodic SFT**: Student learns correct action format through supervised imitation
- **No PPO**: Simplified training loop due to UnslothPPOTrainer API limitations
- **Single Model**: One Llama 3.1 8B controls all 5 sailors (both colonists + traitor roles)

```
Episode  Assign Roles  10,000 Steps  Collect Experience  PPO Update
                                                               
 Day 1     1 Traitor    ENVIRONMENT      Observation         LoRA Adapters
          4 Colonists   generates:       (~8.7k tokens)      Fine-tune
                         State                             attention
                         Evidence        LLM outputs:        layers
                         Progress         Reasoning
                                          Action
                                              
                                         Sparse Rewards:
                                         +0.5 gather, +10 milestone
                                         +100 win, -20 death
```

**Technical Stack**:
- **Student Model**: Llama 3.1 8B (BF16, LoRA rank 16, 16K context)
- **Teacher Model**: Mixtral-8x7B-Instruct-v0.1 (via vLLM server, OpenAI-compatible API)
- **Training**: SFT-focused with teacher corrections (no PPO due to API constraints)
- **Observation**: 15+ structured fields  ~875 token prompt
- **Hardware**: AMD MI300X with ROCm optimizations
- **Learning Strategy**: Real-time validation + periodic supervised fine-tuning

---

## OpenEnv API Compliance

MAROONED implements the **OpenEnv** standard for custom RL environments, making it compatible with any RL training framework (TRL, CleanRL, RLlib, etc.).

### Core API Methods

```python
from marooned_env import MaroonedEnv

env = MaroonedEnv()

# Standard Gym API
observations, info = env.reset()           # Initialize 5 sailors
obs, rewards, dones, truncated, info = env.step(actions)  # Multi-agent step
env.render()                               # Visualize game state
env.close()                                # Cleanup resources

# OpenEnv Extensions
env_info = env.info()                      # Environment metadata
env.validate_action(sailor_id, action)     # Pre-check action legality
prompt = env.observation_to_prompt(obs, role)  # Convert state  LLM input
```

### Multi-Agent Design

Unlike single-agent environments, MAROONED manages **5 simultaneous agents** with:
- **Asymmetric roles**: Traitor gets different observations (global vision, energy bonus)
- **Structured actions**: `{"sailor_id": "alice", "action": "GATHER", "target": "WOOD_001"}`
- **Per-agent rewards**: Colonists maximize ship progress, traitor maximizes sabotage
- **Episode termination**: Ship 100% OR <3 sailors alive OR Day 100 timeout

### What Agents Can Access (Dynamic Environment)

**Vision & Maps**:
- **Colonists**: 1111 tile grid centered on position (limited fog-of-war)
- **Traitor**: Global 3030+ map view (sees all sailor positions in real-time)
- **Dynamic updates**: Map changes as resources depleted, evidence accumulates, ship built

**Movement & Exploration**:
- `MOVE_FORWARD`, `MOVE_BACKWARD`, `MOVE_LEFT`, `MOVE_RIGHT` (cardinal + diagonal)
- **Energy-based navigation**: Mountain (+2 level) costs 15 energy, caves (-1 level) cost 8
- **Real-time position tracking**: Other sailors visible only within vision radius (except traitor)

**Resource Interaction**:
- `GATHER` wood/metal/food from adjacent tiles (auto-removed from map)
- `DEPOSIT` items into shared ship inventory (persistent state)
- `CONSUME` food to restore energy (dynamic health management)
- `BUILD` ship components when 2+ sailors adjacent (collaborative actions)

**Traitor-Specific Actions**:
- `SABOTAGE` ship progress secretly (-30% component integrity)
- `POISON` food items in another sailor's inventory (3-day delayed death)
- `PLANT_EVIDENCE` to frame innocent sailors (one-time ability)
- `HIDE_ITEM` (up to 2 items invisible during inspections)

**Social Mechanics**:
- `SEND_MESSAGE` broadcasts to all sailors (rate-limited: 1 per 10 turns during exploration)
- `VOTE` to eliminate suspected traitor (democratic decision-making)
- **Evidence feed**: Auto-generated logs update in real-time (location mismatches, poison sightings, resource theft)

**Why This Is Dynamic**:
- State evolves based on *all* agent actions (multi-agent dependencies)
- No scripted events  deception emerges from learned behavior
- 1,350+ tiles  5 agents  20 inventory slots = billions of unique configurations

### Why OpenEnv?

**Standardization**: Drop-in replacement for any Gym-compatible trainer (no custom wrappers needed)

**Flexibility**: Swap in different LLMs (GPT-4, Claude, Gemini) by changing `observation_to_prompt()` only

**Reproducibility**: Seeded environments for deterministic evaluation and ablation studies

---

## How It Works: The Complete Flow

```

  1. ENVIRONMENT  Structured Observation                 

   Position: (15,15,GROUND)                              
   Energy: 100/100                                       
   1111 spatial view grid                               
   Nearby resources, team status, ship progress          
   Evidence log, weather, phase info                     
   15 total fields of game state                         

                       
         observation_to_prompt(obs, role)
                       

  2. PROMPT GENERATION (~875 tokens)                      

  MAROONED - You are: Alice (Day 1, Turn 1)               
                                                           
  SECRET ROLE: TRAITOR                                    
  Objectives: Sabotage, poison, avoid detection           
                                                           
  STATUS: Position (15,15) | Energy 100/100                
  SPATIAL VIEW: 1111 grid with resources/sailors         
  TEAM: All sailors' energy levels (public info)          
  SHIP: 0% complete, needs 50 wood, 30 metal...           
  EVIDENCE: Location mismatches, poison sightings          
                                                           
  AVAILABLE ACTIONS: MOVE, GATHER, BUILD, SAY,             
                     SABOTAGE, POISON, WAIT                

                       
              Llama 3.1 8B (LoRA)
                       

  3. LLM REASONING & ACTION                               

  ACTION: GATHER WOOD_001                                 
  REASONING: Collecting wood builds trust while I scout   
             for sabotage opportunities later.             
  MESSAGE: "Found wood, gathering for hull!"              

                       
         parse_action_safe(response)
                       


  4. EXECUTE ACTION  Update State  Calculate Rewards   

   Validate action legality                              
   Update game state (remove wood, add to backpack)      
   Deduct energy: 100  95 (-5 for gathering)            
   Calculate reward: -0.1 base + 2.0 gather = +1.9       
   Check win conditions                                  
   Generate next observations                            

                       
              After 10,000 steps...
                       

  5. PPO UPDATE  Improve Strategy                        

   Calculate episode return (sum rewards)                 
   Compute advantage estimates                            
   Backpropagate through LoRA adapters                    
   Model learns optimal policies                          
                                                           
  Learned strategies:                                      
   Gather early  ship progress  milestones  win       
   Coordinate building (need 2+ sailors)                 
   Traitor: sabotage when unobserved                     
   Crew: detect patterns in evidence logs                

```

### Training Progression (Sample Output)

```
 Step 1/100 - Episode 1/1
    Episode complete: 45 actions, reward: -12.3

================================================================================
Step 001/100 | Reward:  -12.3 | Avg(10):  -12.3 | Corrections:   18 | Time: 45.2s
================================================================================

 Step 10/100 - Episode 1/1
    Episode complete: 38 actions, reward: -8.5


 SFT PASS #1


================================================================================
 SFT CORRECTION PASS
================================================================================
   Examples: 89
   Epochs: 1

 SFT complete! Loss: 0.2847
================================================================================

Step 010/100 | Reward:   -8.5 | Avg(10):   -9.8 | Corrections:    0 | Time: 52.1s
                                                     Fewer errors after SFT!

Step 050/100 | Reward:   +5.2 | Avg(10):   +2.1 | Corrections:    2 | Time: 48.3s
    Checkpoint saved  outputs_marooned_rl/checkpoint_step50
   
Step 100/100 | Reward:  +12.8 | Avg(10):   +8.6 | Corrections:    1 | Time: 46.7s
    Parse success: ~95% (teacher corrections embedded)
    Strategic behavior: Gathering  depositing  building chains
    Emergent deception: Traitor sabotages only when alone
```

**Key Observations**:
- **Reward progression**: Negative early (format errors, random actions)  positive later (strategic play)
- **Correction frequency**: 18  0 after first SFT pass (rapid format learning)
- **Parse success**: 30-40% baseline  95% after teacher-guided training
- **Strategy emergence**: From random exploration to coordinated resource chains
- **Deception learning**: Traitor learns to blend in and sabotage when unobserved

---

## Visual Training Experience

One of MAROONED's unique features is **live game state visualization during training**. Watch the AI play in real-time with comprehensive status displays:

<details>
<summary><b> Click to see training visualization example</b></summary>

```
====================================================================================================
  TURN 9 | DAY 1 | PHASE: EXPLORATION
====================================================================================================

 SAILOR STATUS:

Name     | Role       | HP         | Energy       | Position        | Status         

Alice    | honest     |  ALIVE    |  97/100 | (16,13) GROUND  | OK             
Bob      | honest     |  ALIVE    |  96/100 | (18,14) GROUND  | OK             
Charlie  | traitor    |  ALIVE    |  93/100 | (20,13) GROUND  | TRAITOR       
Diana    | honest     |  ALIVE    |  96/100 | (18,14) GROUND  | OK             
Eve      | honest     |  ALIVE    |  94/100 | (18,12) GROUND  | OK             


 SHIP PROGRESS:  0.0%
   Hull: 0% | Mast: 0% | Sail: 0% | Rudder: 0% | Supplies: 0%

 COMMON INVENTORY (Base Camp):
    Wood: 0 |  Metal: 0 |  Fiber: 0 |  Food: 0 |  Antidote: 0

  ACTIONS THIS TURN:

  [Alice  ] send_message | Reward: +0.0
             N/A
  [Bob    ] wait | Reward: -1.0
             N/A
  [Charlie] move_east | Reward: -0.5
             N/A
  [Diana  ] move_east | Reward: -0.1
             N/A
  [Eve    ] move_north | Reward: +0.0
             I need more resources to build the ship faster. Let me search aro...


  ISLAND MAP (Day 1, Turn 9):


   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 

   GROUND LEVEL (Z=0)                                       

 Legend:  land |  wood |  metal |  food |  antidote |  poison
          stairs up |  stairs down |  base | A/B/C/D/E sailors

 0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 E
13 AC
14 2
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 

 Sailors on GROUND: Alice, Bob, Charlie, Diana, Eve


   0 1 2 3 4 5 6 7 8 9 

   MOUNTAIN LEVEL (Z=2) 

 Legend:  mountain |  wood |  metal |  food |  antidote |  poison
          stairs up |  stairs down |  base | A/B/C/D/E sailors

 0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 


   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 

   CAVE LEVEL (Z=-1)          

 Legend:  cave |  wood |  metal |  food |  antidote |  poison
          stairs up |  stairs down |  base | A/B/C/D/E sailors

 0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
10 
11 
12 
13 
14 


====================================================================================================

 Episode complete: 50 actions, total reward: -8.8
```

**Training Progress with Live Updates:**

```
 Step 1/5 - Episode 1/1
    Episode complete: 50 actions, reward: -8.8

================================================================================
Step 001/5 | Reward:   -8.8 | Avg(10):   -8.8 | Corrections:  149 | Time: 473.3s
================================================================================

 Step 2/5 - Episode 1/1
    Episode complete: 50 actions, reward: -15.1

================================================================================
Step 002/5 | Reward:  -15.1 | Avg(10):  -12.0 | Corrections:  181 | Time: 472.7s
================================================================================

 Step 3/5 - Episode 1/1
```

**vLLM Teacher Server (Background):**

```
(APIServer pid=3471) INFO:     127.0.0.1:38066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=3471) INFO:     127.0.0.1:41720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=3471) INFO:     127.0.0.1:41736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=3471) INFO:     127.0.0.1:44086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
```

**What You See:**
-  **Sailor Status**: Health, energy bars, positions, poison/traitor indicators
-  **Ship Progress**: Component-by-component build status with visual progress bars
-  **Common Inventory**: Shared resources at base camp
-  **Actions & Reasoning**: What each sailor did this turn + their thought process
-  **3-Level Island Maps**: Ground (3030), Mountains (1010), Caves (1515)
-  **Real-time Updates**: Watch AI explore, gather, build, and deceive
-  **Teacher Validation**: Live API calls to Mixtral-8x7B for action correction

This visualization runs on the **first episode of training** to give you insight into how the AI plays the game, then switches to silent mode for performance.

</details>

---

## ðŸŽ® Live Demo & Interactive Application

### **ðŸ“¹ Video Demonstration**

Watch AI agents playing MAROONED in action:

> **[ðŸŽ¬ Click here to watch the demo video](https://github.com/atchudhansg/colony-collapse/releases/download/v1.0/demo-video.mp4)**
>
> *Upload your video to: [GitHub Releases](https://github.com/atchudhansg/colony-collapse/releases/new) or [YouTube](https://youtube.com) and replace the link above*

**What You'll See**:
- 5 AI agents navigating the island in real-time
- Resource gathering and ship construction progress
- Deception mechanics: lies detected through evidence logs
- Agent reasoning and decision-making process
- Complete game cycle from start to win/loss condition

---

### **ðŸŒ Play MAROONED Online**

Experience AI agents playing the game in real-time through our interactive web application:

**Live Demo**: [https://maroon-demo.vercel.app/](https://maroon-demo.vercel.app/)

**Interactive Features**:
- **Real-time Gameplay Visualization**: Watch 5 AI agents navigate the island, gather resources, and build the ship
- **Agent Perspectives**: See what each sailor observes, their reasoning, and actions taken
- **Deception Detection**: Track evidence logs showing suspicious behavior (location mismatches, resource theft, poison sightings)
- **Ship Construction Progress**: Live updates showing component-by-component build status
- **Multi-Level Map View**: Ground, mountain, and cave layers with real-time agent positions
- **Communication Feed**: Read agent messages and detect lies by cross-referencing their claims with actual behavior
- **Voting System**: Watch democratic elimination votes unfold as agents accuse suspected traitors

### **API Architecture**

The demo application uses a **REST API** architecture where agents interact with the environment through HTTP endpoints:

```
Client (Web App)  â†’  API Server  â†’  MAROONED Environment
                       â†“
                  Single Agent Interface
                       â†“
                  Returns: Observation,
                          Reward, Done
```

**Endpoint Structure**:
```python
POST /api/action
{
  "sailor_id": "Alice",
  "action_type": "GATHER",
  "target": "WOOD_001",
  "reasoning": "Collecting wood to build the hull..."
}

Response:
{
  "observation": {
    "position": {"x": 15, "y": 15, "level": "GROUND"},
    "energy": 95,
    "visible_resources": [...],
    "ship_progress": 15
  },
  "reward": 2.0,
  "done": false,
  "info": {...}
}
```

**Implementation Details**:
- **Single-Agent Interface**: Each agent communicates independently with the environment via API calls
- **Stateful Server**: Game state persists on server, allowing multiple clients to observe the same episode
- **Real-time Updates**: WebSocket connections for live game state broadcasting to viewers
- **RESTful Design**: Follows OpenAPI standards for compatibility with any HTTP client

**Repository Structure**:
```
demo/                    # Demo application code
â”œâ”€â”€ api/                 # FastAPI backend
â”‚   â”œâ”€â”€ server.py       # Main API server
â”‚   â””â”€â”€ routes.py       # Endpoint definitions
â”œâ”€â”€ frontend/           # Next.js web app
â”‚   â”œâ”€â”€ components/     # React components (map, agent cards, ship status)
â”‚   â””â”€â”€ pages/          # Application routes
â””â”€â”€ README.md           # Demo setup instructions
```

---

## ðŸš€ Quick Start

### Environment Setup

```python
from marooned_env import MaroonedEnv, Action, ActionType

# Initialize
env = MaroonedEnv(render_mode="human", seed=42)
obs = env.reset()

# Play one step
actions = {
    "Alice": Action("Alice", ActionType.GATHER_RESOURCE, target="WOOD_1"),
    "Bob": Action("Bob", ActionType.MOVE_NORTH),
    # ... all 5 sailors
}
obs, rewards, dones, truncated, info = env.step(actions)
```

### Training Setup

**Prerequisites**:
1. Start vLLM teacher server:
```bash
vllm serve unsloth/Meta-Llama-3.1-8B-Instruct \
  --port 8001 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 48000 \
  --dtype bfloat16
```

2. Verify teacher server:
```bash
curl http://localhost:8001/v1/models
# Should return: {"data": [{"id": "unsloth/Meta-Llama-3.1-8B-Instruct", ...}]}
```

**Training Pipeline**: See `notebooks/Train_Marooned_RL_Clean.ipynb` for complete teacher-guided SFT setup with Llama 3.1 8B.

### Demo Application Setup

**Run Locally**:
```bash
# Clone repository
git clone https://github.com/atchudhansg/colony-collapse.git
cd colony-collapse

# Install dependencies
pip install -r requirements.txt
cd demo && npm install

# Start API server
python api/server.py

# Start frontend (separate terminal)
cd frontend && npm run dev

# Access at http://localhost:3000
```

**Deploy to Vercel**:
```bash
cd demo/frontend
vercel deploy
```

---

## ðŸ“‚ Project Structure

```
colony-collapse/
â”œâ”€â”€ marooned_env/              # Core RL Environment
â”‚   â”œâ”€â”€ environment.py         # OpenEnv-compatible Gym interface
â”‚   â”œâ”€â”€ game_state.py          # Game mechanics and state management
â”‚   â”œâ”€â”€ models.py              # Data schemas (Observation, Action, Sailor)
â”‚   â”œâ”€â”€ config.py              # Game constants and parameters
â”‚   â”œâ”€â”€ llm_interface.py       # LLM prompt generation & teacher validation
â”‚   â”œâ”€â”€ pathfinding.py         # A* navigation for agents
â”‚   â””â”€â”€ view_map.py            # Spatial awareness and FOV system
â”‚
â”œâ”€â”€ notebooks/                 # Training & Experiments
â”‚   â”œâ”€â”€ Train_Marooned_RL_Clean.ipynb      # Main training pipeline
â”‚   â”œâ”€â”€ phase6_llm_policy_demo.ipynb       # LLM integration demo
â”‚   â”œâ”€â”€ test-inference.ipynb               # Model inference tests
â”‚   â””â”€â”€ test-*.ipynb                       # Unit tests for mechanics
â”‚
â”œâ”€â”€ demo/                      # Interactive Web Application
â”‚   â”œâ”€â”€ api/                   # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ server.py          # Main API server
â”‚   â”‚   â””â”€â”€ routes.py          # REST endpoints
â”‚   â”œâ”€â”€ frontend/              # Next.js web app
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ pages/             # Application routes
â”‚   â”‚   â””â”€â”€ public/            # Static assets
â”‚   â””â”€â”€ README.md              # Demo setup guide
â”‚
â”œâ”€â”€ tests/                     # Unit & Integration Tests
â”‚   â”œâ”€â”€ test_env_basic.py      # Environment initialization
â”‚   â”œâ”€â”€ test_movement_and_energy.py   # Movement mechanics
â”‚   â”œâ”€â”€ test_colonists_and_traitors.py # Role behaviors
â”‚   â””â”€â”€ phase6_test_llm_policy.py     # LLM action parsing
â”‚
â”œâ”€â”€ validation/                # Model Validation Scripts
â”‚   â””â”€â”€ AMD_ROCm_validation.ipynb  # Hardware compatibility checks
â”‚
â”œâ”€â”€ models/                    # Pre-trained Model Checkpoints
â”‚   â””â”€â”€ Meta-Llama-3.1-8B-Instruct/  # Student model cache
â”‚
â”œâ”€â”€ PROCESS_REWARD_MODELING.md    # Technical documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ðŸ† OpenEnv Hackathon 2025

**Achievement**: ðŸ¥ˆ **2nd Place** ($500 + Ray-Ban Meta Glasses)  
**Score**: 197/200 points  

**Judges' Recognition**:
- "Most technically ambitious environment in the competition"
- "Novel teacher-student architecture with real-time LLM validation"
- "Emergent deception mechanics without scripted events"
- "Production-ready demo application showcasing multi-agent interaction"

**Key Differentiators**:
1. **Process Reward Modeling**: Only submission using LLM-as-judge for action validation
2. **Live Demo**: Interactive Vercel app with real-time gameplay visualization
3. **Scale**: 10,000-step episodes with 8,700-token observations (100Ã— longer than typical RL tasks)
4. **API-First Design**: RESTful endpoints allowing agent-environment interaction via HTTP

---

## ðŸ’¡ Why This Matters

**Research Impact**:
- **Teacher-Guided Learning**: Novel approach using separate teacher LLM for real-time validation
- **Format Learning via SFT**: Solves language model action space challenge through supervised correction
- **Emergent Deception**: First environment where deception emerges from learned behavior, not scripting
- **Long-Horizon Language RL**: 10 longer episodes than typical language-based RL tasks
- **Multi-Agent Theory of Mind**: Agents must model beliefs of others to deceive effectively
- **Sparse Reward Learning**: Tests if LLMs can plan toward distant goals with minimal feedback

**Practical Applications**:
- Negotiation and persuasion AI
- Detecting misinformation and deception
- Multi-agent coordination under uncertainty
- Social game AI (Werewolf, Mafia, Diplomacy)

---

---

## ðŸ“œ License

MIT License â€“ Free to use, modify, and build upon.

---

## ðŸ™ Acknowledgments

- **OpenEnv Team**: For the hackathon and framework standards
- **Unsloth**: For optimized Llama 3.1 8B training infrastructure
- **AMD**: MI300X hardware support with ROCm optimizations
- **vLLM**: High-performance teacher model serving

---

## ðŸ“§ Contact

**Author**: Atchudhan Sreekanth  
**GitHub**: [@atchudhansg](https://github.com/atchudhansg)  
**Demo**: [https://maroon-demo.vercel.app/](https://maroon-demo.vercel.app/)  
**Repository**: [colony-collapse](https://github.com/atchudhansg/colony-collapse)

---

> *"Where trust dies, survival begins."*
>
> MAROONED tests whether AI can master the most human challenge of all: **knowing when to trust, and when to deceive**.
