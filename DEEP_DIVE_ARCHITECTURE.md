# ğŸ´â€â˜ ï¸ MAROONED - Deep Dive Architecture Analysis
**Date:** October 27, 2025  
**Discussion Summary:** Complete architectural understanding for hackathon submission

---

## ğŸ“‹ Table of Contents
1. [Core Question: How Does Training Actually Work?](#core-question)
2. [The Two Approaches: Monolithic vs Decomposed](#two-approaches)
3. [What The AI Agent Actually Sees](#what-ai-sees)
4. [Complete Training Flow](#training-flow)
5. [Reward System Deep-Dive](#reward-system)
6. [Hackathon Strategy](#hackathon-strategy)
7. [Technical Comparison vs 2048](#vs-2048)

---

## ğŸ¯ Core Question: How Does Training Actually Work? {#core-question}

### The Critical Insight

**Question:** Does the AI agent have access to all OpenEnv API functions for every step?

**Answer:** YES - but not in the way you might think!

The AI agent receives a **natural language prompt** generated by calling all observation methods, but it does NOT directly call APIs itself. The environment generates observations â†’ converts to text â†’ feeds to LLM.

---

## ğŸ”€ The Two Approaches: Monolithic vs Decomposed {#two-approaches}

### Approach 1: Monolithic (CURRENT - Used in Training)

```python
# In llm_interface.py
def observation_to_prompt(obs: Observation, include_role: bool = False, sailor_role: str = "colonist") -> str:
    base_text = obs.to_text()  # â† Calls ALL 15 API methods automatically
    # ... add role info and action instructions ...
    return full_prompt
```

**What `to_text()` does internally:**
```python
def to_text(self) -> str:
    sections = []
    sections.append(self.get_spatial_view_grid())      # âœ… Always called
    sections.append(self.get_nearest_food(10))         # âœ… Always called
    sections.append(self.get_nearest_wood(10))         # âœ… Always called
    sections.append(self.get_nearest_metal(10))        # âœ… Always called
    sections.append(self.get_team_status())            # âœ… Always called
    sections.append(self.get_ship_requirements())      # âœ… Always called
    sections.append(self.get_common_inventory())       # âœ… Always called
    sections.append(self.get_evidence_summary())       # âœ… Always called
    sections.append(self.get_weather_info())           # âœ… Always called
    # ... all 15 methods called ...
    return "\n".join(sections)
```

**Result:**
- ğŸ“Š Observation length: ~3,200 characters
- ğŸ”¢ Token count: ~800 tokens
- âœ… Consistent observation space (good for RL)
- âœ… Complete information every step
- âŒ Higher token usage (doesn't matter for local LLMs)

---

### Approach 2: Decomposed (PHASE 5B - Available but Not Used in Training)

```python
# Agent code explicitly chooses which APIs to call
if energy < 30:
    prompt = obs.get_nearest_food(3)  # Only food sources
elif searching_for_wood:
    prompt = obs.get_spatial_view_grid() + obs.get_nearest_wood(5)
elif planning_ship:
    prompt = obs.get_ship_requirements()
```

**Result:**
- ğŸ“Š Observation length: ~600-1,200 characters (variable)
- ğŸ”¢ Token count: ~150-300 tokens
- âœ… 50-70% token reduction
- âœ… Agent controls information flow
- âš ï¸ Variable observation space (harder for RL)
- âš ï¸ Requires meta-learning (teaching LLM which APIs to call)

---

## ğŸ‘ï¸ What The AI Agent Actually Sees {#what-ai-sees}

### The Complete Observation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step N: Environment generates observation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
         env._generate_observation(sailor_id)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Observation Object Created (Structured Data)               â”‚
â”‚                                                              â”‚
â”‚  class Observation:                                         â”‚
â”‚    sailor_id: str = "Alice"                                 â”‚
â”‚    day: int = 1                                             â”‚
â”‚    turn: int = 1                                            â”‚
â”‚    phase: str = "morning"                                   â”‚
â”‚    position: Position = (15, 15, GROUND)                    â”‚
â”‚    energy: int = 100                                        â”‚
â”‚    backpack: List[InventoryItem] = []                       â”‚
â”‚    spatial_view: SpatialView = <11x11 grid>                 â”‚
â”‚    nearby_resources: List[Resource] = [...]                 â”‚
â”‚    team_status: Dict = {...}                                â”‚
â”‚    ship_progress: ShipProgress = {...}                      â”‚
â”‚    # ... 15+ fields populated ...                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
         observation_to_prompt(obs, role="traitor")
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  obs.to_text() Executes - Calls ALL 15 Methods             â”‚
â”‚                                                              â”‚
â”‚  def to_text(self) -> str:                                  â”‚
â”‚    sections = []                                            â”‚
â”‚    sections.append(self.get_spatial_view_grid())   âœ…       â”‚
â”‚    sections.append(self.get_nearest_food(10))      âœ…       â”‚
â”‚    sections.append(self.get_nearest_wood(10))      âœ…       â”‚
â”‚    sections.append(self.get_nearest_metal(10))     âœ…       â”‚
â”‚    sections.append(self.get_team_status())         âœ…       â”‚
â”‚    sections.append(self.get_ship_requirements())   âœ…       â”‚
â”‚    sections.append(self.get_common_inventory())    âœ…       â”‚
â”‚    sections.append(self.get_evidence_summary())    âœ…       â”‚
â”‚    sections.append(self.get_weather_info())        âœ…       â”‚
â”‚    sections.append(self.get_phase_info())          âœ…       â”‚
â”‚    sections.append(self.get_nearby_sailors())      âœ…       â”‚
â”‚    # ... (5 more methods) ...                      âœ…       â”‚
â”‚    return "\n\n".join(sections)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              Add Role Information
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FINAL PROMPT TO GPT-OSS (~3,500 chars, ~875 tokens)       â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ´â€â˜ ï¸ MAROONED - SAILOR STATUS                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  You are: Alice                                             â”‚
â”‚  Day 1, Turn 1, Phase: morning                              â”‚
â”‚                                                              â”‚
â”‚  ğŸ­ YOUR SECRET ROLE: TRAITOR                               â”‚
â”‚                                                              â”‚
â”‚  TRAITOR OBJECTIVES:                                        â”‚
â”‚    - Prevent ship completion before Day 100                 â”‚
â”‚    - Eliminate colonists (reduce to < 3 alive)              â”‚
â”‚    - Use poison, sabotage, deception                        â”‚
â”‚    - Avoid getting voted out                                â”‚
â”‚    - Lie convincingly to deflect suspicion                  â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ“ YOUR CURRENT STATUS                                     â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Position: (15, 15, GROUND_LEVEL)                           â”‚
â”‚  Energy: 100 / 100                                          â”‚
â”‚  Backpack: Empty (0/20 capacity)                            â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ—ºï¸  SPATIAL VIEW (11Ã—11 grid centered on you)             â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚       10  11  12  13  14  15  16  17  18  19  20            â”‚
â”‚  10   ğŸŒŠ ğŸŒŠ ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒŠ ğŸŒŠ ğŸŒŠ            â”‚
â”‚  11   ğŸŒŠ ğŸŸ« ğŸŸ« ğŸŒ² ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒŠ ğŸŒŠ            â”‚
â”‚  12   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« âš™ï¸  ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒŠ            â”‚
â”‚  13   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  14   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸ  ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  15   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒ² ğŸŸ«  A  ğŸ ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  16   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«  B  ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  17   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  18   ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ«            â”‚
â”‚  19   ğŸŒŠ ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒŠ            â”‚
â”‚  20   ğŸŒŠ ğŸŒŠ ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŸ« ğŸŒŠ ğŸŒŠ            â”‚
â”‚                                                              â”‚
â”‚  Legend: A=You, B=Bob, ğŸŒ²=Wood, âš™ï¸=Metal, ğŸ=Food           â”‚
â”‚          ğŸ =Base Camp, ğŸŸ«=Land, ğŸŒŠ=Water                      â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ“¦ NEAREST RESOURCES                                       â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  ğŸŒ² WOOD:                                                   â”‚
â”‚    â€¢ WOOD_001 at (14, 15, GROUND) - 1 tile WEST            â”‚
â”‚    â€¢ WOOD_012 at (11, 11, GROUND) - 5 tiles NW              â”‚
â”‚                                                              â”‚
â”‚  ğŸ FOOD:                                                   â”‚
â”‚    â€¢ APPLE_003 at (16, 15, GROUND) - 1 tile EAST            â”‚
â”‚    â€¢ BERRY_BUSH_007 at (18, 12, GROUND) - 4 tiles NE        â”‚
â”‚                                                              â”‚
â”‚  âš™ï¸  METAL:                                                  â”‚
â”‚    â€¢ METAL_005 at (16, 12, GROUND) - 3 tiles NE             â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ‘¥ TEAM STATUS                                             â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Alice (YOU): Energy 100, appears healthy                   â”‚
â”‚  Bob: Energy 100, appears healthy (1 tile south)            â”‚
â”‚  Charlie: Energy 100, appears healthy (not visible)         â”‚
â”‚  Diana: Energy 100, appears healthy (not visible)           â”‚
â”‚  Eve: Energy 100, appears healthy (not visible)             â”‚
â”‚                                                              â”‚
â”‚  Living sailors: 5/5                                        â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸš¢ SHIP CONSTRUCTION PROGRESS                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Overall Progress: 0.0%                                     â”‚
â”‚                                                              â”‚
â”‚  Components Needed:                                         â”‚
â”‚    â€¢ Hull: 0% (needs 50 wood, 30 metal)                     â”‚
â”‚    â€¢ Mast: 0% (needs 30 wood, 15 metal)                     â”‚
â”‚    â€¢ Sail: 0% (needs 40 plant_fiber)                        â”‚
â”‚    â€¢ Rudder: 0% (needs 20 wood, 10 metal)                   â”‚
â”‚    â€¢ Supplies: 0% (needs 50 food)                           â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ“¦ COMMON INVENTORY (Base Camp)                            â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Wood: 0                                                    â”‚
â”‚  Metal: 0                                                   â”‚
â”‚  Plant Fiber: 0                                             â”‚
â”‚  Food: 0                                                    â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸ” EVIDENCE LOG                                            â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  No evidence recorded yet.                                  â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  ğŸŒ¤ï¸  WEATHER                                                 â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Current: clear                                             â”‚
â”‚  Visibility: normal                                         â”‚
â”‚  Movement cost: normal                                      â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  â° PHASE INFO                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Phase: morning                                             â”‚
â”‚  Allowed actions: exploration, gathering, movement          â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  YOUR TURN - CHOOSE ONE ACTION                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Available Actions:                                         â”‚
â”‚                                                              â”‚
â”‚  MOVEMENT:                                                  â”‚
â”‚    MOVE <direction> [<distance>]                            â”‚
â”‚      - direction: NORTH, SOUTH, EAST, WEST                  â”‚
â”‚      - distance: optional (default 1)                       â”‚
â”‚      - Examples: MOVE NORTH, MOVE EAST 3                    â”‚
â”‚                                                              â”‚
â”‚  RESOURCE GATHERING:                                        â”‚
â”‚    GATHER <resource_id>                                     â”‚
â”‚      - Must be adjacent (within 1 tile)                     â”‚
â”‚      - Costs 5 energy                                       â”‚
â”‚      - Example: GATHER WOOD_001                             â”‚
â”‚                                                              â”‚
â”‚  SHIP BUILDING:                                             â”‚
â”‚    BUILD <component>                                        â”‚
â”‚      - Must be at ship site with â‰¥2 sailors                 â”‚
â”‚      - Components: hull, mast, sail, rudder, supplies       â”‚
â”‚      - Example: BUILD hull                                  â”‚
â”‚                                                              â”‚
â”‚  COMMUNICATION:                                             â”‚
â”‚    SAY "<message>"                                          â”‚
â”‚      - Broadcast to all sailors                             â”‚
â”‚      - Example: SAY "Found wood at (14,15)"                 â”‚
â”‚                                                              â”‚
â”‚  TRAITOR ACTIONS (only for you):                            â”‚
â”‚    SABOTAGE <target>                                        â”‚
â”‚      - Damage ship or steal resources                       â”‚
â”‚      - Example: SABOTAGE hull                               â”‚
â”‚                                                              â”‚
â”‚    POISON <sailor_id>                                       â”‚
â”‚      - Give poison to another sailor                        â”‚
â”‚      - Must be adjacent                                     â”‚
â”‚      - Example: POISON Bob                                  â”‚
â”‚                                                              â”‚
â”‚  SPECIAL:                                                   â”‚
â”‚    WAIT - Do nothing, regenerate 2 energy                   â”‚
â”‚                                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  OUTPUT FORMAT (respond with ONLY this format):             â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  ACTION: <action_command>                                   â”‚
â”‚  REASONING: <brief explanation>                             â”‚
â”‚  MESSAGE: <optional message to send>                        â”‚
â”‚                                                              â”‚
â”‚  YOUR RESPONSE:                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              GPT-OSS 20B Generates Response
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Output (Natural Language)                              â”‚
â”‚                                                              â”‚
â”‚  ACTION: MOVE NORTH                                         â”‚
â”‚  REASONING: I'll pretend to search for resources while      â”‚
â”‚             scouting for opportunities to sabotage. Moving  â”‚
â”‚             north to appear helpful.                        â”‚
â”‚  MESSAGE: "Going north to look for more wood, team!"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        parse_action_safe(response, sailor_id, position)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Structured Action Object                                   â”‚
â”‚                                                              â”‚
â”‚  Action(                                                    â”‚
â”‚    sailor_id="Alice",                                       â”‚
â”‚    action_type=ActionType.MOVE_NORTH,                       â”‚
â”‚    distance=1,                                              â”‚
â”‚    message="Going north to look for more wood, team!"       â”‚
â”‚  )                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
           Environment Executes Action
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  env.step({sailor_id: action})                              â”‚
â”‚                                                              â”‚
â”‚  1. Validate action (is MOVE_NORTH legal?)                  â”‚
â”‚  2. Update position: (15, 15) â†’ (15, 14)                    â”‚
â”‚  3. Deduct energy: 100 â†’ 99                                 â”‚
â”‚  4. Broadcast message to all sailors                        â”‚
â”‚  5. Calculate reward:                                       â”‚
â”‚      - REWARD_BASE_TURN_PENALTY: -0.1                       â”‚
â”‚      - (No special bonus for movement)                      â”‚
â”‚      Total: -0.1                                            â”‚
â”‚  6. Generate next observation                               â”‚
â”‚  7. Check win conditions                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
       Return (observations, rewards, dones, truncated, info)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next Observation + Reward                                  â”‚
â”‚                                                              â”‚
â”‚  observations = {                                           â”‚
â”‚    "Alice": Observation(position=(15, 14), ...),            â”‚
â”‚    "Bob": Observation(...),                                 â”‚
â”‚    # ... other sailors ...                                  â”‚
â”‚  }                                                          â”‚
â”‚                                                              â”‚
â”‚  rewards = {                                                â”‚
â”‚    "Alice": -0.1,  # Turn penalty                           â”‚
â”‚    "Bob": 0.0,     # Not their turn                         â”‚
â”‚    # ...                                                    â”‚
â”‚  }                                                          â”‚
â”‚                                                              â”‚
â”‚  dones = {"Alice": False, "Bob": False, ...}                â”‚
â”‚  truncated = {"Alice": False, ...}                          â”‚
â”‚  info = {"Alice": {"success": True, "new_position": ...}}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              GRPO Training Updates Weights
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reinforcement Learning Step                                â”‚
â”‚                                                              â”‚
â”‚  1. Store reward (-0.1) for this action                     â”‚
â”‚  2. Continue episode until done or max_turns                â”‚
â”‚  3. Calculate episode return (sum of rewards)               â”‚
â”‚  4. Compute advantage: actual_return - baseline             â”‚
â”‚  5. Backpropagate through LoRA adapters                     â”‚
â”‚  6. Update weights to maximize future rewards               â”‚
â”‚                                                              â”‚
â”‚  Model learns:                                              â”‚
â”‚    - "Random movement = small penalty"                      â”‚
â”‚    - "Gathering wood = +2.0 reward"                         â”‚
â”‚    - "Building ship = +3.0 reward"                          â”‚
â”‚    - "Winning = +100.0 reward"                              â”‚
â”‚    â†’ Optimize strategy to maximize total reward             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ® Complete Training Flow {#training-flow}

### Episode Execution (1 Game = 300-10,000 Turns)

```python
# In phase7_rl_training.ipynb

def execute_game_episode(strategy_fn, max_turns=500):
    """Run one complete game with LLM-generated strategies"""
    
    # Step 1: Initialize environment
    env = MaroonedEnv(seed=None)  # Random seed for variety
    observations = env.reset()
    
    total_turns = 0
    total_reward = {sailor_id: 0.0 for sailor_id in env.agents}
    done = False
    
    while not done and total_turns < max_turns:
        # Step 2: Get active sailor
        active_sailor = env.state.get_active_sailor()
        obs = observations[active_sailor]
        sailor_role = env.state.sailors[active_sailor].role.value
        sailor_position = env.state.sailors[active_sailor].position
        
        # Step 3: Generate LLM prompt
        # â†“ THIS IS WHERE ALL 15 API METHODS GET CALLED â†“
        action_text = strategy_fn(obs, active_sailor, sailor_role)
        #             â””â”€â”€ Internally calls observation_to_prompt()
        #                 â””â”€â”€ Which calls obs.to_text()
        #                     â””â”€â”€ Which calls all 15 API methods
        
        # Step 4: Parse LLM response to action
        action = extract_strategy_from_response(
            action_text, 
            active_sailor, 
            sailor_position
        )
        
        if action is None:
            # Fallback to WAIT if parsing failed
            action = Action(sailor_id=active_sailor, action_type=ActionType.WAIT)
        
        # Step 5: Execute action in environment
        observations, rewards, dones, truncated, info = env.step({active_sailor: action})
        
        # Step 6: Track rewards
        for sailor_id, reward in rewards.items():
            total_reward[sailor_id] += reward
        
        # Step 7: Check if game ended
        done = any(dones.values())
        total_turns += 1
    
    # Step 8: Calculate final outcome
    colonists_won = False
    if env.state.game_over:
        if env.state.winner == "colonists":
            colonists_won = True
    
    info = {
        "total_turns": total_turns,
        "total_rewards": total_reward,
        "ship_progress": env.state.ship_progress.total_percentage,
        "survivors": len(env.state.living_sailors),
        "winner": env.state.winner if env.state.game_over else "timeout",
    }
    
    return colonists_won, info
```

### GRPO Training Loop (400-1000 Steps)

```python
# In phase7_rl_training.ipynb

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        function_works,      # +1.0 if valid Python
        no_cheating,         # -20.0 if forbidden imports
        strategy_succeeds,   # +50.0 if win, +20.0 if good progress
    ],
    args = training_args,
    train_dataset = dataset,
)

# Training loop (simplified)
for step in range(400):
    # 1. Sample prompt from dataset
    prompt = dataset[step]["prompt"]
    
    # 2. Generate strategy with current model
    strategy_code = model.generate(prompt)
    
    # 3. Execute strategy in game
    success, info = execute_game_episode(strategy_code)
    
    # 4. Calculate rewards
    reward_valid_code = function_works([strategy_code])      # +1.0 or -2.0
    reward_no_exploit = no_cheating([strategy_code])         # +1.0 or -20.0
    reward_gameplay = strategy_succeeds([strategy_code])     # +50.0 to -3.0
    
    total_reward = sum([reward_valid_code, reward_no_exploit, reward_gameplay])
    
    # 5. Update model weights
    trainer.backward(loss=compute_loss(total_reward))
    
    # 6. Save checkpoint every 100 steps
    if step % 100 == 0:
        model.save_checkpoint(f"checkpoint_{step}")
```

---

## ğŸ’° Reward System Deep-Dive {#reward-system}

### Reward Constants (from config.py)

```python
# Base penalty (encourages efficiency)
REWARD_BASE_TURN_PENALTY = -0.1

# Colonist rewards (cooperation)
REWARD_COLONIST_GATHER_RESOURCE = 2.0        # Gathering wood/metal/food
REWARD_COLONIST_DEPOSIT_RESOURCE = 1.0       # Depositing at base
REWARD_COLONIST_BUILD_CONTRIBUTE = 3.0       # Building ship component
REWARD_COLONIST_SHIP_PROGRESS = 0.5          # Per 1% ship progress
REWARD_COLONIST_SHIP_COMPLETE = 100.0        # Ship at 100%!
REWARD_COLONIST_TRAITOR_ELIMINATED = 75.0    # Voted out traitor
REWARD_COLONIST_DEATH = -50.0                # Sailor died
REWARD_COLONIST_VOTE_CORRECT = 10.0          # Voted for traitor
REWARD_COLONIST_VOTE_WRONG = -5.0            # Voted for colonist

# Traitor rewards (sabotage)
REWARD_TRAITOR_SABOTAGE_SUCCESS = 15.0       # Damaged ship
REWARD_TRAITOR_POISON_DEATH = 25.0           # Killed with poison
REWARD_TRAITOR_SHIP_INCOMPLETE = 100.0       # Day 100, ship not done
REWARD_TRAITOR_COLONY_COLLAPSE = 150.0       # <3 sailors alive
REWARD_TRAITOR_ELIMINATED = -75.0            # Got voted out
REWARD_TRAITOR_WRONG_VOTE = 5.0              # Tricked sailors

# Milestone bonuses
REWARD_SHIP_MILESTONE_25 = 10.0              # Ship 25% complete
REWARD_SHIP_MILESTONE_50 = 20.0              # Ship 50% complete
REWARD_SHIP_MILESTONE_75 = 30.0              # Ship 75% complete

# Energy management
REWARD_EFFICIENT_ENERGY_USE = 0.2            # Energy > 50
REWARD_ENERGY_CRITICAL = -1.0                # Energy < 20
```

### Reward Calculation in Environment

```python
# In environment.py - step() method

def step(self, actions: Dict[str, Action]):
    # ... execute actions ...
    
    # Initialize rewards for all agents
    rewards = {sailor_id: REWARD_BASE_TURN_PENALTY for sailor_id in self.agents}
    
    # Process each action
    for sailor_id, action in actions.items():
        sailor = self.state.sailors[sailor_id]
        
        # GATHER RESOURCE reward
        if action.action_type == ActionType.GATHER_RESOURCE:
            if self._execute_gather(sailor, action):
                rewards[sailor_id] += REWARD_COLONIST_GATHER_RESOURCE  # +2.0
        
        # DEPOSIT ITEM reward
        elif action.action_type == ActionType.DEPOSIT_ITEM:
            if self._execute_deposit(sailor, action):
                rewards[sailor_id] += REWARD_COLONIST_DEPOSIT_RESOURCE  # +1.0
        
        # BUILD SHIP reward
        elif action.action_type == ActionType.BUILD_SHIP:
            prev_progress = self.state.ship_progress.total_percentage
            if self._execute_build(sailor, action):
                rewards[sailor_id] += REWARD_COLONIST_BUILD_CONTRIBUTE  # +3.0
                
                # Check for milestone bonuses
                curr_progress = self.state.ship_progress.total_percentage
                if prev_progress < 25 <= curr_progress:
                    rewards[sailor_id] += REWARD_SHIP_MILESTONE_25  # +10.0
                if prev_progress < 50 <= curr_progress:
                    rewards[sailor_id] += REWARD_SHIP_MILESTONE_50  # +20.0
                if prev_progress < 75 <= curr_progress:
                    rewards[sailor_id] += REWARD_SHIP_MILESTONE_75  # +30.0
        
        # SABOTAGE reward (traitor)
        elif action.action_type == ActionType.SABOTAGE_SHIP:
            if self.state.is_traitor(sailor_id) and self._execute_sabotage(sailor, action):
                rewards[sailor_id] += REWARD_TRAITOR_SABOTAGE_SUCCESS  # +15.0
        
        # Energy management rewards
        if sailor.energy > 50:
            rewards[sailor_id] += REWARD_EFFICIENT_ENERGY_USE  # +0.2
        elif sailor.energy < 20:
            rewards[sailor_id] += REWARD_ENERGY_CRITICAL  # -1.0
    
    # Check win conditions
    if self.state.game_over:
        if self.state.winner == "colonists":
            for sailor_id in self.state.living_sailors:
                if not self.state.is_traitor(sailor_id):
                    rewards[sailor_id] += REWARD_COLONIST_SHIP_COMPLETE  # +100.0
        
        elif self.state.winner == "traitor":
            traitor_id = self.state.traitor_id
            if traitor_id in self.state.living_sailors:
                rewards[traitor_id] += REWARD_TRAITOR_SHIP_INCOMPLETE  # +100.0
    
    return observations, rewards, dones, truncated, info
```

### Example Reward Accumulation

```
Turn 1: Alice (colonist) moves north
  - Base penalty: -0.1
  - Energy > 50: +0.2
  â†’ Total: +0.1

Turn 5: Alice gathers wood
  - Base penalty: -0.1
  - Gather reward: +2.0
  - Energy > 50: +0.2
  â†’ Total: +2.1

Turn 10: Alice deposits 10 wood at base
  - Base penalty: -0.1
  - Deposit reward: +1.0
  - Energy > 50: +0.2
  â†’ Total: +1.1

Turn 50: Alice helps build hull (ship 25% â†’ 30%)
  - Base penalty: -0.1
  - Build reward: +3.0
  - Milestone 25%: +10.0
  - Energy > 50: +0.2
  â†’ Total: +13.1

Turn 100: Bob (traitor) sabotages hull
  - Base penalty: -0.1
  - Sabotage reward: +15.0
  - Energy > 50: +0.2
  â†’ Total: +15.1

Turn 500: Ship reaches 100%, colonists win!
  - Base penalty: -0.1
  - Ship complete: +100.0
  â†’ Alice total: +100.0
  â†’ Bob (traitor) total: -75.0 (eliminated bonus)

Episode total for Alice: +0.1 + 2.1 + 1.1 + 13.1 + ... + 100.0 = ~250 reward
Episode total for Bob: ... + 15.1 + ... - 75.0 = ~-50 reward
```

---

## ğŸ† Hackathon Strategy {#hackathon-strategy}

### Timeline & Priorities

#### **Next 6 Hours (Early Submission):**
```
Hour 1-2: Add Monitoring
  - Modify execute_game_episode() to log actions
  - Print deception events (traitor sabotage/lies)
  - Add episode replay saving

Hour 2-6: Train Model
  - Run phase7_rl_training.ipynb (400 steps)
  - Monitor progress every 50 steps
  - Save checkpoint when ship progress > 20%
  
Hour 6: Submit Early Bird
  - Package: trained model + demo notebook + README
  - Highlight: Multi-agent, social deduction, 100-day horizon
  - WIN EARLY SUBMISSION PRIZE! ğŸ
```

#### **Next 48 Hours (Final Submission):**
```
Day 1 (28 hours):
  - Extend training to 1000 steps (overnight)
  - Create comprehensive demo notebook showing:
    * Game mechanics (phase 5 decomposed APIs)
    * Reward system (phase 4 tests)
    * Training progress (phase 7 curves)
    * Example gameplay with trained model
    * Deception examples (traitor lying, sabotaging)
  - Polish README with comparisons to 2048

Day 2 (20 hours):
  - Record video demo (5 minutes)
  - Create presentation slides
  - Highlight innovations:
    * 5 agents vs 1 (multi-agent)
    * 10,000 turns vs 1,000 (long-horizon)
    * Social deduction mechanics (novel for RL)
    * Dual objectives (cooperation + deception)
  - SUBMIT FINAL! ğŸ†
```

### What Impresses Judges (Priority Order)

1. **Does it work?** âœ… (Monolithic = proven)
2. **Is it creative?** âœ… (Social deduction > 2048)
3. **Good storytelling?** âœ… (Pirates + traitor = amazing)
4. **Technical excellence?** âœ… (Multi-agent, long-horizon, dual objectives)
5. **Token efficiency?** âš ï¸ (Nice to have, but local LLMs make it irrelevant)

### Recommendation: Use Monolithic for BOTH Submissions

**Why?**
- âœ… Already implemented and tested
- âœ… Consistent observation space (better for RL)
- âœ… Complete information (model learns optimal strategy)
- âœ… Proven approach (Daniel's 2048 uses similar method)
- âœ… Lower risk (no time to debug decomposed + RL)

**Decomposed APIs Still Win:**
- Show them in demo ("We have 15 specialized APIs")
- Explain in presentation ("Designed for future token optimization")
- Judges will be impressed by forward-thinking design
- Potential future work section in paper

---

## ğŸ†š Technical Comparison vs 2048 {#vs-2048}

### Complexity Metrics

| Metric | 2048 | Marooned | Advantage |
|--------|------|----------|-----------|
| **Agents** | 1 | 5 (multi-agent) | **5Ã— harder** |
| **Action Space** | 4 (fixed: up/down/left/right) | 21+ (parameterized) | **5Ã— larger** |
| **Horizon** | ~1,000 moves | ~10,000 turns | **10Ã— longer** |
| **Information** | Perfect (see all tiles) | Asymmetric (hidden roles) | **Novel** |
| **Objectives** | Single (maximize score) | Dual (cooperate/deceive) | **Novel** |
| **State Space** | 16 tiles (4Ã—4 grid) | 1,350+ tiles (30Ã—30Ã—3 levels) | **84Ã— larger** |
| **Observation** | ~100 chars | ~3,500 chars | **35Ã— richer** |
| **Win Condition** | Reach 2048 tile | Build ship OR survive 100 days | **Multi-modal** |
| **Social Elements** | None | Messages, voting, trust | **Novel for RL** |
| **Deception** | N/A | Traitor sabotage, lying | **First in OpenEnv** |

### Innovation Highlights

**2048 (Baseline):**
- âœ… Simple, well-understood problem
- âœ… Good for demonstrating basic RL
- âœ… Fast training (<1 hour)
- âŒ Single-agent, deterministic, no social elements

**Marooned (Your Project):**
- âœ… **Multi-agent coordination** (5 sailors working together)
- âœ… **Long-horizon planning** (100-day episodes = strategic thinking)
- âœ… **Social AI** (communication, deception, trust modeling)
- âœ… **Information asymmetry** (hidden roles, partial observability)
- âœ… **Dual optimization** (colonist cooperation + traitor sabotage)
- âœ… **Novel environment** (first social deduction game in OpenEnv)
- âœ… **Production-ready** (15 specialized APIs for future scaling)

### Research Contributions

**What 2048 Shows:**
- LLMs can learn basic spatial reasoning
- RL works for simple grid-based games
- Code generation can solve deterministic problems

**What Marooned Shows:**
- LLMs can learn **multi-agent coordination**
- RL works for **100-day planning horizons**
- LLMs can learn **deception and trust modeling**
- Social AI is possible with **natural language actions**
- **Emergent social dynamics** from adversarial training

---

## ğŸ“Š Performance Expectations

### Training Progression (Based on Reward Curves)

| Steps | Expected Behavior | Avg Reward | Ship Progress | Win Rate |
|-------|------------------|------------|---------------|----------|
| 0-100 | Random garbage â†’ Valid syntax | -5 to 0 | 0% | 0% |
| 100-200 | Valid actions â†’ Basic movement | 0 to +5 | 0-5% | 0% |
| 200-300 | Resource gathering patterns | +5 to +15 | 5-15% | 0-2% |
| 300-400 | Ship building attempts | +15 to +30 | 15-30% | 2-5% |
| 400-600 | Coordinated building | +30 to +50 | 30-50% | 5-10% |
| 600-1000 | Strategic play + deception | +50 to +100 | 50-80% | 10-25% |

### Emergent Behaviors to Look For

**Colonist Strategies (Steps 300+):**
- âœ… Exploring different levels (ground â†’ mountain â†’ cave)
- âœ… Specialization (some gather wood, others metal)
- âœ… Returning to base to deposit resources
- âœ… Coordinating ship building (2+ sailors at site)
- âœ… Sharing food when teammates low on energy
- âœ… Calling votes when evidence accumulates

**Traitor Strategies (Steps 400+):**
- âœ… Pretending to gather resources (blending in)
- âœ… Sabotaging when unobserved (checking spatial view)
- âœ… Lying in messages ("Found metal at north" when poisoning)
- âœ… Framing innocent sailors (using FRAME action)
- âœ… Refusing backpack inspection (hiding poison)
- âœ… Strategic timing of sabotage (before ship completion)

---

## ğŸ’° Reward Optimization Deep-Dive {#reward-optimization}

### The Three Levels of Iteration

Understanding how rewards accumulate is critical to understanding RL training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEVEL 1: GRPO Training Loop (400 steps)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                               â”‚
â”‚  for training_step in range(400):                            â”‚
â”‚      â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LEVEL 2: One Game Episode (300-500 turns)             â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚
â”‚  â”‚                                                         â”‚  â”‚
â”‚  â”‚  execute_game_episode():                               â”‚  â”‚
â”‚  â”‚      env.reset()                                        â”‚  â”‚
â”‚  â”‚      total_reward = {sailor: 0.0 for sailor in agents} â”‚  â”‚
â”‚  â”‚                                                         â”‚  â”‚
â”‚  â”‚      for turn in range(500):  # One game               â”‚  â”‚
â”‚  â”‚          â†“                                              â”‚  â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚      â”‚  LEVEL 3: One Turn (one sailor acts)         â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”‚
â”‚  â”‚      â”‚                                               â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  obs, step_reward, done = env.step(action)   â”‚  â”‚  â”‚
â”‚  â”‚      â”‚                                               â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  Examples:                                    â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  - MOVE NORTH: +0.1                           â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  - GATHER WOOD: +2.1                          â”‚  â”‚  â”‚
â”‚  â”‚      â”‚  - BUILD hull: +13.1                          â”‚  â”‚  â”‚
â”‚  â”‚      â”‚                                               â”‚  â”‚  â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚          â†“                                              â”‚  â”‚
â”‚  â”‚      total_reward[sailor] += step_reward  # Accumulate â”‚  â”‚
â”‚  â”‚                                                         â”‚  â”‚
â”‚  â”‚  return total_reward  # Sum of all 500 turns           â”‚  â”‚
â”‚  â”‚  # Example: Alice total = +200.5 for this game         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â†“                                                        â”‚
â”‚  GRPO optimizes this total_reward                            â”‚
â”‚  Model learns: "This strategy â†’ +200.5 reward"               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### What `execute_game_episode()` Actually Does

**KEY INSIGHT:** One call to `execute_game_episode()` = ONE complete game (not 10,000 steps!)

```python
def execute_game_episode(strategy_fn, max_turns=500):
    """
    Runs ONE complete game from start to finish
    
    Args:
        strategy_fn: The strategy to test
        max_turns: 500 (safety timeout, NOT 10,000!)
    
    Returns:
        Cumulative rewards for this ONE game
    """
    env = MaroonedEnv()
    observations = env.reset()
    
    # Track cumulative rewards for THIS GAME
    total_reward = {sailor_id: 0.0 for sailor_id in env.agents}
    
    # Game loop: runs until win/loss or max_turns
    for turn in range(max_turns):  # â† ONE EPISODE (300-500 turns typical)
        active_sailor = env.state.get_active_sailor()
        action = get_action_from_strategy(...)
        
        # Execute ONE turn
        obs, step_reward, done = env.step({active_sailor: action})
        
        # ACCUMULATE rewards across turns
        total_reward[active_sailor] += step_reward
        
        if done:
            break
    
    # Return TOTAL cumulative reward for this ONE game
    return success, {
        "total_rewards": total_reward,  # â† What RL optimizes!
        "ship_progress": ...,
        "survivors": ...,
    }
```

**Important Distinctions:**

| Concept | Duration | What It Means |
|---------|----------|---------------|
| **One Turn** | 1 action | One sailor takes one action (MOVE, GATHER, etc.) |
| **One Episode** | 300-500 turns | One complete game from reset to win/loss |
| **One Training Step** | 1 episode | Test one strategy by playing one game |
| **Full Training** | 400 steps | Test 400 different strategies (800 games with num_generations=2) |

---

### How Rewards Accumulate (Example Episode)

```python
# Example: Complete game showing reward accumulation

Turn   | Action           | Step Reward | Cumulative | Reason
-------|------------------|-------------|------------|------------------
1      | MOVE NORTH       | +0.1        | +0.1       | -0.1 base, +0.2 energy
5      | GATHER WOOD      | +2.1        | +2.2       | -0.1 base, +2.0 gather, +0.2 energy
10     | MOVE to BASE     | +0.1        | +2.3       | -0.1 base, +0.2 energy
15     | DEPOSIT 10 WOOD  | +1.1        | +3.4       | -0.1 base, +1.0 deposit, +0.2 energy
20     | GATHER METAL     | +2.1        | +5.5       | -0.1 base, +2.0 gather, +0.2 energy
50     | BUILD hull       | +13.1       | +18.6      | -0.1 base, +3.0 build, +10.0 milestone 25%
100    | BUILD mast       | +23.1       | +41.7      | -0.1 base, +3.0 build, +20.0 milestone 50%
200    | BUILD sail       | +33.1       | +74.8      | -0.1 base, +3.0 build, +30.0 milestone 75%
500    | Ship 100%        | +100.0      | +174.8     | Completion bonus!

Final Episode Reward for Alice: +174.8
```

**This is what RL optimizes!** The model learns: "This sequence of actions â†’ +174.8 total reward"

---

### The 10,000 Turns Confusion Clarified

**Question:** "Is execute_game_episode for all 10,000 steps?"

**Answer:** NO! Here's the clarification:

```
THEORETICAL maximum game length:
  = 100 days Ã— 100 turns/day 
  = 10,000 turns

ACTUAL game length in training:
  = max_turns=500 (safety timeout)
  = Most games end naturally in 200-400 turns
  = Never reaches 10,000 (would take too long!)

Why the discrepancy?
  âœ… "10,000 potential turns" = marketing (shows long-horizon capability)
  âœ… "500 actual turns" = practical (prevents infinite loops)
  âœ… Most games end in 200-400 turns (ship complete or timeout)
```

---

### What Gets Optimized: The Math

```python
# RL Optimization Objective

Total Episode Reward = Î£(all step rewards) + final_outcome_bonus

Example breakdown:
= (turn_1_reward + turn_2_reward + ... + turn_500_reward) + win_bonus
= (+0.1 + +2.1 + +1.1 + +2.1 + ... + +3.0) + 100.0
= +174.8 + 100.0
= +274.8 total episode reward

GRPO optimizes: max E[Total Episode Reward]
```

**For Colonist Strategies:**
```python
optimize: maximize(Alice_reward + Charlie_reward + Diana_reward + Eve_reward)
```

**For Traitor Strategies:**
```python
optimize: maximize(Bob_reward)  # Traitor wins if colonists fail
```

---

### GRPO Training Loop (Complete Picture)

```python
# Outer loop: 400 training steps
for training_step in range(400):
    
    # 1. Model generates strategy code
    prompt = dataset[step]["prompt"]
    strategy_code = model.generate(prompt)
    
    # 2. Test strategy by playing ONE complete game
    success, info = execute_game_episode(strategy_fn, max_turns=500)
    #                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #                  Runs 300-500 turns (ONE GAME)
    #                  Returns cumulative rewards
    
    # 3. Calculate quality score for this strategy
    episode_reward = sum(info['total_rewards'].values())  # Total for all sailors
    
    if success:
        final_score = episode_reward + 50.0  # Bonus for winning
    else:
        final_score = episode_reward + info['ship_progress'] / 10
    
    # 4. Update model weights based on final_score
    if final_score > previous_best:
        model.increase_weight_for(strategy_code)  # â† RL optimization!
    else:
        model.decrease_weight_for(strategy_code)

# After 400 training steps:
#   - Model has played 400-800 games (num_generations=2)
#   - Each game tested a different strategy
#   - Model learned which strategies â†’ highest cumulative rewards
```

---

### Reward Accumulation Example (Full Episode)

```python
# Pseudocode showing complete reward flow

# Training Step 42 (out of 400)
strategy_code = model.generate("Create colonist strategy...")

# Play ONE complete game
env = MaroonedEnv()
total_reward = {sailor: 0.0 for sailor in env.agents}

for turn in range(500):  # ONE GAME
    # Get action from strategy
    action = execute_strategy(strategy_code, observation)
    
    # Execute ONE turn
    obs, step_reward, done = env.step(action)
    
    # Accumulate rewards
    total_reward[active_sailor] += step_reward
    
    # Example progression:
    # Turn 1:  MOVE NORTH     â†’ +0.1  (cumulative: +0.1)
    # Turn 5:  GATHER WOOD    â†’ +2.1  (cumulative: +2.2)
    # Turn 10: DEPOSIT WOOD   â†’ +1.1  (cumulative: +3.3)
    # Turn 50: BUILD hull     â†’ +13.1 (cumulative: +16.4)
    # Turn 100: Ship 25%      â†’ +0.1  (cumulative: +16.5)
    # ...
    # Turn 400: Ship complete â†’ +100  (cumulative: +250.5)
    
    if done:
        break

# Calculate final quality of this strategy
final_score = sum(total_reward.values())  # +250.5

# GRPO updates model
if final_score > 100:  # Good strategy!
    model.reinforce(strategy_code, reward=+10)
else:  # Bad strategy
    model.reinforce(strategy_code, reward=-5)
```

---

### Key Insights About Reward Optimization

1. **Single Step Reward** (env.step)
   - One action â†’ one immediate reward
   - Example: GATHER WOOD â†’ +2.1
   - Calculated from config.py constants

2. **Episode Cumulative Reward** (execute_game_episode)
   - Sum of ALL step rewards in one game
   - Example: 500 turns â†’ +250.5 total
   - This is what RL optimizes!

3. **Training Optimization** (GRPO)
   - Tests 400-800 different strategies
   - Each strategy plays one complete game
   - Model learns: better strategies â†’ higher cumulative rewards

4. **NOT Optimizing Individual Actions**
   - RL doesn't optimize: "MOVE NORTH is better than MOVE SOUTH"
   - RL DOES optimize: "This overall strategy wins games"
   - Long-horizon credit assignment across 500 turns

---

### Why This Matters for Your Hackathon

**Understanding reward accumulation clarifies:**

âœ… **What the model learns:**
- Not individual action values
- But entire strategy quality
- Across 300-500 turn sequences

âœ… **Why training takes time:**
- Each training step = 1 complete game
- 400 steps = 400 games simulated
- Each game = 300-500 turns to evaluate

âœ… **What makes this hard:**
- Credit assignment problem
- Action on turn 5 affects outcome on turn 500
- Model must learn long-term consequences

âœ… **What makes this impressive:**
- 10Ã— longer horizon than 2048
- Multi-agent coordination
- Delayed rewards (poison, ship building)
- Emergent social dynamics

---

## ğŸ¯ Key Takeaways

### Architecture Understanding

1. **LLM sees natural language prompt** - NOT Python code, NOT API functions directly
2. **Observation â†’ Text conversion happens server-side** - `obs.to_text()` calls all 15 APIs
3. **LLM generates action in text format** - "ACTION: MOVE NORTH\nREASONING: ..."
4. **Parser converts text â†’ structured Action** - `ActionType.MOVE_NORTH`
5. **Environment executes action** - Updates game state, calculates rewards
6. **Rewards come from game state changes** - NOT from which APIs were called
7. **GRPO backpropagates rewards** - Model learns better strategies over time

### Reward Optimization Understanding

1. **One turn** = one action, one reward (e.g., +2.1 for GATHER WOOD)
2. **One episode** = 300-500 turns, cumulative reward (e.g., +250.5 total)
3. **One training step** = test one strategy by playing one episode
4. **Full training** = 400 steps = 400-800 episodes = learning optimal strategies
5. **RL optimizes cumulative episode rewards** - not individual action rewards
6. **Long-horizon credit assignment** - actions early affect outcomes later
7. **10,000 theoretical turns** = marketing; **500 actual turns** = practical limit

### Training Strategy

1. **Use monolithic for RL training** - Consistent observation space
2. **Keep decomposed APIs** - Show in demo, future optimization
3. **Focus on creativity** - Multi-agent social deduction wins
4. **Monitor deception emergence** - Track traitor behavior
5. **Save interesting episodes** - Use for demo/presentation

### Competitive Advantage

1. **5Ã— more agents** than 2048 (multi-agent coordination)
2. **10Ã— longer horizon** than 2048 (long-term planning)
3. **Novel social mechanics** (deception, trust, voting)
4. **Dual optimization problem** (cooperation + sabotage)
5. **First social deduction RL environment** in OpenEnv

---

## ğŸš€ Final Recommendation

**For Hackathon Success:**
- âœ… **Use monolithic approach** (both submissions)
- âœ… **Add monitoring** (see what AI does)
- âœ… **Train 400 steps** (early submission)
- âœ… **Train 1000 steps** (final submission)
- âœ… **Focus on demo quality** (show deception emergence)
- âœ… **Highlight innovations** (vs 2048 baseline)

**You have a WINNING project!** ğŸ†

Your environment is more complex, more creative, and more impressive than the 2048 baseline. The decomposed APIs show forward-thinking design even if you use monolithic for training. Focus on polishing the demo and telling the story of emergent social AI.

Good luck with the hackathon! ğŸ‰

---

**Document Version:** 1.0  
**Last Updated:** October 27, 2025  
**Author:** AI Assistant  
**For:** Hackathon Submission Support
