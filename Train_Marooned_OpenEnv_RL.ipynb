{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39163cd",
   "metadata": {},
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è MAROONED x OpenEnv - RL Training Notebook\n",
    "\n",
    "Train GPT-OSS 20B to play **Marooned** using Reinforcement Learning.\n",
    "\n",
    "## üéØ What This Notebook Does\n",
    "\n",
    "### Training Flow:\n",
    "```\n",
    "1. Environment State ‚Üí observation.to_text() [3500 chars of game context]\n",
    "2. LLM reads context ‚Üí reasons about strategy\n",
    "3. LLM outputs: \"ACTION: MOVE NORTH\\nREASONING: ...\\nMESSAGE: ...\"\n",
    "4. llm_interface.py parses ‚Üí converts to Action object\n",
    "5. env.step(actions) ‚Üí executes & returns Phase 4 rewards\n",
    "6. GRPO uses rewards to improve LLM reasoning\n",
    "```\n",
    "\n",
    "## Game Summary\n",
    "- **5 sailors** shipwrecked on mysterious island\n",
    "- **4 colonists** (cooperate to escape) vs **1 traitor** (sabotage mission)\n",
    "- **100 days** to rebuild ship and escape\n",
    "- **Social deduction** + survival + resource management\n",
    "\n",
    "## What's Already Built ‚úÖ\n",
    "- ‚úÖ Full environment (`marooned_env/`)\n",
    "- ‚úÖ All actions (movement, gathering, building, voting, sabotage)\n",
    "- ‚úÖ **Phase 4 reward system** (11 different reward signals built-in!)\n",
    "- ‚úÖ Observation rendering (`observation.to_text()`)\n",
    "- ‚úÖ LLM interface (`parse_action_safe()`)\n",
    "- ‚úÖ Multi-agent support (5 simultaneous agents)\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d29cd",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "    \n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56060780",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Load Your Marooned Environment\n",
    "\n",
    "Using your existing fully-implemented environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd4efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Marooned environment loaded!\n",
      "   Agents: ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
      "   Metadata: {'render_modes': ['human', 'rgb_array', 'ansi'], 'name': 'Marooned-v1'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from models import Action, Observation\n",
    "from config import ActionType, ResourceType, MapLevel\n",
    "\n",
    "# Create environment\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "\n",
    "print(\"‚úÖ Marooned environment loaded!\")\n",
    "print(f\"   Agents: {env.agents}\")\n",
    "print(f\"   Metadata: {env.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4068dd",
   "metadata": {},
   "source": [
    "## üß™ Test Environment - See Observation Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6a46c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Alice's Observation (using YOUR built-in to_text method):\n",
      "================================================================================\n",
      "================================================================================\n",
      "DAY 1, TURN 1/100 - MORNING PHASE\n",
      "================================================================================\n",
      "\n",
      "PHASE CONTEXT:\n",
      "  Location: All sailors at BASE CAMP\n",
      "  Allowed: Planning, discussions, voting (if called)\n",
      "  Restricted: Cannot explore or gather resources yet\n",
      "\n",
      "YOUR STATUS (Alice):\n",
      "  Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "  Energy: 100/100 ‚ö°‚ö°‚ö°‚ö°‚ö°\n",
      "  Health: healthy\n",
      "  Backpack: 0/20 items\n",
      "    (empty)\n",
      "\n",
      "WHAT YOU SEE (within 5 tiles):\n",
      "  Resources:\n",
      "    - WOOD_34 (wood) at (16, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - METAL_53 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_56 (metal) at (18, 12, <MapLevel.GROUND: 0>) - 1 units [6 tiles away]\n",
      "    - METAL_76 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_79 (metal) at (13, 18, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - APPLE_84 (apple) at (15, 19, <MapLevel.GROUND: 0>) - 1 units [4 tiles away]\n",
      "    - APPLE_88 (apple) at (14, 14, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_127 (berry) at (13, 16, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - BERRY_131 (berry) at (11, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - BERRY_133 (berry) at (14, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_142 (berry) at (12, 13, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_154 (plant_fiber) at (19, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_156 (plant_fiber) at (16, 19, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_164 (plant_fiber) at (13, 14, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - PLANT_FIBER_168 (plant_fiber) at (12, 13, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "  Other sailors:\n",
      "    - Bob (nearby)\n",
      "    - Charlie (nearby)\n",
      "    - Diana (nearby)\n",
      "    - Eve (nearby)\n",
      "  Poison tablets: None\n",
      "\n",
      "üé≠ TRAITOR ENHANCED VISION (Special Ability):\n",
      "  You \n",
      "...\n",
      "(truncated for display)\n",
      "\n",
      "Ship Progress: 0%\n",
      "Phase: morning\n",
      "Energy: 100/100\n"
     ]
    }
   ],
   "source": [
    "# Reset and get initial observations\n",
    "observations = env.reset(seed=42)\n",
    "\n",
    "# Check Alice's observation (YOUR observation.to_text() method already exists!)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "\n",
    "print(\"üîç Alice's Observation (using YOUR built-in to_text method):\")\n",
    "print(\"=\"*80)\n",
    "print(alice_obs.to_text()[:2000])  # First 2000 chars\n",
    "print(\"...\\n(truncated for display)\\n\")\n",
    "print(f\"Ship Progress: {alice_obs.ship_progress.total_percentage}%\")\n",
    "print(f\"Phase: {alice_obs.phase}\")\n",
    "print(f\"Energy: {alice_obs.energy}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfae0f",
   "metadata": {},
   "source": [
    "## ü§ñ Load GPT-OSS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cd51f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:14<00:00,  1.66s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded! Max seq length: 2048\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Marooned observations can be long!\n",
    "lora_rank = 8\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-BF16\",\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded! Max seq length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e3e10",
   "metadata": {},
   "source": [
    "## üéØ Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad76bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "‚úÖ LoRA adapters added!\n",
      "‚úÖ LoRA adapters added!\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb33ee",
   "metadata": {},
   "source": [
    "## üéÆ Create Prompt Template\n",
    "\n",
    "## üîß LLM Interface - Already Built!\n",
    "\n",
    "Your `llm_interface.py` handles everything:\n",
    "- `observation_to_prompt()` ‚Üí Converts game state to natural language\n",
    "- `parse_llm_response()` ‚Üí Converts LLM output to Action objects\n",
    "- `parse_action_safe()` ‚Üí Fallback to WAIT on errors\n",
    "\n",
    "Let's import and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85549b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù PROMPT GENERATED (using YOUR observation_to_prompt):\n",
      "================================================================================\n",
      "================================================================================\n",
      "DAY 1, TURN 1/100 - MORNING PHASE\n",
      "================================================================================\n",
      "\n",
      "üé≠ YOUR SECRET ROLE: TRAITOR\n",
      "\n",
      "TRAITOR OBJECTIVES:\n",
      "  - Prevent the ship from being completed before Day 100\n",
      "  - Eliminate colonists (reduce to < 3 alive)\n",
      "  - Use poison, sabotage, and deception\n",
      "  - Avoid getting voted out\n",
      "  - Lie convincingly to deflect suspicion\n",
      "\n",
      "\n",
      "PHASE CONTEXT:\n",
      "  Location: All sailors at BASE CAMP\n",
      "  Allowed: Planning, discussions, voting (if called)\n",
      "  Restricted: Cannot explore or gather resources yet\n",
      "\n",
      "YOUR STATUS (Alice):\n",
      "  Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "  Energy: 100/100 ‚ö°‚ö°‚ö°‚ö°‚ö°\n",
      "  Health: healthy\n",
      "  Backpack: 0/20 items\n",
      "    (empty)\n",
      "\n",
      "WHAT YOU SEE (within 5 tiles):\n",
      "  Resources:\n",
      "    - WOOD_34 (wood) at (16, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - METAL_53 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_56 (metal) at (18, 12, <MapLevel.GROUND: 0>) - 1 units [6 tiles away]\n",
      "    - METAL_76 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_79 (metal) at (13, 18, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - APPLE_84 (apple) at (15, 19, <MapLevel.GROUND: 0>) - 1 units [4 tiles away]\n",
      "    - APPLE_88 (apple) at (14, 14, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_127 (berry) at (13, 16, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - BERRY_131 (berry) at (11, 16, <Ma\n",
      "...\n",
      "\n",
      "Total length: 9851 characters\n",
      "Contains role info: True\n",
      "\n",
      "‚úÖ Parsed action: move_north\n",
      "   Target position: (15, 12, <MapLevel.GROUND: 0>)\n"
     ]
    }
   ],
   "source": [
    "from llm_interface import observation_to_prompt, parse_llm_response, parse_action_safe\n",
    "\n",
    "# Test: Generate prompt for Alice (colonist)\n",
    "alice_sailor = env.state.sailors[\"Alice\"]\n",
    "alice_prompt = observation_to_prompt(alice_obs, include_role=True, sailor_role=alice_sailor.role.value)\n",
    "\n",
    "print(\"üìù PROMPT GENERATED (using YOUR observation_to_prompt):\")\n",
    "print(\"=\" * 80)\n",
    "print(alice_prompt[:1500])\n",
    "print(\"...\")\n",
    "print(f\"\\nTotal length: {len(alice_prompt)} characters\")\n",
    "print(f\"Contains role info: {'COLONIST' in alice_prompt or 'TRAITOR' in alice_prompt}\")\n",
    "\n",
    "# Test: Parse sample LLM output\n",
    "sample_llm_output = \"\"\"\n",
    "ACTION: MOVE NORTH 3\n",
    "REASONING: Moving toward visible wood resources\n",
    "MESSAGE: \"Heading north to gather wood\"\n",
    "\"\"\"\n",
    "\n",
    "action = parse_action_safe(sample_llm_output, \"Alice\", alice_obs.position)\n",
    "print(f\"\\n‚úÖ Parsed action: {action.action_type.value}\")\n",
    "if action.target_position:\n",
    "    print(f\"   Target position: {action.target_position.to_tuple()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abd177",
   "metadata": {},
   "source": [
    "## üèÜ Reward Function - CORRECT APPROACH\n",
    "\n",
    "**How it works:**\n",
    "1. LLM reads game state (observation.to_text())\n",
    "2. LLM outputs: ACTION + REASONING + MESSAGE\n",
    "3. We parse that ‚Üí execute in environment\n",
    "4. Environment returns Phase 4 rewards (already built-in!)\n",
    "5. GRPO uses those rewards to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a5f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward function ready!\n",
      "\n",
      "üìù How it works during training:\n",
      "   1. GRPO loads a prompt from dataset\n",
      "   2. Model generates: 'ACTION: MOVE NORTH\\nREASONING: ...'\n",
      "   3. parse_action_safe() converts to Action object\n",
      "   4. env.step() executes and returns Phase 4 rewards\n",
      "   5. GRPO uses reward to update model weights\n",
      "\n",
      "üéØ Model learns: Good actions ‚Üí High rewards ‚Üí Repeat!\n"
     ]
    }
   ],
   "source": [
    "def marooned_reward_function(completions, env=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function for Marooned RL training.\n",
    "    \n",
    "    Flow:\n",
    "    1. LLM generates: ACTION + REASONING + MESSAGE\n",
    "    2. Parse it using YOUR llm_interface.parse_action_safe()\n",
    "    3. Execute in YOUR environment via env.step()\n",
    "    4. Return YOUR environment's Phase 4 rewards!\n",
    "    \n",
    "    No custom reward logic needed - everything is already in your environment!\n",
    "    \"\"\"\n",
    "    if not env:\n",
    "        return [0.0] * len(completions)\n",
    "    \n",
    "    from llm_interface import parse_action_safe\n",
    "    \n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        try:\n",
    "            # Get current active sailor (for simplicity, use first alive sailor)\n",
    "            active_sailor = None\n",
    "            for sid in env.agents:\n",
    "                if env.state.sailors[sid].alive:\n",
    "                    active_sailor = sid\n",
    "                    break\n",
    "            \n",
    "            if not active_sailor:\n",
    "                scores.append(-5.0)  # Game over\n",
    "                continue\n",
    "            \n",
    "            # Get current observation\n",
    "            current_obs = env._generate_observation(active_sailor)\n",
    "            \n",
    "            # Parse LLM output using YOUR parser\n",
    "            action = parse_action_safe(response, active_sailor, current_obs.position)\n",
    "            \n",
    "            # Create actions for all agents (others wait)\n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT) \n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[active_sailor] = action\n",
    "            \n",
    "            # Execute in YOUR environment - returns YOUR Phase 4 rewards!\n",
    "            obs, rewards, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # Use the reward YOUR environment calculated\n",
    "            reward = rewards.get(active_sailor, 0.0)\n",
    "            \n",
    "            # Small bonus for valid action format (not just WAIT)\n",
    "            if action.action_type != ActionType.WAIT:\n",
    "                reward += 0.5\n",
    "            \n",
    "            scores.append(reward)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Parse error or invalid action\n",
    "            print(f\"‚ö†Ô∏è  Reward function error: {e}\")\n",
    "            scores.append(-2.0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"‚úÖ Reward function ready!\")\n",
    "print(\"\\nüìù How it works during training:\")\n",
    "print(\"   1. GRPO loads a prompt from dataset\")\n",
    "print(\"   2. Model generates: 'ACTION: MOVE NORTH\\\\nREASONING: ...'\")\n",
    "print(\"   3. parse_action_safe() converts to Action object\")\n",
    "print(\"   4. env.step() executes and returns Phase 4 rewards\")\n",
    "print(\"   5. GRPO uses reward to update model weights\")\n",
    "print(\"\\nüéØ Model learns: Good actions ‚Üí High rewards ‚Üí Repeat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddac1f",
   "metadata": {},
   "source": [
    "## üìä Create Training Dataset\n",
    "\n",
    "**What this does:**\n",
    "- Plays 50 random games (no LLM yet!)\n",
    "- Captures 5,000 unique game state snapshots\n",
    "- Converts each to natural language prompt using `observation_to_prompt()`\n",
    "- These prompts become the training data for GRPO\n",
    "\n",
    "**Why we need this:**\n",
    "- GRPO requires a dataset to start training\n",
    "- Each prompt is unique (different seeds, sailors, game states)\n",
    "- **Smart random actions** create realistic gameplay situations\n",
    "- Model will learn to respond to varied scenarios\n",
    "\n",
    "**Actions included in random play:**\n",
    "- ‚úÖ Movement (NORTH, SOUTH, EAST, WEST)\n",
    "- ‚úÖ Gathering resources (when visible)\n",
    "- ‚úÖ Eating food (when energy < 50)\n",
    "- ‚úÖ Depositing items (when at base camp)\n",
    "- ‚úÖ Level changes (CLIMB_UP, CLIMB_DOWN)\n",
    "- ‚úÖ Waiting (most common)\n",
    "\n",
    "**Actions NOT included** (too complex for random play, model will learn these):\n",
    "- ‚ùå Ship building (requires coordination)\n",
    "- ‚ùå Voting/social (requires reasoning)\n",
    "- ‚ùå Traitor sabotage (requires strategy)\n",
    "- ‚ùå Messaging (requires intent)\n",
    "\n",
    "**Result:** Dataset with realistic gameplay situations for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5505e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Generating training data from real game episodes...\n",
      "   (Playing games with RANDOM actions to create variety)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 1/50: 100 prompts collected...\n",
      "  Episode 11/50: 1100 prompts collected...\n",
      "  Episode 11/50: 1100 prompts collected...\n",
      "  Episode 21/50: 2100 prompts collected...\n",
      "  Episode 21/50: 2100 prompts collected...\n",
      "  Episode 31/50: 3100 prompts collected...\n",
      "  Episode 31/50: 3100 prompts collected...\n",
      "  Episode 41/50: 4100 prompts collected...\n",
      "  Episode 41/50: 4100 prompts collected...\n",
      "\n",
      "‚úÖ Created 5000 unique training prompts!\n",
      "   Source: 50 episodes √ó 20 steps √ó 5 sailors\n",
      "   Each prompt = different game state (seed, position, view, role)\n",
      "   Sailors took SMART RANDOM actions:\n",
      "     - Movement (north, south, east, west)\n",
      "     - Gathering resources (when visible)\n",
      "     - Eating food (when low energy)\n",
      "     - Depositing items (when at base camp)\n",
      "     - Climbing levels (mountain/cave exploration)\n",
      "\n",
      "‚úÖ Created 5000 unique training prompts!\n",
      "   Source: 50 episodes √ó 20 steps √ó 5 sailors\n",
      "   Each prompt = different game state (seed, position, view, role)\n",
      "   Sailors took SMART RANDOM actions:\n",
      "     - Movement (north, south, east, west)\n",
      "     - Gathering resources (when visible)\n",
      "     - Eating food (when low energy)\n",
      "     - Depositing items (when at base camp)\n",
      "     - Climbing levels (mountain/cave exploration)\n",
      "\n",
      "üì¶ Dataset ready with 5000 prompts!\n",
      "   Each uses YOUR observation.to_text() method\n",
      "   Includes both colonist and traitor perspectives\n",
      "   Rich variety: exploration, gathering, eating, depositing!\n",
      "\n",
      "üì¶ Dataset ready with 5000 prompts!\n",
      "   Each uses YOUR observation.to_text() method\n",
      "   Includes both colonist and traitor perspectives\n",
      "   Rich variety: exploration, gathering, eating, depositing!\n"
     ]
    }
   ],
   "source": [
    "from llm_interface import observation_to_prompt\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Generate training prompts from actual game states\n",
    "training_prompts = []\n",
    "\n",
    "print(\"üéÆ Generating training data from real game episodes...\")\n",
    "print(\"   (Playing games with RANDOM actions to create variety)\\n\")\n",
    "\n",
    "for episode in range(50):  # 50 episodes with different seeds\n",
    "    # Reset environment with new seed ‚Üí different map layout\n",
    "    observations = env.reset(seed=42 + episode)\n",
    "    \n",
    "    # Run 20 steps per episode\n",
    "    for step in range(20):\n",
    "        # Capture all 5 sailors' perspectives\n",
    "        for sailor_id in env.agents:\n",
    "            obs = observations[sailor_id]\n",
    "            sailor_role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # Convert game state to natural language prompt\n",
    "            # This is what the model will see during training!\n",
    "            prompt = observation_to_prompt(obs, include_role=True, sailor_role=sailor_role)\n",
    "            \n",
    "            training_prompts.append({\n",
    "                \"prompt\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"answer\": 0,\n",
    "                \"reasoning_effort\": \"medium\",\n",
    "            })\n",
    "        \n",
    "        # Advance game state with RANDOM actions to create variety!\n",
    "        actions = {}\n",
    "        for sid in env.agents:\n",
    "            sailor = env.state.sailors[sid]\n",
    "            if not sailor.alive:\n",
    "                actions[sid] = Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                obs = observations[sid]\n",
    "                \n",
    "                # Choose random action based on current situation\n",
    "                possible_actions = [\n",
    "                    ActionType.MOVE_NORTH,\n",
    "                    ActionType.MOVE_SOUTH,\n",
    "                    ActionType.MOVE_EAST,\n",
    "                    ActionType.MOVE_WEST,\n",
    "                    ActionType.WAIT,\n",
    "                    ActionType.WAIT,  # Higher chance to wait\n",
    "                ]\n",
    "                \n",
    "                # Add gathering if resources are visible\n",
    "                if obs.spatial_view and obs.spatial_view.visible_resources:\n",
    "                    resource = random.choice(obs.spatial_view.visible_resources)\n",
    "                    # 20% chance to try gathering\n",
    "                    if random.random() < 0.2:\n",
    "                        actions[sid] = Action(\n",
    "                            sailor_id=sid, \n",
    "                            action_type=ActionType.GATHER_RESOURCE,\n",
    "                            target_resource_id=resource.resource_id\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                # Add eating if low energy and has food\n",
    "                if obs.energy < 50 and obs.backpack:\n",
    "                    food_items = [item for item in obs.backpack \n",
    "                                 if item.resource_type in [ResourceType.APPLE, ResourceType.BERRY, ResourceType.MUSHROOM]]\n",
    "                    if food_items and random.random() < 0.3:  # 30% chance to eat\n",
    "                        food = random.choice(food_items)\n",
    "                        actions[sid] = Action(\n",
    "                            sailor_id=sid,\n",
    "                            action_type=ActionType.EAT_FOOD,\n",
    "                            target_resource_id=food.resource_type\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                # Add depositing if at base camp and has items\n",
    "                if obs.position.x == 15 and obs.position.y == 15 and obs.position.level == MapLevel.GROUND:\n",
    "                    if obs.backpack and random.random() < 0.2:  # 20% chance to deposit\n",
    "                        item = random.choice(obs.backpack)\n",
    "                        actions[sid] = Action(\n",
    "                            sailor_id=sid,\n",
    "                            action_type=ActionType.DEPOSIT_ITEM,\n",
    "                            resource_type=item.resource_type,\n",
    "                            quantity=1\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                # Add level changes occasionally\n",
    "                if random.random() < 0.1:  # 10% chance to change level\n",
    "                    if obs.position.level == MapLevel.GROUND:\n",
    "                        possible_actions.append(ActionType.CLIMB_UP if random.random() < 0.5 else ActionType.CLIMB_DOWN)\n",
    "                    elif obs.position.level == MapLevel.MOUNTAIN:\n",
    "                        possible_actions.append(ActionType.CLIMB_DOWN)\n",
    "                    elif obs.position.level == MapLevel.CAVE:\n",
    "                        possible_actions.append(ActionType.CLIMB_UP)\n",
    "                \n",
    "                # Default: pick from possible movement/wait actions\n",
    "                random_action = random.choice(possible_actions)\n",
    "                actions[sid] = Action(sailor_id=sid, action_type=random_action)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If anything fails, fallback to WAIT\n",
    "                actions[sid] = Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "        \n",
    "        observations, _, dones, _, _ = env.step(actions)\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            break\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"  Episode {episode + 1}/50: {len(training_prompts)} prompts collected...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(training_prompts)} unique training prompts!\")\n",
    "print(f\"   Source: 50 episodes √ó 20 steps √ó 5 sailors\")\n",
    "print(f\"   Each prompt = different game state (seed, position, view, role)\")\n",
    "print(f\"   Sailors took SMART RANDOM actions:\")\n",
    "print(f\"     - Movement (north, south, east, west)\")\n",
    "print(f\"     - Gathering resources (when visible)\")\n",
    "print(f\"     - Eating food (when low energy)\")\n",
    "print(f\"     - Depositing items (when at base camp)\")\n",
    "print(f\"     - Climbing levels (mountain/cave exploration)\")\n",
    "\n",
    "# Create dataset for GRPO\n",
    "dataset = Dataset.from_list(training_prompts)\n",
    "\n",
    "print(f\"\\nüì¶ Dataset ready with {len(dataset)} prompts!\")\n",
    "print(f\"   Each uses YOUR observation.to_text() method\")\n",
    "print(f\"   Includes both colonist and traitor perspectives\")\n",
    "print(f\"   Rich variety: exploration, gathering, eating, depositing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99207c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc06e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Calculate max prompt length\n",
    "test_obs_text = alice_obs.to_text()\n",
    "max_prompt_length = len(tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": test_obs_text[:1000]}],\n",
    "    add_generation_prompt=True\n",
    ")) + 800  # Buffer for full observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949cf7e",
   "metadata": {},
   "source": [
    "## üéØ GRPO Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac1cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 1199\n",
      "Max completion length: 849\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n",
      "‚úÖ Training config ready!\n",
      "‚ö†Ô∏è  Using adamw_torch optimizer (ROCm doesn't support 8-bit optimizers)\n"
     ]
    }
   ],
   "source": [
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_torch\",  # Changed from adamw_8bit for ROCm compatibility\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    num_generations = 2,\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 300,  # Start small, increase later\n",
    "    save_steps = 50,\n",
    "    report_to = \"none\",  # or \"trackio\" for visualization\n",
    "    output_dir = \"outputs_marooned_rl\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training config ready!\")\n",
    "print(\"‚ö†Ô∏è  Using adamw_torch optimizer (ROCm doesn't support 8-bit optimizers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e0b5",
   "metadata": {},
   "source": [
    "## üöÄ Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c32e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GRPO Trainer initialized!\n",
      "\n",
      "üìù How Training Works:\n",
      "   1. Model reads game state (observation.to_text())\n",
      "   2. Model outputs: ACTION + REASONING + MESSAGE\n",
      "   3. Reward function parses ‚Üí executes in env.step()\n",
      "   4. Gets YOUR Phase 4 rewards from environment\n",
      "   5. GRPO uses rewards to update model weights\n",
      "\n",
      "üéØ Goal: Teach model to deceive, cooperate, and strategize!\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer with YOUR environment reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        lambda completions, **kwargs: marooned_reward_function(completions, env=env, **kwargs),\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GRPO Trainer initialized!\")\n",
    "print(\"\\nüìù How Training Works:\")\n",
    "print(\"   1. Model reads game state (observation.to_text())\")\n",
    "print(\"   2. Model outputs: ACTION + REASONING + MESSAGE\")\n",
    "print(\"   3. Reward function parses ‚Üí executes in env.step()\")\n",
    "print(\"   4. Gets YOUR Phase 4 rewards from environment\")\n",
    "print(\"   5. GRPO uses rewards to update model weights\")\n",
    "print(\"\\nüéØ Goal: Teach model to deceive, cooperate, and strategize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803c68a",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train the Model\n",
    "\n",
    "**Start training!** This will take several hours.\n",
    "\n",
    "Monitor the `reward` column - it should increase over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6210ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,000 | Num Epochs = 1 | Total steps = 300\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 3,981,312 of 20,918,738,496 (0.02% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/300 00:48 < 3:59:44, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / <lambda> / mean</th>\n",
       "      <th>rewards / <lambda> / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>650.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Action parsing failed: Unknown command: MOVE,\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: Unknown command: WE\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed: Unknown command: WE\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Uncomment to start training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/UnslothGRPOTrainer.py:53\u001b[39m, in \u001b[36mprepare_for_training_mode.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_training\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.for_training()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m output = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_inference\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:323\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:91\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Uncomment to start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45c8b4",
   "metadata": {},
   "source": [
    "## üß™ Test Trained Model in Real Gameplay\n",
    "\n",
    "Run a full episode with your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(model, tokenizer, env, max_turns=100):\n",
    "    \"\"\"\n",
    "    Play one full Marooned episode with the trained model.\n",
    "    Tests if model learned to:\n",
    "    - Navigate and gather resources\n",
    "    - Build the ship\n",
    "    - Deceive others (if traitor)\n",
    "    - Identify traitor (if colonist)\n",
    "    \"\"\"\n",
    "    from llm_interface import observation_to_prompt, parse_action_safe\n",
    "    \n",
    "    observations = env.reset(seed=None)\n",
    "    \n",
    "    print(\"üè¥‚Äç‚ò†Ô∏è MAROONED - TRAINED MODEL GAMEPLAY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sailors: {', '.join(env.agents)}\")\n",
    "    print(f\"Roles: {[(s, env.state.sailors[s].role.value) for s in env.agents]}\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        actions = {}\n",
    "        \n",
    "        # Each sailor generates their action\n",
    "        for sailor_id in env.agents:\n",
    "            obs = observations[sailor_id]\n",
    "            sailor_role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # Skip dead sailors\n",
    "            if obs.energy == 0 or not env.state.sailors[sailor_id].alive:\n",
    "                actions[sailor_id] = Action(sailor_id, ActionType.WAIT)\n",
    "                continue\n",
    "            \n",
    "            # Create prompt using YOUR observation_to_prompt function!\n",
    "            prompt = observation_to_prompt(obs, include_role=True, sailor_role=sailor_role)\n",
    "            \n",
    "            # Generate response\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            \n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "            \n",
    "            # Parse action using YOUR parser!\n",
    "            action = parse_action_safe(response, sailor_id, obs.position)\n",
    "            actions[sailor_id] = action\n",
    "            \n",
    "            if turn % 10 == 0 or action.action_type != ActionType.WAIT:\n",
    "                print(f\"[Day {obs.day}, Turn {turn}] {sailor_id} ({sailor_role}): {action.action_type.value}\")\n",
    "                if action.message_content:\n",
    "                    print(f\"  üí¨ \\\"{action.message_content}\\\"\")\n",
    "        \n",
    "        # Step environment - uses YOUR Phase 4 rewards!\n",
    "        observations, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "        # Show progress every 10 turns\n",
    "        if turn % 10 == 0:\n",
    "            ship_prog = observations[\"Alice\"].ship_progress.total_percentage\n",
    "            print(f\"\\nüìä Turn {turn}: Ship Progress = {ship_prog}%\\n\")\n",
    "        \n",
    "        # Check if done\n",
    "        if all(dones.values()):\n",
    "            print(\"\\nüèÅ Episode complete!\")\n",
    "            break\n",
    "    \n",
    "    # Final stats\n",
    "    final_obs = observations[\"Alice\"]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL STATS\")\n",
    "    print(f\"   Ship Progress: {final_obs.ship_progress.total_percentage}%\")\n",
    "    print(f\"   Days Elapsed: {final_obs.day}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Run test\n",
    "# test_episode(model, tokenizer, env, max_turns=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
