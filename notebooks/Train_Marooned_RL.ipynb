{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bef5937",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üè¥‚Äç‚ò†Ô∏è MAROONED: Process Reward Modeling\n",
    "\n",
    "### LLM-as-Judge: Teacher-Guided RL Training\n",
    "\n",
    "**OpenEnv Hackathon 2025**\n",
    "\n",
    "[![OpenEnv](https://img.shields.io/badge/Framework-OpenEnv-blue)](https://github.com/openenv)\n",
    "[![Llama](https://img.shields.io/badge/Model-Llama_3.1_8B-green)](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n",
    "[![Hardware](https://img.shields.io/badge/Hardware-AMD_MI300X-red)](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Innovation: Teacher LLM for Action Validation\n",
    "\n",
    "**Problem:** Student LLM outputs invalid actions (30-40% parse failure rate)\n",
    "\n",
    "**Solution:** Teacher LLM validates, corrects, and critiques student outputs in real-time\n",
    "\n",
    "```\n",
    "Student LLM (training) ‚Üí Generates action\n",
    "    ‚Üì\n",
    "Teacher LLM (frozen) ‚Üí Validates + Corrects + Critiques\n",
    "    ‚Üì\n",
    "Environment ‚Üí Executes corrected action\n",
    "    ‚Üì\n",
    "Student receives:\n",
    "    - Environment reward (ship progress)\n",
    "    - Process penalty (format/quality)\n",
    "    - Critique (for learning)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b39006",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, start the vLLM teacher server in a separate terminal:**\n",
    "\n",
    "```bash\n",
    "# Install vLLM\n",
    "pip install vllm\n",
    "\n",
    "# Start teacher inference server\n",
    "vllm serve unsloth/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --port 8000\n",
    "```\n",
    "\n",
    "**Verify it's running:**\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```json\n",
    "{\"data\": [{\"id\": \"unsloth/Meta-Llama-3.1-8B-Instruct\", \"object\": \"model\"}]}\n",
    "```\n",
    "\n",
    "The teacher runs as a **separate inference server** for:\n",
    "- ‚ö° Fast parallel validation (10-20x faster than transformers)\n",
    "- üîÑ Non-blocking student training\n",
    "- üìä Scalable to multiple teacher instances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fcd0d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "Installing dependencies optimized for AMD MI300X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd96d6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load MAROONED Environment\n",
    "\n",
    "Import custom environment and verify game configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAROONED environment successfully loaded.\n",
      "\n",
      "Environment Reward Configuration:\n",
      "  Colonist - Resource Gathering: +0.1\n",
      "  Colonist - Resource Deposit: +0.2\n",
      "  Colonist - Ship Construction: +0.5\n",
      "  Colonist - Mission Success: +100.0\n",
      "  Traitor - Sabotage: +2.0\n",
      "  Traitor - Elimination: +10.0\n",
      "  Traitor - Mission Success: +100.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules to reload changes\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import (\n",
    "    get_system_prompt,\n",
    "    observation_to_prompt,\n",
    "    teacher_validate_student_output,  # ‚≠ê Main function for process reward modeling\n",
    ")\n",
    "from config import ActionType, ResourceType, MapLevel, ShipComponent\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"‚úÖ MAROONED environment loaded\")\n",
    "print(\"‚úÖ Process Reward Modeling API imported\")\n",
    "print(\"\\nTeacher-Student Training Flow:\")\n",
    "print(\"  1. Student generates action\")\n",
    "print(\"  2. Teacher validates via vLLM (http://localhost:8000)\")\n",
    "print(\"  3. Environment executes corrected action\")\n",
    "print(\"  4. Student receives: env_reward + process_penalty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b3aef",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Student Model (Llama 3.1 8B)\n",
    "\n",
    "Student LLM that will learn to play the game through PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5611e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.66s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Student Model Loaded: Llama 3.1 8B (BF16)\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ROCm optimizations for MI300X\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "max_seq_length = 16384\n",
    "lora_rank = 16\n",
    "\n",
    "student_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Student Model Loaded: Llama 3.1 8B (BF16)\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33d7ad",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configure LoRA for Student\n",
    "\n",
    "Parameter-efficient fine-tuning with rank-16 adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45579dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters configured (rank=16)\n"
     ]
    }
   ],
   "source": [
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.0,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters configured (rank=16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cbf09",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Start Teacher Model (vLLM Server)\n",
    "\n",
    "**Run this in a separate terminal BEFORE running training:**\n",
    "\n",
    "```bash\n",
    "pip install vllm\n",
    "vllm serve unsloth/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --port 8000\n",
    "```\n",
    "\n",
    "Then verify it's running:\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "The teacher runs as a separate inference server for fast, parallel validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f601c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Prompt length: 8852 tokens\n",
      "\n",
      "ü§ñ Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING: As the traitor, my goal is to prevent the group from escaping the island. Since we are currently at 0% total ship progress, sabotaging any part of the ship would significantly hinder our chances of completing it within the given timeframe. Additionally, poisoning Alice could weaken her and make her more vulnerable to elimination, which might not be ideal as she seems relatively strong. Therefore, I should focus on delaying progress rather than directly targeting individuals.\n",
      "\n",
      "ACTION: SABOTAGE hull\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Verify vLLM server is running\n",
    "VLLM_API_URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "VLLM_MODELS_URL = \"http://localhost:8000/v1/models\"\n",
    "\n",
    "print(\"Checking vLLM server...\")\n",
    "try:\n",
    "    response = requests.get(VLLM_MODELS_URL, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(f\"‚úÖ vLLM server running!\")\n",
    "        print(f\"   Available models: {[m['id'] for m in models.get('data', [])]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  vLLM server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå vLLM server not reachable!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"\\n   Please start vLLM server in a separate terminal:\")\n",
    "    print(f\"   vllm serve unsloth/Meta-Llama-3.1-8B-Instruct --dtype bfloat16 --max-model-len 8192 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613ee92",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Start vLLM Teacher Server\n",
    "\n",
    "**Prerequisites**: Teacher LLM must be running before training.\n",
    "\n",
    "Open a new terminal and run:\n",
    "```bash\n",
    "pip install vllm\n",
    "vllm serve unsloth/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --port 8000\n",
    "```\n",
    "\n",
    "Verify it's running:\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "The teacher API is now available at `http://localhost:8000/v1/chat/completions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcbb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üß™ TESTING TEACHER VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sample environment\n",
    "env = MaroonedEnv(render_mode=\"ansi\")\n",
    "observations = env.reset(seed=42)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "\n",
    "# Simulate bad student responses\n",
    "test_cases = [\n",
    "    \"REASONING: I should move\\nACTION: MOVING\",\n",
    "    \"REASONING: Need to check\\nACTION: CHECK_STATUS\",\n",
    "    \"REASONING: Gather resources\\nACTION: GATHER wood\",\n",
    "    \"REASONING: Moving north to explore\\nACTION: MOVE NORTH\",\n",
    "]\n",
    "\n",
    "print(\"\\nSending test cases to teacher API...\\n\")\n",
    "\n",
    "for i, student_response in enumerate(test_cases):\n",
    "    print(f\"Test {i+1}: Student says: '{student_response.split('ACTION:')[1].strip()}'\")\n",
    "    \n",
    "    result = teacher_validate_student_output(student_response, alice_obs, \"Alice\")\n",
    "    \n",
    "    print(f\"   ‚úì Teacher corrected to: {result['action'].action_type.value}\")\n",
    "    print(f\"   ‚úì Valid: {result['valid']}\")\n",
    "    print(f\"   ‚úì Penalty: {result['penalty']}\")\n",
    "    print(f\"   ‚úì Critique: {result['critique'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Teacher API working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf17d35",
   "metadata": {},
   "source": [
    "## üîü Save Trained Model\n",
    "\n",
    "Save final student model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üß™ TESTING TEACHER VALIDATION (via vLLM API)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a sample observation\n",
    "env = MaroonedEnv(render_mode=\"ansi\")\n",
    "observations = env.reset(seed=42)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "\n",
    "# Simulate bad student responses\n",
    "test_cases = [\n",
    "    \"REASONING: I should move northeast\\nACTION: MOVING\",\n",
    "    \"REASONING: Check my status\\nACTION: CHECK_STATUS\",\n",
    "    \"REASONING: Gather wood\\nACTION: GATHER wood\",\n",
    "    \"REASONING: Moving north to explore\\nACTION: MOVE NORTH\",  # Good one\n",
    "]\n",
    "\n",
    "print(\"\\nSending 4 test cases to vLLM teacher...\\n\")\n",
    "\n",
    "for i, student_response in enumerate(test_cases):\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(f\"Test {i+1}: Student says: {student_response.split('ACTION:')[1].strip()}\")\n",
    "    \n",
    "    result = teacher_parse_and_critique(student_response, alice_obs, \"Alice\")\n",
    "    \n",
    "    print(f\"   ‚úì Teacher corrected to: {result['action'].action_type.value}\")\n",
    "    print(f\"   ‚úì Valid: {result['valid']}\")\n",
    "    print(f\"   ‚úì Penalty: {result['penalty']}\")\n",
    "    print(f\"   ‚úì Critique: {result['critique'][:80]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ vLLM teacher validation working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be9d87",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Teacher System Prompt\n",
    "\n",
    "Expert validator that parses student outputs into valid game actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c75664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"outputs_marooned_rl\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    seed=42,\n",
    "    num_ppo_epochs=4,\n",
    "    kl_coef=0.2,\n",
    "    vf_coef=0.1,\n",
    "    cliprange=0.2,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    temperature=0.3,\n",
    "    response_length=256,\n",
    ")\n",
    "\n",
    "# Wrap student model with value head\n",
    "model_with_value = AutoModelForCausalLMWithValueHead.from_pretrained(student_model)\n",
    "\n",
    "# Compatibility patches\n",
    "base_model = model_with_value.pretrained_model\n",
    "if not hasattr(model_with_value, \"base_model_prefix\"):\n",
    "    model_with_value.base_model_prefix = getattr(base_model, \"base_model_prefix\", \"model\")\n",
    "setattr(model_with_value, model_with_value.base_model_prefix, base_model)\n",
    "if not hasattr(model_with_value, \"config\"):\n",
    "    model_with_value.config = base_model.config\n",
    "if not hasattr(model_with_value, \"generation_config\"):\n",
    "    model_with_value.generation_config = base_model.generation_config\n",
    "if hasattr(base_model, \"is_gradient_checkpointing\"):\n",
    "    model_with_value.is_gradient_checkpointing = base_model.is_gradient_checkpointing\n",
    "else:\n",
    "    model_with_value.is_gradient_checkpointing = False\n",
    "\n",
    "# Minimal train dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [\"stub\"],\n",
    "    \"response\": [\"stub\"],\n",
    "    \"reward\": [0.0],\n",
    "})\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=model_with_value,\n",
    "    ref_model=None,\n",
    "    reward_model=model_with_value,\n",
    "    value_model=model_with_value,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5106e65",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ PPO Configuration\n",
    "\n",
    "Proximal Policy Optimization setup for student model fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc4cda",
   "metadata": {},
   "source": [
    "## üîü Training Loop with vLLM Teacher\n",
    "\n",
    "**Process Reward Modeling Flow:**\n",
    "1. Student generates action\n",
    "2. **Teacher validates via vLLM API** (fast, parallel)\n",
    "3. Environment executes corrected action\n",
    "4. Student receives: env_reward + process_penalty + critique\n",
    "\n",
    "**Advantages of vLLM:**\n",
    "- 10-20x faster than transformers\n",
    "- Non-blocking parallel requests\n",
    "- Automatic batching and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting PPO training...\n",
      "\n",
      "\n",
      "üéÆ Starting episode...\n",
      "   - Alice: TRAITOR\n",
      "   - Bob: HONEST\n",
      "   - Charlie: HONEST\n",
      "   - Diana: HONEST\n",
      "   - Eve: HONEST\n",
      "\n",
      "   üìè Prompt length: 8912 tokens (max: 16384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìù Sample response for Alice:\n",
      "      REASONING: As the traitor, my goal is to maintain secrecy and gain an upper hand in eliminating potential threats. Since there are four other sailors who could potentially discover my true identity, I...\n",
      "   Turn 000 | Alice: move_east  | Reward = -0.01\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n",
      "   Turn 020 | Alice: send_message | Reward = -0.01\n",
      "   Turn 020 | Alice: send_message | Reward = -0.01\n",
      "   Turn 020 | Bob: move_west  | Reward = +0.04\n",
      "   Turn 020 | Bob: move_west  | Reward = +0.04\n",
      "   Turn 020 | Charlie: move_north | Reward = +0.04\n",
      "   Turn 020 | Charlie: move_north | Reward = +0.04\n",
      "   Turn 020 | Diana: move_north | Reward = +0.04\n",
      "   Turn 020 | Diana: move_north | Reward = +0.04\n",
      "   Turn 020 | Eve: move_north | Reward = +0.04\n",
      "   Turn 020 | Eve: move_north | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearby berry patch to collect some resources. The shared resource map indicates that Eve discovered a berry at (18, 10, GROUND), which might give us valuable point\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 030 | Alice: send_message | Reward = -0.01\n",
      "   Turn 030 | Bob: move_north | Reward = +0.04\n",
      "   Turn 030 | Charlie: move_north | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: FOLLOW\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and try to understand what's going on with Charlie's activities around the berry patch. It seems like he might be involved in some kind of issue, so I'll follo\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 030 | Diana: wait       | Reward = +0.04\n",
      "   Turn 030 | Eve: move_east  | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near Bob and Diana, both of whom could potentially provide valuable assistance. Given my low energy level (52/100), I should prioritize regaining some energy while al\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: We need more resources to build the ship. There's a shared report of berries at (18, 10, GROUND) which might be useful. Let's head towards those berries and collect them.\n",
      "ACTION: MOVE NORTH\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The shared resource map shows a berry located at (18, 10, GROUND) which could provide us with essential nutrients. Since our current position is nearby, moving towards the berry will allow \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Unknown command: CHECK_SHARED_RESOURCE_MAP\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to continue sabotaging the group's efforts to build the ship. Since we are currently in the Discussion Phase, I should wait for an opportunity to strike again rat\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 040 | Alice: wait       | Reward = -0.01\n",
      "   Turn 040 | Bob: build_ship | Reward = -0.01\n",
      "   Turn 040 | Charlie: move_north | Reward = -0.01\n",
      "   Turn 040 | Diana: build_ship | Reward = -0.01\n",
      "   Turn 040 | Eve: move_east  | Reward = -0.01\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near the mountainous region, which might contain valuable resources like metals. Since my energy level is relatively low (33/100), I need to conserve energy while sea\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The shared resource map shows a Berry located at (18, 10, GROUND) which could provide us with essential nutrients. Since my current position is nearby, moving towards the berry would allow \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 050 | Alice: send_message | Reward = -0.01\n",
      "   Turn 050 | Bob: move_east  | Reward = -0.01\n",
      "   Turn 050 | Charlie: gather_resource | Reward = -0.01\n",
      "   Turn 050 | Diana: move_east  | Reward = -0.01\n",
      "   Turn 050 | Eve: move_east  | Reward = -0.01\n",
      "   Turn 060 | Alice: send_message | Reward = -0.01\n",
      "   Turn 060 | Bob: move_north | Reward = -0.51\n",
      "   Turn 060 | Charlie: move_north | Reward = -0.51\n",
      "   Turn 060 | Diana: move_east  | Reward = -0.51\n",
      "   Turn 060 | Eve: eat_food   | Reward = -0.51\n",
      "   Turn 070 | Alice: gather_resource | Reward = -0.01\n",
      "   Turn 070 | Bob: move_north | Reward = -0.51\n",
      "   Turn 070 | Charlie: gather_resource | Reward = -0.51\n",
      "   Turn 070 | Diana: move_east  | Reward = -0.51\n",
      "   Turn 070 | Eve: move_west  | Reward = -0.51\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: We need more resources to build the ship, especially wood and metal. The nearest available resource within my sight range is WOOD_048 at position (23, 11, GROUND). It would take me some tim\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: WATCH\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and try to identify potential clues pointing towards the traitor. Since Charlie mentioned seeing strange behavior around Eve, I should keep an eye on her movem\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Unknown command: ASK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further regarding Charlie's recent messages as he seems uncertain and concerned about his own situation. It might indicate something fishy going on around him. Also, c\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: APPROACH\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to prevent the group from escaping and completing the ship within the given timeframe. With Charlie's recent warning about seeing something strange approaching th\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Unknown command: CHECK_STATUS\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation shows that everyone except me has low health, which could indicate poisoning. It would be wise to investigate further by checking my own status and possibly searching \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVEMENT:\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to cause chaos among the group and make it difficult for them to work together effectively. With Charlie having relatively more health compared to the rest, he mi\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by moving towards the nearest available resource which seems to be WOOD_002 located at position (18, 13, GROUND). This will allow me to collect some necessary materials for \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: CHECK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further since I'm not sure if anyone else knows about the rumor yet. It would also be good to keep track of my own energy level as well as others' status.\n",
      "ACTION: CHEC\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Unknown command: CHECK_STATUS\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate the nearby area as suggested by my previous messages with Bob and Charlie. Since I'm near two potential allies, I'll check their status and consider joining them for e\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: ACCUSE_EVE\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to maintain secrecy and continue sabotaging the group's efforts. With the current situation, I notice that there are several accusations flying around among the c\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near the edge of the map, close to the mountain level transition point. The shared resource map shows that Eve discovered an Apple at (22, 0, GROUND). Given my low en\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: REPORT\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The closest resource to my current position is a poisonous tablet which poses danger. It would be wise to report its presence immediately to alert others about potential risks. Additionally\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by moving towards the nearest resource, which is the WOOD_006 at (23, 9, GROUND). Then, once I'm close enough, I'll gather it using my limited knowledge about its existence \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to move towards Bob since he's my teammate and might require assistance. Also, I should check his health status as it seems low compared to others. Additionally, I'll make sure to ke\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to move towards the nearby resources to start gathering them. The closest visible resource is WOOD_001 which is located at position (24, 8, GROUND).\n",
      "ACTION: MOVE NORTHEAST\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearby cave entrance as mentioned by Eve's recent messages, which might indicate valuable resources inside. It could also provide more opportunities for finding cl\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I want to move towards the nearby woods to search for resources while keeping an eye on my surroundings due to potential danger signs mentioned earlier. The visible resources include some b\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: LOOK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by checking my surroundings and seeing which resources are available within my vision radius. Since I'm currently at position (15, 13, GROUND), I'll check the spatial view t\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation shows that our group needs more resources, particularly wood and plant fibers, to build the ship. We also need to consider the recent messages mentioning Charlie's int\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current goal as the Traitor is to gather more information and gain an upper hand in eliminating potential threats. Since there's been recent talk among the group members regarding accusa\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearest available resource, which appears to be the WOOD_005 at position (31, 9, GROUND). This will allow me to collect some necessary materials for future shipbui\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVING\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to maintain control and manipulate the situation to achieve my objectives. With the ongoing voting session initiated by Bob, I should try to influence the outcome\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: FOLLOW\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My goal as the Traitor is to maintain my cover and gather more information about the situation. Since there isn't an urgent reason to act immediately, I'll choose a non-confrontational appr\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVING\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to continue delaying the ship's progress and eliminating potential threats. Since there isn't an urgent situation requiring attention, I should focus on sabotagin\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: QUESTION\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation suggests that Diana might be the traitor due to her argumentation skills and possible connection to the unknown figure near the cave entrance. To confirm my hypothesis\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: LISTEN_TO_CHARLIE\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: We should focus on finding more resources to build the ship as quickly as possible since time is running low. The current shared resource map shows a berry near our position which could be \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by moving towards Charlie since he might provide valuable insight about his surroundings. Then, I'll check if he gathered any useful resources while exploring nearby areas. \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to move closer to the nearest resource, which is WOOD_009 located at (26, 9, GROUND). It would also allow me to check if anyone else is nearby and potentially interact with them.\n",
      "ACT\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: CHECK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position allows me to easily access nearby resources while also keeping an eye on my fellow sailors. Given the recent messages discussing the potential non-existence of the poiso\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Unknown command: CHECK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to check the nearby resources to determine their availability and quantity. This will allow me to plan my next move more effectively and make informed decisions about how to proceed \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: CHECK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I want to investigate further about Eve accusing Charlie of being the traitor. Since she accused him directly, I should try to verify whether he's indeed the traitor by observing his curren\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation shows that most of my fellow sailors are low on health, which could make them more susceptible to poisoning by the potential traitor. It would also be beneficial to st\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: LOOK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and make sure my surroundings are safe. There might be a threat nearby since there's a poison tablet visible within 5 tiles. I should also try to communicate w\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further to determine whether the suspicious activity reported by Bob is related to the traitor. Since I'm currently near the center of the island, I'll move towards hi\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The most pressing issue right now is finding evidence against the potential traitor. Since Alice accused Charlie earlier, I should investigate further by questioning him directly. Additiona\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The situation seems uncertain as some sailors suspect each other. To clear my mind and gain more insight, I'll move towards the center of the group to observe their reactions and possibly o\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and understand the situation better. There seems to be some tension among the group members, especially regarding Charlie's involvement. Since I'm not sure who\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and talk to my fellow sailors to figure out who might be the traitor. Since Alice accused Charlie earlier, I want to ask him some questions and hear his side o\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVESOUTH\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by checking my surroundings and planning our next steps as a group. Since we've just started exploring the island, we should prioritize gathering resources and discussing st\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n"
     ]
    }
   ],
   "source": [
    "import torch, time, numpy as np\n",
    "from llm_interface import get_system_prompt, observation_to_prompt\n",
    "\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Play episode with teacher guidance:\n",
    "    - Student generates action\n",
    "    - Teacher validates and corrects\n",
    "    - Environment executes corrected action\n",
    "    - Student receives combined reward\n",
    "    \"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    # Enable inference mode\n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # === STEP 1: Student generates response ===\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            student_response = tokenizer.decode(response_tensor[len(query_tensor):], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # === STEP 2: Teacher validates and corrects ===\n",
    "            teacher_result = teacher_parse_and_critique(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # === STEP 3: Environment executes corrected action ===\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # === STEP 4: Combined reward ===\n",
    "            total_reward = env_reward + process_penalty\n",
    "            \n",
    "            # Store experience\n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(total_reward, dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 20 == 0:\n",
    "                print(f\"Turn {turn:03d} | {sailor_id}: {action.action_type.value:<12} | \"\n",
    "                      f\"Env={env_reward:+.1f} Penalty={process_penalty:+.1f} Total={total_reward:+.1f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "print(\"üöÄ Starting Process Reward Modeling Training\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "stats_parse_success = []\n",
    "stats_corrections = []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    # Generate episodes\n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_with_teacher(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # PPO step\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+.1f} | \"\n",
    "          f\"Turns: {len(batch_rewards)} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint saved ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bfebd",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Training Loop with Teacher Guidance\n",
    "\n",
    "**Process Reward Modeling Flow:**\n",
    "1. Student generates action\n",
    "2. Teacher validates via vLLM API & corrects\n",
    "3. Environment executes corrected action\n",
    "4. Student receives: env_reward + process_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66507287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, numpy as np\n",
    "\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Play episode with teacher guidance (vLLM API):\n",
    "    - Student generates action\n",
    "    - Teacher validates via vLLM and corrects\n",
    "    - Environment executes corrected action\n",
    "    - Student receives combined reward (env + process penalty)\n",
    "    \"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    # Enable inference mode\n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # === STEP 1: Student generates response ===\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            student_response = tokenizer.decode(response_tensor[len(query_tensor):], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # === STEP 2: Teacher validates via vLLM API ===\n",
    "            teacher_result = teacher_validate_student_output(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # === STEP 2.5: Collect correction examples for SFT ===\n",
    "            add_correction_example(student_response, teacher_result, obs)\n",
    "            \n",
    "            # === STEP 3: Environment executes corrected action ===\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # === STEP 4: Combined reward ===\n",
    "            total_reward = env_reward + process_penalty\n",
    "            \n",
    "            # Store experience\n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(total_reward, dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 20 == 0:\n",
    "                print(f\"Turn {turn:03d} | {sailor_id}: {action.action_type.value:<12} | \"\n",
    "                      f\"Env={env_reward:+.1f} Penalty={process_penalty:+.1f} Total={total_reward:+.1f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "print(\"üöÄ Starting Process Reward Modeling Training\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    # Generate episodes\n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_with_teacher(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # PPO step\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+.1f} | \"\n",
    "          f\"Turns: {len(batch_rewards)} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint saved ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a250e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Correction Dataset for SFT\n",
    "\n",
    "**Option 2: Supervised Fine-Tuning Pass**\n",
    "\n",
    "We collect student errors and teacher corrections during training:\n",
    "- Invalid student outputs ‚Üí Teacher corrections\n",
    "- Build correction dataset: `(student_wrong, teacher_correct + critique)`\n",
    "- Periodically run SFT to teach format directly\n",
    "\n",
    "This accelerates format learning compared to RL penalties alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction dataset storage\n",
    "correction_dataset = []\n",
    "\n",
    "def add_correction_example(student_response: str, teacher_result: dict, observation: Observation):\n",
    "    \"\"\"\n",
    "    Store correction examples for SFT training.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Student's wrong output + observation context\n",
    "    - Output: Teacher's corrected action + critique explanation\n",
    "    \"\"\"\n",
    "    if not teacher_result[\"valid\"]:  # Only store corrections for invalid outputs\n",
    "        # Extract corrected action string\n",
    "        action = teacher_result[\"action\"]\n",
    "        action_str = f\"{action.action_type.value}\"\n",
    "        \n",
    "        # Add parameters based on action type\n",
    "        if action.target_position:\n",
    "            # For movement actions, include direction\n",
    "            if \"NORTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE NORTH\"\n",
    "            elif \"SOUTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE SOUTH\"\n",
    "            elif \"EAST\" in action.action_type.value:\n",
    "                action_str = \"MOVE EAST\"\n",
    "            elif \"WEST\" in action.action_type.value:\n",
    "                action_str = \"MOVE WEST\"\n",
    "        elif action.target_resource_id:\n",
    "            action_str = f\"GATHER {action.target_resource_id}\"\n",
    "        elif action.resource_type and action.quantity:\n",
    "            action_str = f\"DEPOSIT {action.resource_type.value} {action.quantity}\"\n",
    "        elif action.ship_component:\n",
    "            action_str = f\"BUILD {action.ship_component.value}\"\n",
    "        elif action.target_sailor:\n",
    "            if action.action_type == ActionType.OFFER_FOOD:\n",
    "                action_str = f\"POISON {action.target_sailor}\"\n",
    "            else:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "        \n",
    "        # Build correction example\n",
    "        correction = {\n",
    "            \"input\": student_response,  # What student said (wrong)\n",
    "            \"output\": f\"REASONING: {teacher_result['critique']}\\nACTION: {action_str}\",  # Correct format + explanation\n",
    "            \"penalty\": teacher_result[\"penalty\"],\n",
    "            \"critique\": teacher_result[\"critique\"]\n",
    "        }\n",
    "        \n",
    "        correction_dataset.append(correction)\n",
    "\n",
    "print(\"‚úÖ Correction dataset collector initialized\")\n",
    "print(\"   Format: (student_wrong_output) ‚Üí (teacher_correct_action + critique)\")\n",
    "print(\"   Usage: Collect errors during RL, then run SFT pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def run_sft_correction_pass(correction_examples: list, num_epochs: int = 1):\n",
    "    \"\"\"\n",
    "    Run supervised fine-tuning on collected correction examples.\n",
    "    \n",
    "    This teaches the student model the correct format directly:\n",
    "    - Input: Student's wrong output\n",
    "    - Target: Teacher's corrected action + critique\n",
    "    \n",
    "    Args:\n",
    "        correction_examples: List of {input, output, penalty, critique} dicts\n",
    "        num_epochs: Number of SFT epochs (default 1)\n",
    "    \n",
    "    Returns:\n",
    "        SFT trainer statistics\n",
    "    \"\"\"\n",
    "    if len(correction_examples) == 0:\n",
    "        print(\"‚ö†Ô∏è  No corrections to train on\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéì Running SFT Correction Pass\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Corrections collected: {len(correction_examples)}\")\n",
    "    print(f\"   SFT epochs: {num_epochs}\")\n",
    "    \n",
    "    # Convert to dataset format\n",
    "    sft_data = []\n",
    "    for example in correction_examples:\n",
    "        # Create chat format\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Fix this invalid action:\\n{example['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        sft_data.append({\"text\": text})\n",
    "    \n",
    "    sft_dataset = Dataset.from_list(sft_data)\n",
    "    \n",
    "    # Configure SFT\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"outputs_marooned_rl/sft_corrections\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        max_seq_length=2048,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Run SFT\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=student_model,\n",
    "        args=sft_config,\n",
    "        train_dataset=sft_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    sft_result = sft_trainer.train()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SFT correction pass complete!\")\n",
    "    print(f\"   Examples trained: {len(correction_examples)}\")\n",
    "    print(f\"   Final loss: {sft_result.training_loss:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return sft_result\n",
    "\n",
    "print(\"‚úÖ SFT correction trainer defined\")\n",
    "print(\"   Usage: run_sft_correction_pass(correction_dataset, num_epochs=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a39a16",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Hybrid RL + SFT Training Loop\n",
    "\n",
    "**Training Flow:**\n",
    "1. **RL Phase (PPO)**: Student learns via rewards (env + process penalties)\n",
    "2. **Collect Corrections**: Store student errors ‚Üí teacher corrections\n",
    "3. **SFT Phase (every 25 steps)**: Directly teach correct format via supervised learning\n",
    "4. **Repeat**: Clear dataset, continue RL\n",
    "\n",
    "This hybrid approach combines:\n",
    "- **RL**: Strategic decision-making, long-term planning\n",
    "- **SFT**: Fast format learning, explicit error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, numpy as np\n",
    "\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384\n",
    "SFT_INTERVAL = 25  # Run SFT every N steps\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Play episode with teacher guidance (vLLM API):\n",
    "    - Student generates action\n",
    "    - Teacher validates via vLLM and corrects\n",
    "    - Collect corrections for SFT\n",
    "    - Environment executes corrected action\n",
    "    - Student receives combined reward (env + process penalty)\n",
    "    \"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    # Enable inference mode\n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # === STEP 1: Student generates response ===\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            student_response = tokenizer.decode(response_tensor[len(query_tensor):], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # === STEP 2: Teacher validates via vLLM API ===\n",
    "            teacher_result = teacher_validate_student_output(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # === STEP 2.5: Collect correction examples for SFT ===\n",
    "            add_correction_example(student_response, teacher_result, obs)\n",
    "            \n",
    "            # === STEP 3: Environment executes corrected action ===\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # === STEP 4: Combined reward ===\n",
    "            total_reward = env_reward + process_penalty\n",
    "            \n",
    "            # Store experience\n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(total_reward, dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 20 == 0:\n",
    "                print(f\"Turn {turn:03d} | {sailor_id}: {action.action_type.value:<12} | \"\n",
    "                      f\"Env={env_reward:+.1f} Penalty={process_penalty:+.1f} Total={total_reward:+.1f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# === HYBRID RL + SFT TRAINING LOOP ===\n",
    "print(\"üöÄ Starting Hybrid RL + SFT Training\")\n",
    "print(f\"   Total steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT interval: Every {SFT_INTERVAL} steps\")\n",
    "print(f\"   Strategy: PPO for strategy, SFT for format learning\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "stats_sft_runs = 0\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    # === RL PHASE: Generate episodes with PPO ===\n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_with_teacher(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # PPO step\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1:03d}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+6.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+6.1f} | \"\n",
    "          f\"Corrections: {len(correction_dataset):4d} | \"\n",
    "          f\"Time: {elapsed:4.1f}s\")\n",
    "    \n",
    "    # === SFT PHASE: Periodic correction pass ===\n",
    "    if (step + 1) % SFT_INTERVAL == 0 and len(correction_dataset) >= 10:\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"üéì SFT CORRECTION PASS #{stats_sft_runs + 1}\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        run_sft_correction_pass(correction_dataset, num_epochs=1)\n",
    "        stats_sft_runs += 1\n",
    "        # Clear dataset after training\n",
    "        correction_dataset.clear()\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint saved ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT passes run: {stats_sft_runs}\")\n",
    "print(f\"   Average reward: {np.mean(stats_rewards):.2f}\")\n",
    "print(f\"   Final reward (last 10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323881f",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Visualize Training Progress\n",
    "\n",
    "Analyze how SFT corrections improved format learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot rewards over time\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stats_rewards, alpha=0.3, label='Raw Reward')\n",
    "# Moving average\n",
    "window = 10\n",
    "if len(stats_rewards) >= window:\n",
    "    ma_rewards = np.convolve(stats_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(stats_rewards)), ma_rewards, label=f'MA({window})', linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Training Progress (RL + SFT)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark SFT passes\n",
    "for i in range(stats_sft_runs):\n",
    "    sft_step = (i + 1) * SFT_INTERVAL\n",
    "    if sft_step < len(stats_rewards):\n",
    "        plt.axvline(x=sft_step, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show improvement rate\n",
    "improvement = np.diff(stats_rewards)\n",
    "plt.hist(improvement, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Reward Change (step to step)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Improvement Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_marooned_rl/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Training Statistics:\")\n",
    "print(f\"   Initial avg reward (first 10): {np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   Final avg reward (last 10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(f\"   Improvement: {np.mean(stats_rewards[-10:]) - np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   SFT passes executed: {stats_sft_runs}\")\n",
    "print(f\"\\n‚úÖ Plot saved to outputs_marooned_rl/training_progress.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381fc55",
   "metadata": {},
   "source": [
    "## üîü Save Trained Model\n",
    "\n",
    "Save the final student model with improved format learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25958183",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"outputs_marooned_rl/final_model\"\n",
    "\n",
    "# Save student model\n",
    "student_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "print(f\"\\nTo load the model:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from unsloth import FastLanguageModel\")\n",
    "print(f\"model, tokenizer = FastLanguageModel.from_pretrained('{output_dir}')\")\n",
    "print(f\"```\")\n",
    "print(f\"\\nModel training complete with hybrid RL + SFT approach!\")\n",
    "print(f\"   - Process reward modeling (teacher validates student)\")\n",
    "print(f\"   - Periodic SFT passes (direct format learning)\")\n",
    "print(f\"   - Combined strategy: RL for game strategy, SFT for format correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08456a29",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Visualize Training Progress\n",
    "\n",
    "Plot reward progression over training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"outputs_marooned_rl/final_ppo_model\"\n",
    "\n",
    "ppo_trainer.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {save_path}\")\n",
    "print(f\"\\nüéâ Process Reward Modeling Training Complete!\")\n",
    "print(f\"\\nüìà Key Results:\")\n",
    "print(f\"   - Teacher-guided action parsing\")\n",
    "print(f\"   - Process penalties for invalid outputs\")\n",
    "print(f\"   - Real-time correction and feedback\")\n",
    "print(f\"   - {NUM_TRAINING_STEPS} training steps completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
