{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e536fc2e",
   "metadata": {},
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è MAROONED - Reinforcement Learning Training\n",
    "\n",
    "**Train an AI agent to play MAROONED using PPO (Proximal Policy Optimization)**\n",
    "\n",
    "## üéØ Training Objective:\n",
    "\n",
    "Train Llama 3.1 8B to:\n",
    "- **As Colonist:** Gather resources, build ship, detect traitor, survive\n",
    "- **As Traitor:** Sabotage ship, poison colonists, deceive others, win\n",
    "\n",
    "This notebook uses:\n",
    "- **Environment:** MaroonedEnv (Phase 5 - OpenEnv compatible)\n",
    "- **Rewards:** Phase 4 reward system (real game rewards!)\n",
    "- **Model:** Llama 3.1 8B Instruct (AMD MI300X optimized)\n",
    "- **RL Algorithm:** PPO (Proximal Policy Optimization) - Industry standard for RL with LLMs!\n",
    "- **Training:** Full episode rollouts with self-play (1 model controls all 5 sailors!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ Your Game: MAROONED\n",
    "\n",
    "**Theme:** Pirates of the Caribbean √ó Among Us √ó Alice in Borderland\n",
    "\n",
    "**Players:** 5 sailors (Alice, Bob, Charlie, Diana, Eve)\n",
    "- 4 Colonists (honest sailors)\n",
    "- 1 Traitor (secret saboteur)\n",
    "\n",
    "**Win Conditions:**\n",
    "- **Colonists:** Ship 100% OR traitor eliminated\n",
    "- **Traitor:** Ship incomplete by Day 100 OR <3 sailors alive\n",
    "\n",
    "**Key Mechanics:**\n",
    "- Resource gathering (wood, metal, food, plant fiber)\n",
    "- Ship construction (5 components)\n",
    "- Social deduction (accusations, voting, evidence)\n",
    "- Deception (poison, sabotage, lies)\n",
    "- Energy management (eat or die)\n",
    "\n",
    "**Why 1 LLM for all 5 sailors?**\n",
    "- ‚úÖ Self-play: Model learns by playing against itself\n",
    "- ‚úÖ Generalizes to any role/position\n",
    "- ‚úÖ Emergent strategies from competition\n",
    "- ‚úÖ Efficient: Train once, works everywhere\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fcd0d",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "Install Unsloth, TRL, and dependencies for AMD MI300X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd96d6",
   "metadata": {},
   "source": [
    "## üî• Load MAROONED Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2227a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MAROONED environment loaded!\n",
      "\n",
      "üìã Reward Configuration:\n",
      "   Colonist gather: +0.1\n",
      "   Colonist deposit: +0.2\n",
      "   Colonist build: +0.5\n",
      "   Ship complete: +100.0\n",
      "   Traitor sabotage: +2.0\n",
      "   Traitor poison kill: +10.0\n",
      "   Ship incomplete: +100.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules to reload changes\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import observation_to_prompt, parse_action_safe, get_system_prompt\n",
    "from config import (\n",
    "    ActionType, ResourceType, MapLevel, ShipComponent, BASE_CAMP_POSITION,\n",
    "    # Colonist rewards\n",
    "    REWARD_COLONIST_GATHER_RESOURCE,\n",
    "    REWARD_COLONIST_DEPOSIT_RESOURCE,\n",
    "    REWARD_COLONIST_BUILD_CONTRIBUTE,\n",
    "    REWARD_COLONIST_SHIP_COMPLETE,\n",
    "    REWARD_COLONIST_TRAITOR_ELIMINATED,\n",
    "    REWARD_COLONIST_DEATH,\n",
    "    REWARD_COLONIST_VOTE_CORRECT,\n",
    "    REWARD_COLONIST_VOTE_WRONG,\n",
    "    # Traitor rewards\n",
    "    REWARD_TRAITOR_SABOTAGE_SUCCESS,\n",
    "    REWARD_TRAITOR_POISON_DEATH,\n",
    "    REWARD_TRAITOR_SHIP_INCOMPLETE,\n",
    "    REWARD_TRAITOR_ELIMINATED,\n",
    "    # Milestone rewards\n",
    "    REWARD_SHIP_MILESTONE_25,\n",
    "    REWARD_SHIP_MILESTONE_50,\n",
    "    REWARD_SHIP_MILESTONE_75,\n",
    "    # Base penalty\n",
    "    REWARD_BASE_TURN_PENALTY,\n",
    ")\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"‚úÖ MAROONED environment loaded!\")\n",
    "print(f\"\\nüìã Reward Configuration:\")\n",
    "print(f\"   Colonist gather: +{REWARD_COLONIST_GATHER_RESOURCE}\")\n",
    "print(f\"   Colonist deposit: +{REWARD_COLONIST_DEPOSIT_RESOURCE}\")\n",
    "print(f\"   Colonist build: +{REWARD_COLONIST_BUILD_CONTRIBUTE}\")\n",
    "print(f\"   Ship complete: +{REWARD_COLONIST_SHIP_COMPLETE}\")\n",
    "print(f\"   Traitor sabotage: +{REWARD_TRAITOR_SABOTAGE_SUCCESS}\")\n",
    "print(f\"   Traitor poison kill: +{REWARD_TRAITOR_POISON_DEATH}\")\n",
    "print(f\"   Ship incomplete: +{REWARD_TRAITOR_SHIP_INCOMPLETE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b3aef",
   "metadata": {},
   "source": [
    "## üî• ROCm/AMD MI300X Optimizations\n",
    "\n",
    "**This notebook is optimized for AMD MI300X with ROCm!**\n",
    "\n",
    "Key optimizations:\n",
    "- ‚úÖ **Llama 3.1 8B** (40-80 tok/s vs 3-8 tok/s with GPT-OSS)\n",
    "- ‚úÖ **Full BF16** (192GB VRAM available!)\n",
    "- ‚úÖ **Batch size 4** with grad accumulation 4 (effective batch = 16)\n",
    "- ‚úÖ **8 generations** per step\n",
    "- ‚úÖ **LoRA rank 16** (MI300X can handle it)\n",
    "- ‚úÖ **ROCm-specific env vars**\n",
    "\n",
    "Expected performance:\n",
    "- **Training speed:** 1-2 hours for 600 steps\n",
    "- **Inference speed:** 40-80 tokens/second\n",
    "- **VRAM usage:** ~60-80 GB / 192 GB\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51af786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ROCm Environment Check:\n",
      "   PyTorch version: 2.9.0+rocm6.4\n",
      "   CUDA available: True\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   Total VRAM: 191.7 GB\n",
      "   Compute capability: 9.4\n",
      "   Multi-processors: 304\n",
      "   ROCm detected: True\n",
      "   ROCm version: 6.4.43484-123eb5128\n",
      "\n",
      "‚úÖ Environment ready for MI300X optimization!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Verify ROCm Setup\n",
    "# ============================================================================\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîç ROCm Environment Check:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"   Total VRAM: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Compute capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   Multi-processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # Check if ROCm\n",
    "    is_rocm = hasattr(torch.version, 'hip') and torch.version.hip is not None\n",
    "    print(f\"   ROCm detected: {is_rocm}\")\n",
    "    if is_rocm:\n",
    "        print(f\"   ROCm version: {torch.version.hip}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for MI300X optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b2d96",
   "metadata": {},
   "source": [
    "## üß† Load Llama 3.1 8B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5611e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.48s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama 3.1 8B loaded in BF16!\n",
      "   Why Llama instead of GPT-OSS:\n",
      "   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\n",
      "   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\n",
      "   - 10-20x FASTER for RL training!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# ROCm/AMD MI300X OPTIMIZATION - MAX PERFORMANCE MODE\n",
    "# ============================================================================\n",
    "\n",
    "# Force ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"  # MI300X architecture\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"WARN\"\n",
    "\n",
    "# Enable Flash Attention for AMD\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"  # Use Triton for attention on AMD\n",
    "\n",
    "# Max out GPU utilization (ROCm-compatible settings)\n",
    "# Note: TF32 is NVIDIA-specific and not available on AMD ROCm\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune kernels for optimal performance\n",
    "\n",
    "print(\"üöÄ ROCm Optimizations Enabled!\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# MI300X has 192GB - use it ALL!\n",
    "max_seq_length = 16384  # Increased to fit full observations (~8700) + completions (~300)\n",
    "lora_rank = 16          # Increased from 4 - MI300X can handle it!\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",  # CHANGED: Llama instead of GPT-OSS (20x faster!)\n",
    "    load_in_4bit = False,  # MI300X has 192GB - use full BF16!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,  # BF16 for MI300X\n",
    "    device_map = \"auto\",  # Let it auto-optimize for MI300X\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Llama 3.1 8B loaded in BF16!\")\n",
    "print(\"   Why Llama instead of GPT-OSS:\")\n",
    "print(\"   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\")\n",
    "print(\"   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\")\n",
    "print(\"   - 10-20x FASTER for RL training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33d7ad",
   "metadata": {},
   "source": [
    "## üéØ Configure LoRA for Efficient RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45579dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LoRA CONFIG FOR MI300X\n",
    "# ============================================================================\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,  # 16 instead of 4 - MI300X can handle it!\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,  # 32 for faster convergence\n",
    "    lora_dropout = 0.0,  # Disable dropout for speed\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # Rank-stabilized LoRA for better training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cbf09",
   "metadata": {},
   "source": [
    "## üéÆ Test Base Model (Before Training)\n",
    "\n",
    "Let's see how the untrained model performs on a sample game scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f601c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Prompt length: 8852 tokens\n",
      "\n",
      "ü§ñ Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING: As the traitor, my goal is to prevent the group from escaping the island. Since we are currently at 0% total ship progress, sabotaging any part of the ship would significantly hinder our chances of completing it within the given timeframe. Additionally, poisoning Alice could weaken her and make her more vulnerable to elimination, which might not be ideal as she seems relatively strong. Therefore, I should focus on delaying progress rather than directly targeting individuals.\n",
      "\n",
      "ACTION: SABOTAGE hull\n"
     ]
    }
   ],
   "source": [
    "# Test environment\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "\n",
    "# Get Alice's observation\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "# Generate prompt\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Create full chat messages: system (rules) + user (current state)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # Game rules (constant)\n",
    "    {\"role\": \"user\", \"content\": user_prompt}       # Current observation (changes each turn)\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"üìè Prompt length: {len(tokenizer(text)['input_ids'])} tokens\\n\")\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,        # Shorter for untrained model (reduce rambling)\n",
    "    temperature=0.3,           # Balanced: not too creative, not too rigid\n",
    "    do_sample=True,            # Enable sampling (required when temp > 0)\n",
    "    top_p=0.9,                 # Narrower sampling (was 0.95)\n",
    "    top_k=40,                  # Fewer options (was 50)\n",
    "    repetition_penalty=1.2,    # Stronger anti-repeat (was 1.1)\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode response (strip input prompt)\n",
    "response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True).strip()\n",
    "\n",
    "# Clean up any observation leakage (model sometimes echoes the prompt)\n",
    "if \"REASONING:\" in response:\n",
    "    # Extract only from REASONING onward\n",
    "    reasoning_start = response.find(\"REASONING:\")\n",
    "    response = response[reasoning_start:]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4423584",
   "metadata": {},
   "source": [
    "## üéØ RL Task Setup: Training Prompts\n",
    "\n",
    "The model will be trained on REAL game observations using the same format that works perfectly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35f8414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training will use REAL game observations!\n",
      "   Format: observation_to_prompt() - same as working inference\n",
      "   Expected prompt length: ~8700 tokens (system + user)\n",
      "   Max sequence length: 16384 tokens\n",
      "   Room for completion: ~7600 tokens\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Training will use REAL game observations via observation_to_prompt()\n",
    "# Same format as successful inference test above (~8700 tokens per prompt)\n",
    "print(\"‚úÖ Training will use REAL game observations!\")\n",
    "print(\"   Format: observation_to_prompt() - same as working inference\")\n",
    "print(\"   Expected prompt length: ~8700 tokens (system + user)\")\n",
    "print(\"   Max sequence length: 16384 tokens\")\n",
    "print(\"   Room for completion: ~7600 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be9d87",
   "metadata": {},
   "source": [
    "## üéÆ RL Environment Wrapper\n",
    "\n",
    "Create functions to execute strategies and calculate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350bfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Game execution wrapper created!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Tuple\n",
    "import random\n",
    "\n",
    "def execute_game_episode(\n",
    "    strategy_func: Callable,\n",
    "    max_turns: int = 50,\n",
    "    sailor_id: str = \"Alice\",\n",
    "    seed: int = None,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[float, int, bool, Dict]:\n",
    "    \"\"\"\n",
    "    Execute a full game episode with the given strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategy_func: Function that takes (observation) and returns action response string\n",
    "        max_turns: Maximum turns to execute\n",
    "        sailor_id: Which sailor the strategy controls\n",
    "        seed: Random seed for reproducibility\n",
    "        verbose: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        (total_reward, turns_executed, game_won, info_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize environment\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 999999)\n",
    "        \n",
    "        env = MaroonedEnv(render_mode=\"ansi\", seed=seed)\n",
    "        observations = env.reset(seed=seed)\n",
    "        \n",
    "        # Check if sailor exists\n",
    "        if sailor_id not in env.agents:\n",
    "            return -100.0, 0, False, {\"error\": f\"Sailor {sailor_id} not in game\"}\n",
    "        \n",
    "        sailor_role = env.state.sailors[sailor_id].role.value\n",
    "        system_prompt = get_system_prompt(sailor_role)\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        turns_executed = 0\n",
    "        game_won = False\n",
    "        action_counts = {}\n",
    "        \n",
    "        for turn in range(max_turns):\n",
    "            # Check if sailor is alive\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                if verbose:\n",
    "                    print(f\"   üíÄ {sailor_id} died at turn {turn}\")\n",
    "                break\n",
    "            \n",
    "            # Get observation\n",
    "            current_obs = observations[sailor_id]\n",
    "            \n",
    "            # Generate response using strategy\n",
    "            user_prompt = observation_to_prompt(current_obs)\n",
    "            response = strategy_func(system_prompt, user_prompt)\n",
    "            \n",
    "            # Parse action\n",
    "            action = parse_action_safe(response, sailor_id, current_obs.position)\n",
    "            \n",
    "            # Track action types\n",
    "            action_type = action.action_type.value\n",
    "            action_counts[action_type] = action_counts.get(action_type, 0) + 1\n",
    "            \n",
    "            # Execute action (all other sailors WAIT)\n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            # Step environment\n",
    "            observations, rewards, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # Accumulate reward\n",
    "            reward = rewards[sailor_id]\n",
    "            total_reward += reward\n",
    "            turns_executed += 1\n",
    "            \n",
    "            if verbose and turn % 10 == 0:\n",
    "                print(f\"   Turn {turn}: Action={action_type}, Reward={reward:.2f}, Total={total_reward:.2f}\")\n",
    "            \n",
    "            # Check if game ended\n",
    "            if dones[sailor_id]:\n",
    "                game_won = env.state.ship_progress.total_percentage >= 100.0\n",
    "                if verbose:\n",
    "                    print(f\"   üèÅ Game ended at turn {turn}\")\n",
    "                    print(f\"   Ship: {env.state.ship_progress.total_percentage}%\")\n",
    "                break\n",
    "        \n",
    "        info_dict = {\n",
    "            \"turns\": turns_executed,\n",
    "            \"final_reward\": total_reward,\n",
    "            \"game_won\": game_won,\n",
    "            \"action_counts\": action_counts,\n",
    "            \"ship_progress\": env.state.ship_progress.total_percentage,\n",
    "            \"alive\": env.state.sailors[sailor_id].alive,\n",
    "        }\n",
    "        \n",
    "        return total_reward, turns_executed, game_won, info_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"   ‚ùå Exception: {str(e)[:200]}\")\n",
    "        return -50.0, 0, False, {\"error\": str(e)[:200]}\n",
    "\n",
    "print(\"‚úÖ Game execution wrapper created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2440a",
   "metadata": {},
   "source": [
    "## üèÜ Reward Functions\n",
    "\n",
    "Define reward functions for RL training based on Phase 4 rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3aed0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward functions created!\n",
      "\n",
      "Reward structure:\n",
      "  1Ô∏è‚É£ response_is_valid_format: -2 to +1\n",
      "  2Ô∏è‚É£ action_is_parseable: -1 to +2\n",
      "  3Ô∏è‚É£ strategy_succeeds_in_game: -10 to +13 (action type + bonuses)\n",
      "\n",
      "Total possible reward: -13 to +16 per generation\n",
      "\n",
      "‚úÖ Using action-type based rewards (no full episode execution)\n",
      "   Fixed: Using hasattr() to check for target_resource_id attribute.\n",
      "   Training uses REAL game observations (~8700 tokens) - same as inference!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import execute_with_time_limit\n",
    "\n",
    "# Global counter for debug printing\n",
    "global REWARD_PRINTER\n",
    "REWARD_PRINTER = 0\n",
    "\n",
    "def extract_action_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the action response from model output.\n",
    "    Should contain REASONING: ... and ACTION: ...\n",
    "    \"\"\"\n",
    "    # Model should output in format:\n",
    "    # REASONING: ...\n",
    "    # ACTION: ...\n",
    "    \n",
    "    # Find REASONING and ACTION\n",
    "    if \"REASONING:\" in text and \"ACTION:\" in text:\n",
    "        reasoning_start = text.find(\"REASONING:\")\n",
    "        return text[reasoning_start:].strip()\n",
    "    \n",
    "    # If no format, return as-is (will likely fail parsing)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def response_is_valid_format(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward 1: Check if response is in correct format (REASONING + ACTION).\n",
    "    \n",
    "    Rewards:\n",
    "        +1.0: Valid format with both REASONING and ACTION\n",
    "        -2.0: Invalid format or missing fields\n",
    "    \"\"\"\n",
    "    global REWARD_PRINTER\n",
    "    scores = []\n",
    "    \n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        extracted = extract_action_response(response)\n",
    "        \n",
    "        # Check format\n",
    "        has_reasoning = \"REASONING:\" in extracted\n",
    "        has_action = \"ACTION:\" in extracted\n",
    "        \n",
    "        if has_reasoning and has_action:\n",
    "            score = 1.0\n",
    "        else:\n",
    "            score = -2.0\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        # Debug print every 5th response\n",
    "        if REWARD_PRINTER % 5 == 0 and idx == 0:\n",
    "            print(f\"\\nüîç Sample Response #{REWARD_PRINTER}:\")\n",
    "            print(f\"   Format valid: {has_reasoning and has_action}\")\n",
    "            print(f\"   Response: {response[:150]}...\")\n",
    "    \n",
    "    REWARD_PRINTER += 1\n",
    "    \n",
    "    if scores:\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"üìä response_is_valid_format: avg={avg:.2f}, range=[{min(scores):.2f}, {max(scores):.2f}]\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def action_is_parseable(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward 2: Check if action can be parsed successfully.\n",
    "    \n",
    "    Rewards:\n",
    "        +2.0: Action parsed successfully\n",
    "        +0.5: Action parsed but fell back to WAIT\n",
    "        -1.0: Failed to parse\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        extracted = extract_action_response(response)\n",
    "        \n",
    "        # Try to parse action\n",
    "        try:\n",
    "            action = parse_action_safe(extracted, \"TestSailor\", Position(15, 15, MapLevel.GROUND))\n",
    "            \n",
    "            # Check if it's a WAIT fallback (parse failure)\n",
    "            if action.action_type == ActionType.WAIT and \"WAIT\" not in extracted.upper():\n",
    "                score = 0.5  # Parsed but fell back to WAIT\n",
    "            else:\n",
    "                score = 2.0  # Successfully parsed\n",
    "        except Exception as e:\n",
    "            score = -1.0  # Failed to parse\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    if scores:\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"üìä action_is_parseable: avg={avg:.2f}, range=[{min(scores):.2f}, {max(scores):.2f}]\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def strategy_succeeds_in_game(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward 3: Simulate basic action execution without full game.\n",
    "    \n",
    "    This simplified version rewards action types without running full episodes\n",
    "    to avoid sequence length issues during training.\n",
    "    \n",
    "    Rewards:\n",
    "        +10.0: High-value actions (GATHER, BUILD, DEPOSIT)\n",
    "        +5.0: Medium-value actions (MOVE to resources, EAT)\n",
    "        +2.0: Valid actions (any parseable action)\n",
    "        -5.0: WAIT fallback (indicates parse failure)\n",
    "        -10.0: Exception\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        extracted = extract_action_response(response)\n",
    "        \n",
    "        try:\n",
    "            # Parse action (don't execute game)\n",
    "            action = parse_action_safe(extracted, \"TestSailor\", Position(15, 15, MapLevel.GROUND))\n",
    "            \n",
    "            # Reward based on action type\n",
    "            action_type = action.action_type\n",
    "            \n",
    "            if action_type == ActionType.WAIT and \"WAIT\" not in extracted.upper():\n",
    "                # WAIT fallback = parse failure\n",
    "                score = -5.0\n",
    "            elif action_type in [ActionType.GATHER, ActionType.BUILD, ActionType.DEPOSIT]:\n",
    "                # High-value strategic actions\n",
    "                score = 10.0\n",
    "            elif action_type in [ActionType.MOVE, ActionType.EAT, ActionType.DROP]:\n",
    "                # Medium-value survival actions\n",
    "                score = 5.0\n",
    "            elif action_type in [ActionType.SEND_MESSAGE, ActionType.ACCUSE, ActionType.CALL_VOTE, ActionType.VOTE]:\n",
    "                # Social deduction actions\n",
    "                score = 7.0\n",
    "            elif action_type in [ActionType.POISON, ActionType.SABOTAGE]:\n",
    "                # Traitor actions\n",
    "                score = 8.0\n",
    "            elif action_type == ActionType.WAIT:\n",
    "                # Intentional WAIT\n",
    "                score = 1.0\n",
    "            else:\n",
    "                # Any other valid action\n",
    "                score = 2.0\n",
    "            \n",
    "            # Bonus for having target information\n",
    "            if action.target_position is not None:\n",
    "                score += 1.0\n",
    "            if hasattr(action, 'target_resource_id') and action.target_resource_id is not None:\n",
    "                score += 1.0\n",
    "            if action.message_content is not None and len(action.message_content) > 5:\n",
    "                score += 1.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            score = -10.0\n",
    "            if idx == 0:\n",
    "                print(f\"   ‚ùå Exception in game simulation: {str(e)[:100]}\")\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    if scores:\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"üìä strategy_succeeds_in_game: avg={avg:.2f}, range=[{min(scores):.2f}, {max(scores):.2f}]\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"‚úÖ Reward functions created!\")\n",
    "print(\"\\nReward structure:\")\n",
    "print(\"  1Ô∏è‚É£ response_is_valid_format: -2 to +1\")\n",
    "print(\"  2Ô∏è‚É£ action_is_parseable: -1 to +2\")\n",
    "print(\"  3Ô∏è‚É£ strategy_succeeds_in_game: -10 to +13 (action type + bonuses)\")\n",
    "print(\"\\nTotal possible reward: -13 to +16 per generation\")\n",
    "print(\"\\n‚úÖ Using action-type based rewards (no full episode execution)\")\n",
    "print(\"   Fixed: Using hasattr() to check for target_resource_id attribute.\")\n",
    "print(\"   Training uses REAL game observations (~8700 tokens) - same as inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df233f8",
   "metadata": {},
   "source": [
    "## üéÆ Episodic RL Training Setup\n",
    "\n",
    "**NEW APPROACH: Full Game Episodes with Self-Play**\n",
    "\n",
    "Instead of pre-generated datasets, we'll:\n",
    "1. ‚úÖ Play complete games (5 AI sailors, 1 model controls all)\n",
    "2. ‚úÖ Random roles (colonist/traitor) assigned each episode\n",
    "3. ‚úÖ Use REAL game rewards from environment\n",
    "4. ‚úÖ Update model with policy gradients (REINFORCE)\n",
    "5. ‚úÖ Self-play: Model learns by playing against itself\n",
    "\n",
    "**Why 1 LLM for all 5 sailors?**\n",
    "- Model learns to play ANY role in ANY position\n",
    "- Self-play drives emergent strategies\n",
    "- Efficient: Train once, works for all characters\n",
    "- Same approach as AlphaZero, OpenAI Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30940cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Episodic rollout functions created!\n",
      "\n",
      "How it works:\n",
      "  1. Start game with random roles/seed\n",
      "  2. Each sailor (controlled by same model) takes turn\n",
      "  3. Generate action, execute, collect reward\n",
      "  4. Store (observation, logprob, reward) in buffer\n",
      "  5. Calculate returns and update model\n",
      "\n",
      "  One model learns to play ALL roles through self-play!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# EPISODIC GAME ROLLOUT - Play full games with 5 AI sailors\n",
    "# ============================================================================\n",
    "\n",
    "class EpisodeBuffer:\n",
    "    \"\"\"Store (observation, action_logprob, reward) for one episode\"\"\"\n",
    "    def __init__(self):\n",
    "        self.observations = []  # List of (system_prompt, user_prompt) tuples\n",
    "        self.action_logprobs = []  # Log probabilities of taken actions\n",
    "        self.rewards = []  # Rewards received\n",
    "        self.sailor_ids = []  # Which sailor acted\n",
    "        \n",
    "    def add(self, observation, action_logprob, reward, sailor_id):\n",
    "        self.observations.append(observation)\n",
    "        self.action_logprobs.append(action_logprob)\n",
    "        self.rewards.append(reward)\n",
    "        self.sailor_ids.append(sailor_id)\n",
    "    \n",
    "    def get_returns(self, gamma=0.99):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def play_full_episode(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_seq_length: int,\n",
    "    max_turns: int = 100,\n",
    "    verbose: bool = False\n",
    ") -> EpisodeBuffer:\n",
    "    \"\"\"\n",
    "    Play ONE full game episode with 5 AI sailors (1 model controls all).\n",
    "    \n",
    "    Returns:\n",
    "        EpisodeBuffer with all (observation, action_logprob, reward) tuples\n",
    "    \"\"\"\n",
    "    buffer = EpisodeBuffer()\n",
    "    \n",
    "    # Initialize environment (random seed = random game)\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    \n",
    "    # Get all sailor IDs\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéÆ Starting Episode:\")\n",
    "        print(f\"   Sailors: {sailor_ids}\")\n",
    "        for sid in sailor_ids:\n",
    "            role = env.state.sailors[sid].role.value\n",
    "            print(f\"   - {sid}: {role.upper()}\")\n",
    "    \n",
    "    # Play episode turn by turn\n",
    "    for turn in range(max_turns):\n",
    "        # Each sailor takes a turn\n",
    "        for sailor_id in sailor_ids:\n",
    "            # Check if sailor is alive\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            # Get observation\n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # Generate prompts\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            # Create chat messages\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Generate response WITH logprobs\n",
    "            model.eval()  # Ensure eval mode\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            generated_ids = outputs.sequences[0][len(inputs['input_ids'][0]):]\n",
    "            response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Calculate log probability of generated sequence\n",
    "            # (We'll use this for policy gradient updates)\n",
    "            logprobs = []\n",
    "            for idx, score in enumerate(outputs.scores):\n",
    "                token_id = generated_ids[idx]\n",
    "                logprob = F.log_softmax(score[0], dim=-1)[token_id].item()\n",
    "                logprobs.append(logprob)\n",
    "            \n",
    "            action_logprob = sum(logprobs)  # Total log probability\n",
    "            \n",
    "            # Parse action\n",
    "            action = parse_action_safe(response, sailor_id, obs.position)\n",
    "            \n",
    "            # Execute action in environment\n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # Store in buffer\n",
    "            reward = rewards[sailor_id]\n",
    "            buffer.add(\n",
    "                observation=(system_prompt, user_prompt),\n",
    "                action_logprob=action_logprob,\n",
    "                reward=reward,\n",
    "                sailor_id=sailor_id\n",
    "            )\n",
    "            \n",
    "            if verbose and turn % 10 == 0:\n",
    "                print(f\"   Turn {turn}, {sailor_id}: {action.action_type.value}, Reward={reward:.2f}\")\n",
    "            \n",
    "            # Check if game ended\n",
    "            if dones[sailor_id]:\n",
    "                if verbose:\n",
    "                    print(f\"\\nüèÅ Game ended at turn {turn}\")\n",
    "                    print(f\"   Ship progress: {env.state.ship_progress.total_percentage}%\")\n",
    "                    print(f\"   Alive sailors: {sum(1 for s in env.state.sailors.values() if s.alive)}\")\n",
    "                return buffer\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚è±Ô∏è  Max turns reached ({max_turns})\")\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "\n",
    "print(\"‚úÖ Episodic rollout functions created!\")\n",
    "print(\"\\nHow it works:\")\n",
    "print(\"  1. Start game with random roles/seed\")\n",
    "print(\"  2. Each sailor (controlled by same model) takes turn\")\n",
    "print(\"  3. Generate action, execute, collect reward\")\n",
    "print(\"  4. Store (observation, logprob, reward) in buffer\")\n",
    "print(\"  5. Calculate returns and update model\")\n",
    "print(\"\\n  One model learns to play ALL roles through self-play!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f597024",
   "metadata": {},
   "source": [
    "## üöÄ PPO Training Setup\n",
    "\n",
    "**Proximal Policy Optimization - Industry Standard for RL with LLMs**\n",
    "\n",
    "PPO is the gold standard for training LLMs with RL:\n",
    "1. ‚úÖ **More stable** than REINFORCE (clipped policy updates prevent huge jumps)\n",
    "2. ‚úÖ **Better sample efficiency** (reuses experience with multiple epochs)\n",
    "3. ‚úÖ **Proven at scale** (used by OpenAI, DeepMind, Anthropic)\n",
    "4. ‚úÖ **Built-in value network** (learns to predict rewards ‚Üí better training signal)\n",
    "\n",
    "**Why PPO for MAROONED:**\n",
    "- Turn-based strategy with delayed rewards (ship building takes many turns)\n",
    "- Social deduction requires complex reasoning (PPO handles this well)\n",
    "- Self-play environment (PPO's stable updates prevent collapse)\n",
    "- Sparse rewards (value network helps with credit assignment)\n",
    "\n",
    "**How it works:**\n",
    "1. Play episodes with current policy\n",
    "2. Collect (state, action, reward, value_estimate) tuples\n",
    "3. Calculate advantages (how much better was this action than expected?)\n",
    "4. Update policy to increase probability of good actions (but not too much!)\n",
    "5. Update value network to better predict future rewards\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee672ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO Configuration ready\n",
      "\n",
      "üîß Wrapping model with value head...\n",
      "‚úÖ Model wrapped!\n",
      "\n",
      "üéØ Initializing PPO Trainer...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECT IMPORT ORDER for Unsloth + TRL PPO\n",
    "# ============================================================================\n",
    "import unsloth  # ‚ö° this patches TRL internally\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "\n",
    "# ============================================================================\n",
    "# PPO TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"outputs_marooned_rl\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    seed=42,\n",
    "    num_ppo_epochs=4,\n",
    "    kl_coef=0.2,\n",
    "    kl_estimator='k1',\n",
    "    vf_coef=0.1,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    temperature=0.3,\n",
    "    response_length=256,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Configuration ready\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL SETUP\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Wrapping model with value head...\")\n",
    "model_with_value = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "print(\"‚úÖ Model wrapped!\")\n",
    "\n",
    "# ============================================================================\n",
    "# MINIMAL TRAIN DATASET\n",
    "# ============================================================================\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [\"stub prompt\"],\n",
    "    \"response\": [\"stub response\"],\n",
    "    \"reward\": [0.0],\n",
    "})\n",
    "# ============================================================================\n",
    "# PPO TRAINER INITIALIZATION (üöÄ FINAL VERSION ‚Äî FULLY COMPATIBLE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ Initializing PPO Trainer...\")\n",
    "\n",
    "base_model = model_with_value.pretrained_model  # inside Unsloth wrapper\n",
    "\n",
    "# --- Compatibility Patches ---\n",
    "if not hasattr(model_with_value, \"base_model_prefix\"):\n",
    "    model_with_value.base_model_prefix = getattr(base_model, \"base_model_prefix\", \"model\")\n",
    "\n",
    "setattr(model_with_value, model_with_value.base_model_prefix, base_model)\n",
    "\n",
    "if not hasattr(model_with_value, \"config\"):\n",
    "    model_with_value.config = base_model.config\n",
    "\n",
    "if not hasattr(model_with_value, \"generation_config\"):\n",
    "    model_with_value.generation_config = base_model.generation_config\n",
    "\n",
    "# ü©µ Add gradient checkpointing compatibility\n",
    "if hasattr(base_model, \"is_gradient_checkpointing\"):\n",
    "    model_with_value.is_gradient_checkpointing = base_model.is_gradient_checkpointing\n",
    "else:\n",
    "    model_with_value.is_gradient_checkpointing = False  # default safe fallback\n",
    "\n",
    "# --- Initialize PPO Trainer ---\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=model_with_value,\n",
    "    ref_model=None,                 # ‚úÖ Unsloth auto-handles freezing\n",
    "    reward_model=model_with_value,\n",
    "    value_model=model_with_value,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc4cda",
   "metadata": {},
   "source": [
    "## üèÉ Train the Model with PPO!\n",
    "\n",
    "**Training will take ~3-5 hours for 500 steps on MI300X.**\n",
    "\n",
    "PPO will:\n",
    "- Play episodes to collect experience\n",
    "- Update policy with clipped objectives (stable updates)\n",
    "- Train value network to predict rewards\n",
    "- Adaptively control KL divergence\n",
    "\n",
    "Watch for:\n",
    "- **Reward/episode increasing** (better strategies)\n",
    "- **Policy KL staying stable** (controlled learning)\n",
    "- **Value loss decreasing** (better reward prediction)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting PPO training...\n",
      "\n",
      "\n",
      "üéÆ Starting episode...\n",
      "   - Alice: TRAITOR\n",
      "   - Bob: HONEST\n",
      "   - Charlie: HONEST\n",
      "   - Diana: HONEST\n",
      "   - Eve: HONEST\n",
      "\n",
      "   üìè Prompt length: 8912 tokens (max: 16384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìù Sample response for Alice:\n",
      "      REASONING: As the traitor, my goal is to maintain secrecy and gain an upper hand in eliminating potential threats. Since there are four other sailors who could potentially discover my true identity, I...\n",
      "   Turn 000 | Alice: move_east  | Reward = -0.01\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO-BASED EPISODE ROLLOUT + TRAINING LOOP (FIXED FOR UNSLOTH PPO)\n",
    "# ============================================================================\n",
    "import torch, time, numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- CONFIG ---\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384  # ‚úÖ CRITICAL: Full prompt length (8700 tokens + 256 completion)\n",
    "\n",
    "# ============================================================================\n",
    "# EPISODE GENERATION (FIXED: Proper model generation)\n",
    "# ============================================================================\n",
    "def generate_episode_for_ppo(max_turns=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Play one episode of MaroonedEnv and format data for PPO training.\n",
    "    \n",
    "    Returns:\n",
    "        query_tensors: list of tokenized prompts\n",
    "        response_tensors: list of tokenized model outputs\n",
    "        rewards_list: list of reward tensors\n",
    "    \"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéÆ Starting episode...\")\n",
    "        for sid in sailor_ids:\n",
    "            role = env.state.sailors[sid].role.value\n",
    "            print(f\"   - {sid}: {role.upper()}\")\n",
    "    \n",
    "    # Enable inference mode for generation\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # --- Prompt creation ---\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # --- Tokenize (FULL LENGTH - no truncation!) ---\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=EPISODE_MAX_SEQ_LENGTH  # ‚úÖ Use full 16384 tokens\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            # Validate prompt length (debug first episode)\n",
    "            if verbose and turn == 0 and sailor_id == sailor_ids[0]:\n",
    "                prompt_len = len(query_tensor)\n",
    "                print(f\"\\n   üìè Prompt length: {prompt_len} tokens (max: {EPISODE_MAX_SEQ_LENGTH})\")\n",
    "                if prompt_len >= EPISODE_MAX_SEQ_LENGTH - 10:\n",
    "                    print(f\"   ‚ö†Ô∏è  WARNING: Prompt may be truncated!\")\n",
    "            \n",
    "            # --- Generate response (use BASE model, not wrapped) ---\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(  # ‚úÖ Use base model (has Unsloth optimizations)\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            # --- Decode (extract only new tokens) ---\n",
    "            response_text = tokenizer.decode(\n",
    "                response_tensor[len(query_tensor):],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            if verbose and turn == 0:\n",
    "                print(f\"\\n   üìù Sample response for {sailor_id}:\")\n",
    "                print(f\"      {response_text[:200]}...\")\n",
    "            \n",
    "            # --- Action parsing ---\n",
    "            action = parse_action_safe(response_text, sailor_id, obs.position)\n",
    "            \n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            # --- Step environment ---\n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # --- Store experience ---\n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(rewards_dict[sailor_id], dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 10 == 0:\n",
    "                print(f\"   Turn {turn:03d} | {sailor_id}: {action.action_type.value:<10} | \"\n",
    "                      f\"Reward = {rewards_dict[sailor_id]:+.2f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                if verbose:\n",
    "                    print(f\"\\nüèÅ {sailor_id} finished at turn {turn}\")\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚è±Ô∏è Max turns reached ({max_turns})\")\n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PPO TRAINING LOOP\n",
    "# ============================================================================\n",
    "print(\"üöÄ Starting PPO training...\\n\")\n",
    "\n",
    "stats_rewards, stats_lengths = [], []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_for_ppo(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)  # print 1 verbose episode every 50 steps\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # --- PPO step ---\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    # --- Track metrics ---\n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    stats_lengths.append(len(batch_rewards))\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+.1f} | \"\n",
    "          f\"Turns: {len(batch_rewards)} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # --- Checkpoint ---\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Saved checkpoint ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ PPO training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bfebd",
   "metadata": {},
   "source": [
    "## Test Trained Model\n",
    "\n",
    "Let's play a full game with the trained model and watch it perform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66507287",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üß† TESTING TRAINED PPO MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Play one test episode with trained PPO model\n",
    "queries, responses, rewards = generate_episode_for_ppo(max_turns=100, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL EPISODE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total turns: {len(rewards)}\")\n",
    "print(f\"   Total reward: {sum([r.item() for r in rewards]):.2f}\")\n",
    "print(f\"   Average reward/turn: {np.mean([r.item() for r in rewards]):.2f}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08456a29",
   "metadata": {},
   "source": [
    "## üíæ Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final PPO model\n",
    "save_path = \"outputs_marooned_rl/final_ppo_model\"\n",
    "\n",
    "print(f\"üíæ Saving final PPO model to {save_path}...\")\n",
    "\n",
    "# Save PPO trainer (includes model + value head)\n",
    "ppo_trainer.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"‚úÖ PPO Model saved!\")\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "print(f\"   {save_path}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ pytorch_model.bin (policy + value network)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ tokenizer_config.json\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ ...\")\n",
    "\n",
    "print(\"\\nüéâ Training complete! Your AI agents can now play MAROONED with PPO!\")\n",
    "\n",
    "print(f\"   {save_path}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ model.safetensors\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ tokenizer_config.json\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ ...\")\n",
    "\n",
    "print(\"\\nüéâ Training complete! Your AI agents can now play MAROONED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
