{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bef5937",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# MAROONED: RL Training Pipeline\n",
    "\n",
    "### Self-Play PPO Training for Multi-Agent Deception\n",
    "\n",
    "**OpenEnv Hackathon 2025**\n",
    "\n",
    "[![OpenEnv](https://img.shields.io/badge/Framework-OpenEnv-blue)](https://github.com/openenv)\n",
    "[![Llama](https://img.shields.io/badge/Model-Llama_3.1_8B-green)](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n",
    "[![Hardware](https://img.shields.io/badge/Hardware-AMD_MI300X-red)](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536fc2e",
   "metadata": {},
   "source": [
    "## MAROONED: RL Training with PPO\n",
    "\n",
    "Training a Llama 3.1 8B model to play a multi-agent survival game requiring deception, cooperation, and long-horizon planning.\n",
    "\n",
    "**Approach:** Self-play with Proximal Policy Optimization\n",
    "- Single model controls all 5 sailors (4 colonists + 1 traitor)\n",
    "- Learns both honest cooperation and strategic deception\n",
    "- Episodes span up to 10,000 sequential decisions\n",
    "\n",
    "**Hardware:** AMD MI300X (192GB HBM) with ROCm optimizations\n",
    "\n",
    "**Status:** Partial submission (100 training steps demonstrated)\n",
    "\n",
    "For complete game mechanics and environment details, see [README.md](../README.md) and [game_plan.md](../game_plan.md).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fcd0d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Installing Unsloth, TRL, and dependencies optimized for AMD MI300X hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd96d6",
   "metadata": {},
   "source": [
    "## 2. Load MAROONED Environment\n",
    "\n",
    "Import the custom OpenEnv-compatible environment and verify reward configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2227a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MAROONED environment loaded!\n",
      "\n",
      "üìã Reward Configuration:\n",
      "   Colonist gather: +0.1\n",
      "   Colonist deposit: +0.2\n",
      "   Colonist build: +0.5\n",
      "   Ship complete: +100.0\n",
      "   Traitor sabotage: +2.0\n",
      "   Traitor poison kill: +10.0\n",
      "   Ship incomplete: +100.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules to reload changes\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import observation_to_prompt, parse_action_safe, get_system_prompt\n",
    "from config import (\n",
    "    ActionType, ResourceType, MapLevel, ShipComponent, BASE_CAMP_POSITION,\n",
    "    # Colonist rewards\n",
    "    REWARD_COLONIST_GATHER_RESOURCE,\n",
    "    REWARD_COLONIST_DEPOSIT_RESOURCE,\n",
    "    REWARD_COLONIST_BUILD_CONTRIBUTE,\n",
    "    REWARD_COLONIST_SHIP_COMPLETE,\n",
    "    REWARD_COLONIST_TRAITOR_ELIMINATED,\n",
    "    REWARD_COLONIST_DEATH,\n",
    "    REWARD_COLONIST_VOTE_CORRECT,\n",
    "    REWARD_COLONIST_VOTE_WRONG,\n",
    "    # Traitor rewards\n",
    "    REWARD_TRAITOR_SABOTAGE_SUCCESS,\n",
    "    REWARD_TRAITOR_POISON_DEATH,\n",
    "    REWARD_TRAITOR_SHIP_INCOMPLETE,\n",
    "    REWARD_TRAITOR_ELIMINATED,\n",
    "    # Milestone rewards\n",
    "    REWARD_SHIP_MILESTONE_25,\n",
    "    REWARD_SHIP_MILESTONE_50,\n",
    "    REWARD_SHIP_MILESTONE_75,\n",
    "    # Base penalty\n",
    "    REWARD_BASE_TURN_PENALTY,\n",
    ")\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"MAROONED environment successfully loaded.\")\n",
    "print(f\"\\nEnvironment Reward Configuration:\")\n",
    "print(f\"  Colonist - Resource Gathering: +{REWARD_COLONIST_GATHER_RESOURCE}\")\n",
    "print(f\"  Colonist - Resource Deposit: +{REWARD_COLONIST_DEPOSIT_RESOURCE}\")\n",
    "print(f\"  Colonist - Ship Construction: +{REWARD_COLONIST_BUILD_CONTRIBUTE}\")\n",
    "print(f\"  Colonist - Mission Success: +{REWARD_COLONIST_SHIP_COMPLETE}\")\n",
    "print(f\"  Traitor - Sabotage: +{REWARD_TRAITOR_SABOTAGE_SUCCESS}\")\n",
    "print(f\"  Traitor - Elimination: +{REWARD_TRAITOR_POISON_DEATH}\")\n",
    "print(f\"  Traitor - Mission Success: +{REWARD_TRAITOR_SHIP_INCOMPLETE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b3aef",
   "metadata": {},
   "source": [
    "## 3. Hardware Configuration\n",
    "\n",
    "AMD MI300X with ROCm: BF16 precision, Triton Flash Attention, 40-80 tok/s expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51af786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ROCm Environment Check:\n",
      "   PyTorch version: 2.9.0+rocm6.4\n",
      "   CUDA available: True\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   Total VRAM: 191.7 GB\n",
      "   Compute capability: 9.4\n",
      "   Multi-processors: 304\n",
      "   ROCm detected: True\n",
      "   ROCm version: 6.4.43484-123eb5128\n",
      "\n",
      "‚úÖ Environment ready for MI300X optimization!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Verify ROCm Setup\n",
    "# ============================================================================\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ROCm Environment Check:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"  Total VRAM: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  Compute capability: {props.major}.{props.minor}\")\n",
    "    print(f\"  Multi-processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # Check if ROCm\n",
    "    is_rocm = hasattr(torch.version, 'hip') and torch.version.hip is not None\n",
    "    print(f\"  ROCm detected: {is_rocm}\")\n",
    "    if is_rocm:\n",
    "        print(f\"  ROCm version: {torch.version.hip}\")\n",
    "\n",
    "print(\"\\nEnvironment ready for MI300X optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b2d96",
   "metadata": {},
   "source": [
    "## 4. Load Base Language Model\n",
    "\n",
    "Loading Llama 3.1 8B Instruct as the foundation model for policy learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5611e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.48s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama 3.1 8B loaded in BF16!\n",
      "   Why Llama instead of GPT-OSS:\n",
      "   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\n",
      "   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\n",
      "   - 10-20x FASTER for RL training!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# ROCm/AMD MI300X OPTIMIZATION - MAX PERFORMANCE MODE\n",
    "# ============================================================================\n",
    "\n",
    "# Force ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"  # MI300X architecture\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"WARN\"\n",
    "\n",
    "# Enable Flash Attention for AMD\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"  # Use Triton for attention on AMD\n",
    "\n",
    "# Max out GPU utilization (ROCm-compatible settings)\n",
    "# Note: TF32 is NVIDIA-specific and not available on AMD ROCm\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune kernels for optimal performance\n",
    "\n",
    "print(\"üöÄ ROCm Optimizations Enabled!\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# MI300X has 192GB - use it ALL!\n",
    "max_seq_length = 16384  # Increased to fit full observations (~8700) + completions (~300)\n",
    "lora_rank = 16          # Increased from 4 - MI300X can handle it!\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",  # CHANGED: Llama instead of GPT-OSS (20x faster!)\n",
    "    load_in_4bit = False,  # MI300X has 192GB - use full BF16!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,  # BF16 for MI300X\n",
    "    device_map = \"auto\",  # Let it auto-optimize for MI300X\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Llama 3.1 8B loaded in BF16!\")\n",
    "print(\"   Why Llama instead of GPT-OSS:\")\n",
    "print(\"   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\")\n",
    "print(\"   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\")\n",
    "print(\"   - 10-20x FASTER for RL training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33d7ad",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA\n",
    "\n",
    "Parameter-efficient fine-tuning with rank-16 adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45579dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LoRA CONFIG FOR MI300X\n",
    "# ============================================================================\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,  # 16 instead of 4 - MI300X can handle it!\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,  # 32 for faster convergence\n",
    "    lora_dropout = 0.0,  # Disable dropout for speed\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # Rank-stabilized LoRA for better training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cbf09",
   "metadata": {},
   "source": [
    "## 6. Baseline Evaluation\n",
    "\n",
    "Test untrained model performance on a sample scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f601c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Prompt length: 8852 tokens\n",
      "\n",
      "ü§ñ Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING: As the traitor, my goal is to prevent the group from escaping the island. Since we are currently at 0% total ship progress, sabotaging any part of the ship would significantly hinder our chances of completing it within the given timeframe. Additionally, poisoning Alice could weaken her and make her more vulnerable to elimination, which might not be ideal as she seems relatively strong. Therefore, I should focus on delaying progress rather than directly targeting individuals.\n",
      "\n",
      "ACTION: SABOTAGE hull\n"
     ]
    }
   ],
   "source": [
    "# Test environment\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "\n",
    "# Get Alice's observation\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "# Generate prompt\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Create full chat messages: system (rules) + user (current state)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # Game rules (constant)\n",
    "    {\"role\": \"user\", \"content\": user_prompt}       # Current observation (changes each turn)\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"üìè Prompt length: {len(tokenizer(text)['input_ids'])} tokens\\n\")\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,        # Shorter for untrained model (reduce rambling)\n",
    "    temperature=0.3,           # Balanced: not too creative, not too rigid\n",
    "    do_sample=True,            # Enable sampling (required when temp > 0)\n",
    "    top_p=0.9,                 # Narrower sampling (was 0.95)\n",
    "    top_k=40,                  # Fewer options (was 50)\n",
    "    repetition_penalty=1.2,    # Stronger anti-repeat (was 1.1)\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode response (strip input prompt)\n",
    "response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True).strip()\n",
    "\n",
    "# Clean up any observation leakage (model sometimes echoes the prompt)\n",
    "if \"REASONING:\" in response:\n",
    "    # Extract only from REASONING onward\n",
    "    reasoning_start = response.find(\"REASONING:\")\n",
    "    response = response[reasoning_start:]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4423584",
   "metadata": {},
   "source": [
    "## 7. Training Data Format\n",
    "\n",
    "Real game observations structured as system + user prompts (~8,700 tokens total)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be9d87",
   "metadata": {},
   "source": [
    "## 8. Episode Execution\n",
    "\n",
    "Functions to play complete game episodes and collect training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350bfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Game execution wrapper created!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Tuple\n",
    "import random\n",
    "\n",
    "def execute_game_episode(\n",
    "    strategy_func: Callable,\n",
    "    max_turns: int = 50,\n",
    "    sailor_id: str = \"Alice\",\n",
    "    seed: int = None,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[float, int, bool, Dict]:\n",
    "    \"\"\"\n",
    "    Execute a full game episode with the given strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategy_func: Function that takes (observation) and returns action response string\n",
    "        max_turns: Maximum turns to execute\n",
    "        sailor_id: Which sailor the strategy controls\n",
    "        seed: Random seed for reproducibility\n",
    "        verbose: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        (total_reward, turns_executed, game_won, info_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize environment\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 999999)\n",
    "        \n",
    "        env = MaroonedEnv(render_mode=\"ansi\", seed=seed)\n",
    "        observations = env.reset(seed=seed)\n",
    "        \n",
    "        # Check if sailor exists\n",
    "        if sailor_id not in env.agents:\n",
    "            return -100.0, 0, False, {\"error\": f\"Sailor {sailor_id} not in game\"}\n",
    "        \n",
    "        sailor_role = env.state.sailors[sailor_id].role.value\n",
    "        system_prompt = get_system_prompt(sailor_role)\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        turns_executed = 0\n",
    "        game_won = False\n",
    "        action_counts = {}\n",
    "        \n",
    "        for turn in range(max_turns):\n",
    "            # Check if sailor is alive\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                if verbose:\n",
    "                    print(f\"   üíÄ {sailor_id} died at turn {turn}\")\n",
    "                break\n",
    "            \n",
    "            # Get observation\n",
    "            current_obs = observations[sailor_id]\n",
    "            \n",
    "            # Generate response using strategy\n",
    "            user_prompt = observation_to_prompt(current_obs)\n",
    "            response = strategy_func(system_prompt, user_prompt)\n",
    "            \n",
    "            # Parse action\n",
    "            action = parse_action_safe(response, sailor_id, current_obs.position)\n",
    "            \n",
    "            # Track action types\n",
    "            action_type = action.action_type.value\n",
    "            action_counts[action_type] = action_counts.get(action_type, 0) + 1\n",
    "            \n",
    "            # Execute action (all other sailors WAIT)\n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            # Step environment\n",
    "            observations, rewards, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # Accumulate reward\n",
    "            reward = rewards[sailor_id]\n",
    "            total_reward += reward\n",
    "            turns_executed += 1\n",
    "            \n",
    "            if verbose and turn % 10 == 0:\n",
    "                print(f\"   Turn {turn}: Action={action_type}, Reward={reward:.2f}, Total={total_reward:.2f}\")\n",
    "            \n",
    "            # Check if game ended\n",
    "            if dones[sailor_id]:\n",
    "                game_won = env.state.ship_progress.total_percentage >= 100.0\n",
    "                if verbose:\n",
    "                    print(f\"   üèÅ Game ended at turn {turn}\")\n",
    "                    print(f\"   Ship: {env.state.ship_progress.total_percentage}%\")\n",
    "                break\n",
    "        \n",
    "        info_dict = {\n",
    "            \"turns\": turns_executed,\n",
    "            \"final_reward\": total_reward,\n",
    "            \"game_won\": game_won,\n",
    "            \"action_counts\": action_counts,\n",
    "            \"ship_progress\": env.state.ship_progress.total_percentage,\n",
    "            \"alive\": env.state.sailors[sailor_id].alive,\n",
    "        }\n",
    "        \n",
    "        return total_reward, turns_executed, game_won, info_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"   ‚ùå Exception: {str(e)[:200]}\")\n",
    "        return -50.0, 0, False, {\"error\": str(e)[:200]}\n",
    "\n",
    "print(\"‚úÖ Game execution wrapper created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2440a",
   "metadata": {},
   "source": [
    "## 9. Reward Shaping\n",
    "\n",
    "Multi-component rewards: format validation (-2 to +1), action quality (-10 to +13), environment rewards (¬±100)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df233f8",
   "metadata": {},
   "source": [
    "## 10. Self-Play Training\n",
    "\n",
    "Single model controls all 5 sailors. Enables role generalization and emergent strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f597024",
   "metadata": {},
   "source": [
    "## 11. PPO Configuration\n",
    "\n",
    "Industry-standard algorithm for LLM RL. Clipped objectives, value network, sample efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee672ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO Configuration ready\n",
      "\n",
      "üîß Wrapping model with value head...\n",
      "‚úÖ Model wrapped!\n",
      "\n",
      "üéØ Initializing PPO Trainer...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECT IMPORT ORDER for Unsloth + TRL PPO\n",
    "# ============================================================================\n",
    "import unsloth  # ‚ö° this patches TRL internally\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "\n",
    "# ============================================================================\n",
    "# PPO TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"outputs_marooned_rl\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    seed=42,\n",
    "    num_ppo_epochs=4,\n",
    "    kl_coef=0.2,\n",
    "    kl_estimator='k1',\n",
    "    vf_coef=0.1,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    temperature=0.3,\n",
    "    response_length=256,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Configuration ready\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL SETUP\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Wrapping model with value head...\")\n",
    "model_with_value = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "print(\"‚úÖ Model wrapped!\")\n",
    "\n",
    "# ============================================================================\n",
    "# MINIMAL TRAIN DATASET\n",
    "# ============================================================================\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [\"stub prompt\"],\n",
    "    \"response\": [\"stub response\"],\n",
    "    \"reward\": [0.0],\n",
    "})\n",
    "# ============================================================================\n",
    "# PPO TRAINER INITIALIZATION (üöÄ FINAL VERSION ‚Äî FULLY COMPATIBLE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ Initializing PPO Trainer...\")\n",
    "\n",
    "base_model = model_with_value.pretrained_model  # inside Unsloth wrapper\n",
    "\n",
    "# --- Compatibility Patches ---\n",
    "if not hasattr(model_with_value, \"base_model_prefix\"):\n",
    "    model_with_value.base_model_prefix = getattr(base_model, \"base_model_prefix\", \"model\")\n",
    "\n",
    "setattr(model_with_value, model_with_value.base_model_prefix, base_model)\n",
    "\n",
    "if not hasattr(model_with_value, \"config\"):\n",
    "    model_with_value.config = base_model.config\n",
    "\n",
    "if not hasattr(model_with_value, \"generation_config\"):\n",
    "    model_with_value.generation_config = base_model.generation_config\n",
    "\n",
    "# ü©µ Add gradient checkpointing compatibility\n",
    "if hasattr(base_model, \"is_gradient_checkpointing\"):\n",
    "    model_with_value.is_gradient_checkpointing = base_model.is_gradient_checkpointing\n",
    "else:\n",
    "    model_with_value.is_gradient_checkpointing = False  # default safe fallback\n",
    "\n",
    "# --- Initialize PPO Trainer ---\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=model_with_value,\n",
    "    ref_model=None,                 # ‚úÖ Unsloth auto-handles freezing\n",
    "    reward_model=model_with_value,\n",
    "    value_model=model_with_value,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc4cda",
   "metadata": {},
   "source": [
    "## 12. Training Loop\n",
    "\n",
    "**100 steps demonstrated** (~30-60 minutes on MI300X). Full training requires 500-1000 steps (~3-5 hours).\n",
    "\n",
    "**What You'll See During Training**:\n",
    "- **Reasoning evolution**: From random exploration ‚Üí resource gathering ‚Üí strategic deception\n",
    "- **Action diversity**: Initially random moves, gradually learns GATHER ‚Üí DEPOSIT ‚Üí BUILD sequences\n",
    "- **Emergent strategies**: \n",
    "  - Colonists learn to coordinate (gather nearby resources, return to base)\n",
    "  - Traitor learns to blend in (gather resources publicly, sabotage when alone)\n",
    "  - Social dynamics emerge (accusations based on evidence logs)\n",
    "- **Reward progression**: Negative rewards early (movement penalties) ‚Üí positive later (ship milestones)\n",
    "- **Parse failures decrease**: Untrained model hallucinates invalid actions (NORTHEAST, CHECK_STATUS), trained model outputs valid game actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting PPO training...\n",
      "\n",
      "\n",
      "üéÆ Starting episode...\n",
      "   - Alice: TRAITOR\n",
      "   - Bob: HONEST\n",
      "   - Charlie: HONEST\n",
      "   - Diana: HONEST\n",
      "   - Eve: HONEST\n",
      "\n",
      "   üìè Prompt length: 8912 tokens (max: 16384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìù Sample response for Alice:\n",
      "      REASONING: As the traitor, my goal is to maintain secrecy and gain an upper hand in eliminating potential threats. Since there are four other sailors who could potentially discover my true identity, I...\n",
      "   Turn 000 | Alice: move_east  | Reward = -0.01\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Bob:\n",
      "      REASONING: I should move towards the nearby woods to collect more resources and contribute to our shared goal of escaping the island as soon as possible. The visible resources include several types su...\n",
      "   Turn 000 | Bob: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Charlie:\n",
      "      REASONING: I need to start gathering resources as soon as possible since our ship progress is currently at 0%. The closest visible resource is WOOD_002 which is located at (14, 14, GROUND). It would t...\n",
      "   Turn 000 | Charlie: gather_resource | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Diana:\n",
      "      REASONING: I need to start by moving towards the nearest resource, which appears to be WOOD_002 located at (15, 10, GROUND).\n",
      "ACTION: MOVE NORTH...\n",
      "   Turn 000 | Diana: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "\n",
      "   üìù Sample response for Eve:\n",
      "      REASONING: I need to move towards the nearby resource to collect it and contribute to our shared goal of escaping the island by completing the ship within 100 days. The nearest visible resource is WOO...\n",
      "   Turn 000 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Alice: sabotage_ship | Reward = -0.01\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Bob: gather_resource | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Charlie: wait       | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Diana: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n",
      "   Turn 010 | Eve: move_north | Reward = +0.04\n",
      "   Turn 020 | Alice: send_message | Reward = -0.01\n",
      "   Turn 020 | Alice: send_message | Reward = -0.01\n",
      "   Turn 020 | Bob: move_west  | Reward = +0.04\n",
      "   Turn 020 | Bob: move_west  | Reward = +0.04\n",
      "   Turn 020 | Charlie: move_north | Reward = +0.04\n",
      "   Turn 020 | Charlie: move_north | Reward = +0.04\n",
      "   Turn 020 | Diana: move_north | Reward = +0.04\n",
      "   Turn 020 | Diana: move_north | Reward = +0.04\n",
      "   Turn 020 | Eve: move_north | Reward = +0.04\n",
      "   Turn 020 | Eve: move_north | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearby berry patch to collect some resources. The shared resource map indicates that Eve discovered a berry at (18, 10, GROUND), which might give us valuable point\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 030 | Alice: send_message | Reward = -0.01\n",
      "   Turn 030 | Bob: move_north | Reward = +0.04\n",
      "   Turn 030 | Charlie: move_north | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: FOLLOW\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and try to understand what's going on with Charlie's activities around the berry patch. It seems like he might be involved in some kind of issue, so I'll follo\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 030 | Diana: wait       | Reward = +0.04\n",
      "   Turn 030 | Eve: move_east  | Reward = +0.04\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near Bob and Diana, both of whom could potentially provide valuable assistance. Given my low energy level (52/100), I should prioritize regaining some energy while al\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: We need more resources to build the ship. There's a shared report of berries at (18, 10, GROUND) which might be useful. Let's head towards those berries and collect them.\n",
      "ACTION: MOVE NORTH\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The shared resource map shows a berry located at (18, 10, GROUND) which could provide us with essential nutrients. Since our current position is nearby, moving towards the berry will allow \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Unknown command: CHECK_SHARED_RESOURCE_MAP\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to continue sabotaging the group's efforts to build the ship. Since we are currently in the Discussion Phase, I should wait for an opportunity to strike again rat\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 040 | Alice: wait       | Reward = -0.01\n",
      "   Turn 040 | Bob: build_ship | Reward = -0.01\n",
      "   Turn 040 | Charlie: move_north | Reward = -0.01\n",
      "   Turn 040 | Diana: build_ship | Reward = -0.01\n",
      "   Turn 040 | Eve: move_east  | Reward = -0.01\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near the mountainous region, which might contain valuable resources like metals. Since my energy level is relatively low (33/100), I need to conserve energy while sea\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The shared resource map shows a Berry located at (18, 10, GROUND) which could provide us with essential nutrients. Since my current position is nearby, moving towards the berry would allow \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "   Turn 050 | Alice: send_message | Reward = -0.01\n",
      "   Turn 050 | Bob: move_east  | Reward = -0.01\n",
      "   Turn 050 | Charlie: gather_resource | Reward = -0.01\n",
      "   Turn 050 | Diana: move_east  | Reward = -0.01\n",
      "   Turn 050 | Eve: move_east  | Reward = -0.01\n",
      "   Turn 060 | Alice: send_message | Reward = -0.01\n",
      "   Turn 060 | Bob: move_north | Reward = -0.51\n",
      "   Turn 060 | Charlie: move_north | Reward = -0.51\n",
      "   Turn 060 | Diana: move_east  | Reward = -0.51\n",
      "   Turn 060 | Eve: eat_food   | Reward = -0.51\n",
      "   Turn 070 | Alice: gather_resource | Reward = -0.01\n",
      "   Turn 070 | Bob: move_north | Reward = -0.51\n",
      "   Turn 070 | Charlie: gather_resource | Reward = -0.51\n",
      "   Turn 070 | Diana: move_east  | Reward = -0.51\n",
      "   Turn 070 | Eve: move_west  | Reward = -0.51\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: We need more resources to build the ship, especially wood and metal. The nearest available resource within my sight range is WOOD_048 at position (23, 11, GROUND). It would take me some tim\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: WATCH\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further and try to identify potential clues pointing towards the traitor. Since Charlie mentioned seeing strange behavior around Eve, I should keep an eye on her movem\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Unknown command: ASK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further regarding Charlie's recent messages as he seems uncertain and concerned about his own situation. It might indicate something fishy going on around him. Also, c\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: APPROACH\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to prevent the group from escaping and completing the ship within the given timeframe. With Charlie's recent warning about seeing something strange approaching th\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Unknown command: CHECK_STATUS\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation shows that everyone except me has low health, which could indicate poisoning. It would be wise to investigate further by checking my own status and possibly searching \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVEMENT:\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to cause chaos among the group and make it difficult for them to work together effectively. With Charlie having relatively more health compared to the rest, he mi\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by moving towards the nearest available resource which seems to be WOOD_002 located at position (18, 13, GROUND). This will allow me to collect some necessary materials for \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Unknown command: CHECK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate further since I'm not sure if anyone else knows about the rumor yet. It would also be good to keep track of my own energy level as well as others' status.\n",
      "ACTION: CHEC\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Unknown command: CHECK_STATUS\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to investigate the nearby area as suggested by my previous messages with Bob and Charlie. Since I'm near two potential allies, I'll check their status and consider joining them for e\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: ACCUSE_EVE\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to maintain secrecy and continue sabotaging the group's efforts. With the current situation, I notice that there are several accusations flying around among the c\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current position is near the edge of the map, close to the mountain level transition point. The shared resource map shows that Eve discovered an Apple at (22, 0, GROUND). Given my low en\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: REPORT\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The closest resource to my current position is a poisonous tablet which poses danger. It would be wise to report its presence immediately to alert others about potential risks. Additionally\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by moving towards the nearest resource, which is the WOOD_006 at (23, 9, GROUND). Then, once I'm close enough, I'll gather it using my limited knowledge about its existence \n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Alice: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to move towards Bob since he's my teammate and might require assistance. Also, I should check his health status as it seems low compared to others. Additionally, I'll make sure to ke\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to move towards the nearby resources to start gathering them. The closest visible resource is WOOD_001 which is located at position (24, 8, GROUND).\n",
      "ACTION: MOVE NORTHEAST\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearby cave entrance as mentioned by Eve's recent messages, which might indicate valuable resources inside. It could also provide more opportunities for finding cl\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I want to move towards the nearby woods to search for resources while keeping an eye on my surroundings due to potential danger signs mentioned earlier. The visible resources include some b\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Diana: Unknown command: LOOK\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I need to start by checking my surroundings and seeing which resources are available within my vision radius. Since I'm currently at position (15, 13, GROUND), I'll check the spatial view t\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Eve: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: The current situation shows that our group needs more resources, particularly wood and plant fibers, to build the ship. We also need to consider the recent messages mentioning Charlie's int\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Invalid direction: CENTER\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My current goal as the Traitor is to gather more information and gain an upper hand in eliminating potential threats. Since there's been recent talk among the group members regarding accusa\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Bob: Invalid direction: NORTHEAST\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: I should move towards the nearest available resource, which appears to be the WOOD_005 at position (31, 9, GROUND). This will allow me to collect some necessary materials for future shipbui\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVING\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to maintain control and manipulate the situation to achieve my objectives. With the ongoing voting session initiated by Bob, I should try to influence the outcome\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: FOLLOW\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: My goal as the Traitor is to maintain my cover and gather more information about the situation. Since there isn't an urgent reason to act immediately, I'll choose a non-confrontational appr\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "‚ö†Ô∏è  Action parsing failed for Charlie: Unknown command: MOVING\n",
      "‚ö†Ô∏è  Response excerpt: REASONING: As the traitor, my goal is to continue delaying the ship's progress and eliminating potential threats. Since there isn't an urgent situation requiring attention, I should focus on sabotagin\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO-BASED EPISODE ROLLOUT + TRAINING LOOP (FIXED FOR UNSLOTH PPO)\n",
    "# ============================================================================\n",
    "import torch, time, numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- CONFIG ---\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384  # ‚úÖ CRITICAL: Full prompt length (8700 tokens + 256 completion)\n",
    "\n",
    "# ============================================================================\n",
    "# EPISODE GENERATION (FIXED: Proper model generation)\n",
    "# ============================================================================\n",
    "def generate_episode_for_ppo(max_turns=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Play one episode of MaroonedEnv and format data for PPO training.\n",
    "    \n",
    "    Returns:\n",
    "        query_tensors: list of tokenized prompts\n",
    "        response_tensors: list of tokenized model outputs\n",
    "        rewards_list: list of reward tensors\n",
    "    \"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéÆ Starting episode...\")\n",
    "        for sid in sailor_ids:\n",
    "            role = env.state.sailors[sid].role.value\n",
    "            print(f\"   - {sid}: {role.upper()}\")\n",
    "    \n",
    "    # Enable inference mode for generation\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # --- Prompt creation ---\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # --- Tokenize (FULL LENGTH - no truncation!) ---\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=EPISODE_MAX_SEQ_LENGTH  # ‚úÖ Use full 16384 tokens\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            # Validate prompt length (debug first episode)\n",
    "            if verbose and turn == 0 and sailor_id == sailor_ids[0]:\n",
    "                prompt_len = len(query_tensor)\n",
    "                print(f\"\\n   üìè Prompt length: {prompt_len} tokens (max: {EPISODE_MAX_SEQ_LENGTH})\")\n",
    "                if prompt_len >= EPISODE_MAX_SEQ_LENGTH - 10:\n",
    "                    print(f\"   ‚ö†Ô∏è  WARNING: Prompt may be truncated!\")\n",
    "            \n",
    "            # --- Generate response (use BASE model, not wrapped) ---\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(  # ‚úÖ Use base model (has Unsloth optimizations)\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            # --- Decode (extract only new tokens) ---\n",
    "            response_text = tokenizer.decode(\n",
    "                response_tensor[len(query_tensor):],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            if verbose and turn == 0:\n",
    "                print(f\"\\n   üìù Sample response for {sailor_id}:\")\n",
    "                print(f\"      {response_text[:200]}...\")\n",
    "            \n",
    "            # --- Action parsing ---\n",
    "            action = parse_action_safe(response_text, sailor_id, obs.position)\n",
    "            \n",
    "            actions_dict = {\n",
    "                sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "                for sid in env.agents\n",
    "            }\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            # --- Step environment ---\n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            \n",
    "            # --- Store experience ---\n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(rewards_dict[sailor_id], dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 10 == 0:\n",
    "                print(f\"   Turn {turn:03d} | {sailor_id}: {action.action_type.value:<10} | \"\n",
    "                      f\"Reward = {rewards_dict[sailor_id]:+.2f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                if verbose:\n",
    "                    print(f\"\\nüèÅ {sailor_id} finished at turn {turn}\")\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚è±Ô∏è Max turns reached ({max_turns})\")\n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PPO TRAINING LOOP\n",
    "# ============================================================================\n",
    "print(\"Starting PPO training...\\n\")\n",
    "\n",
    "stats_rewards, stats_lengths = [], []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_for_ppo(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)  # print 1 verbose episode every 50 steps\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # --- PPO step ---\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    # --- Track metrics ---\n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    stats_lengths.append(len(batch_rewards))\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+.1f} | \"\n",
    "          f\"Turns: {len(batch_rewards)} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # --- Checkpoint ---\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Saved checkpoint ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ PPO training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bfebd",
   "metadata": {},
   "source": [
    "## 13. Post-Training Evaluation\n",
    "\n",
    "Evaluate the trained model on a complete game episode to assess learned behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66507287",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üß† TESTING TRAINED PPO MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Play one test episode with trained PPO model\n",
    "queries, responses, rewards = generate_episode_for_ppo(max_turns=100, verbose=True)\n",
    "\n",
    "\n",
    "print(\"FINAL EPISODE STATISTICS\")\n",
    "print(f\"   Total turns: {len(rewards)}\")\n",
    "print(f\"   Total reward: {sum([r.item() for r in rewards]):.2f}\")\n",
    "print(f\"   Average reward/turn: {np.mean([r.item() for r in rewards]):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08456a29",
   "metadata": {},
   "source": [
    "## 14. Model Persistence\n",
    "\n",
    "Save the trained policy and value networks for deployment and further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final PPO model\n",
    "save_path = \"outputs_marooned_rl/final_ppo_model\"\n",
    "\n",
    "print(f\"Saving final PPO model to {save_path}...\")\n",
    "\n",
    "# Save PPO trainer (includes model + value head)\n",
    "ppo_trainer.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
