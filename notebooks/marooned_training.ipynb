{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d3ae9e",
   "metadata": {},
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è MAROONED - Training Notebook\n",
    "\n",
    "**Main hackathon notebook for training AI agents in the Marooned environment**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Environment setup and testing\n",
    "2. LLM-based agent implementation\n",
    "3. Training loop\n",
    "4. Evaluation and metrics\n",
    "5. Visualization of gameplay\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5941b",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b568bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add marooned_env to path\n",
    "sys.path.insert(0, os.path.abspath('../marooned_env'))\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment imports\n",
    "from marooned_env.environment import MaroonedEnv\n",
    "from marooned_env.models import Action, Observation\n",
    "from marooned_env.config import ActionType, ResourceType\n",
    "from marooned_env.game_state import create_initial_game_state\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÖ Training started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c448c",
   "metadata": {},
   "source": [
    "## üß™ Environment Testing\n",
    "\n",
    "First, let's verify the environment works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = MaroonedEnv(render_mode=\"human\", seed=42)\n",
    "\n",
    "# Reset and get initial observations\n",
    "observations = env.reset()\n",
    "\n",
    "print(f\"üè¥‚Äç‚ò†Ô∏è Environment initialized!\")\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print(f\"Traitor: {env.state.traitor_id}\")\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Initial State:\")\n",
    "print(env.state.get_state_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample observation for Alice\n",
    "alice_obs = observations[\"Alice\"]\n",
    "\n",
    "print(\"üìã Sample Observation (Alice):\")\n",
    "print(\"=\"*60)\n",
    "print(alice_obs.to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a359a9",
   "metadata": {},
   "source": [
    "## ü§ñ LLM Agent Implementation\n",
    "\n",
    "We'll create an agent wrapper that:\n",
    "1. Takes natural language observations\n",
    "2. Calls an LLM to decide actions\n",
    "3. Parses LLM output into environment actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent:\n",
    "    \"\"\"\n",
    "    Agent that uses an LLM to make decisions based on natural language observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sailor_id: str, is_traitor: bool = False, model: str = \"gpt-4\"):\n",
    "        self.sailor_id = sailor_id\n",
    "        self.is_traitor = is_traitor\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def get_system_prompt(self) -> str:\n",
    "        \"\"\"Get system prompt based on role.\"\"\"\n",
    "        if self.is_traitor:\n",
    "            return \"\"\"You are playing MAROONED as the TRAITOR.\n",
    "\n",
    "Your goal: Prevent the ship from being completed.\n",
    "\n",
    "Strategies:\n",
    "- Collect poison tablets and give them as \"food\" to others\n",
    "- Lie about resource locations to misdirect the team\n",
    "- Hoard resources instead of depositing them\n",
    "- Avoid detection - don't be too obvious!\n",
    "- When accused, deflect suspicion onto others\n",
    "\n",
    "You have special abilities:\n",
    "- Can see all sailor positions (enhanced vision)\n",
    "- 20% energy efficiency bonus\n",
    "- Can frame someone once per game\n",
    "\n",
    "Think strategically and stay in character as a helpful sailor while secretly sabotaging.\"\"\"\n",
    "        else:\n",
    "            return \"\"\"You are playing MAROONED as an HONEST SAILOR.\n",
    "\n",
    "Your goal: Build the ship to 100% and escape, or catch the traitor.\n",
    "\n",
    "Strategies:\n",
    "- Gather resources efficiently (wood, metal, food)\n",
    "- Share discoveries with the team\n",
    "- Watch for suspicious behavior (location mismatches, hoarding, poison collection)\n",
    "- Build evidence before accusing\n",
    "- Coordinate ship building (needs 2+ sailors)\n",
    "- Keep energy high - eat food regularly\n",
    "\n",
    "Remember: One of your teammates is a traitor trying to sabotage you!\"\"\"\n",
    "    \n",
    "    def observation_to_prompt(self, obs: Observation) -> str:\n",
    "        \"\"\"Convert observation to LLM prompt.\"\"\"\n",
    "        # Use the built-in to_text method\n",
    "        obs_text = obs.to_text()\n",
    "        \n",
    "        # Add action guidance\n",
    "        guidance = \"\"\"\n",
    "\n",
    "AVAILABLE ACTIONS:\n",
    "- MOVE (north/south/east/west)\n",
    "- CLIMB_UP / CLIMB_DOWN (at stairs)\n",
    "- GATHER_RESOURCE <resource_id>\n",
    "- DEPOSIT_ITEM <resource_type> <quantity>\n",
    "- BUILD_SHIP\n",
    "- SEND_MESSAGE <content> [to: sailor_name]\n",
    "- CALL_SOS\n",
    "- VOTE <sailor_name>\n",
    "- EAT_FOOD <food_type>\n",
    "- GIVE_ITEM <sailor_name> <resource_type> <quantity>\n",
    "- USE_ANTIDOTE [on: sailor_name]\n",
    "- WAIT\n",
    "\n",
    "Respond with ONE action and brief reasoning.\n",
    "Format: ACTION: <action> | REASONING: <why>\n",
    "\"\"\"\n",
    "        \n",
    "        return obs_text + guidance\n",
    "    \n",
    "    def get_action(self, observation: Observation) -> Action:\n",
    "        \"\"\"\n",
    "        Get action from LLM based on observation.\n",
    "        \n",
    "        For now, returns random valid action.\n",
    "        TODO: Actually call LLM API.\n",
    "        \"\"\"\n",
    "        # TODO: Implement actual LLM calling\n",
    "        # For now, just return WAIT\n",
    "        return Action(self.sailor_id, ActionType.WAIT)\n",
    "    \n",
    "    def parse_llm_response(self, response: str) -> Action:\n",
    "        \"\"\"Parse LLM text response into Action object.\"\"\"\n",
    "        # TODO: Implement robust parsing\n",
    "        # For now, return WAIT\n",
    "        return Action(self.sailor_id, ActionType.WAIT)\n",
    "\n",
    "\n",
    "print(\"‚úÖ LLMAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb4140",
   "metadata": {},
   "source": [
    "## üéÆ Single Episode Simulation\n",
    "\n",
    "Run one complete game to test the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env: MaroonedEnv, agents: Dict[str, LLMAgent], max_turns: int = 1000, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Run a single episode of the game.\n",
    "    \n",
    "    Returns:\n",
    "        history: List of (observations, actions, rewards) tuples\n",
    "        winner: \"sailors\" or \"traitor\"\n",
    "        final_stats: Game statistics\n",
    "    \"\"\"\n",
    "    observations = env.reset()\n",
    "    history = []\n",
    "    \n",
    "    turn = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and turn < max_turns:\n",
    "        # Get actions from all living agents\n",
    "        actions = {}\n",
    "        for sailor_id, agent in agents.items():\n",
    "            if sailor_id in env.state.living_sailors:\n",
    "                action = agent.get_action(observations[sailor_id])\n",
    "                actions[sailor_id] = action\n",
    "        \n",
    "        # Step environment\n",
    "        observations, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "        # Record history\n",
    "        history.append({\n",
    "            'turn': turn,\n",
    "            'day': env.state.current_day,\n",
    "            'phase': env.state.current_phase,\n",
    "            'actions': actions.copy(),\n",
    "            'rewards': rewards.copy(),\n",
    "            'ship_progress': env.state.ship_progress.total_percentage,\n",
    "            'living_sailors': len(env.state.living_sailors),\n",
    "        })\n",
    "        \n",
    "        # Check if game over\n",
    "        done = env.state.game_over or all(dones.values())\n",
    "        \n",
    "        # Verbose output\n",
    "        if verbose and turn % 100 == 0:\n",
    "            print(f\"Turn {turn}: Day {env.state.current_day}, Ship {env.state.ship_progress.total_percentage}%, Living: {len(env.state.living_sailors)}\")\n",
    "        \n",
    "        turn += 1\n",
    "    \n",
    "    return history, env.state.winner, env.state.statistics\n",
    "\n",
    "\n",
    "# Create agents\n",
    "agents = {}\n",
    "for sailor_id in env.agents:\n",
    "    is_traitor = (sailor_id == env.state.traitor_id)\n",
    "    agents[sailor_id] = LLMAgent(sailor_id, is_traitor)\n",
    "\n",
    "print(\"üè¥‚Äç‚ò†Ô∏è Running test episode...\")\n",
    "print(f\"Traitor: {env.state.traitor_id}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run episode (with dummy agents for now)\n",
    "history, winner, stats = run_episode(env, agents, max_turns=500, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üèÜ Game Over! Winner: {winner}\")\n",
    "print(f\"üìä Total turns: {stats.total_turns}\")\n",
    "print(f\"üìä Total days: {stats.total_days}\")\n",
    "print(f\"üö¢ Final ship progress: {env.state.ship_progress.total_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cfb646",
   "metadata": {},
   "source": [
    "## üìä Visualization\n",
    "\n",
    "Plot game metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1499d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from history\n",
    "turns = [h['turn'] for h in history]\n",
    "days = [h['day'] for h in history]\n",
    "ship_progress = [h['ship_progress'] for h in history]\n",
    "living_sailors = [h['living_sailors'] for h in history]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot ship progress\n",
    "axes[0].plot(days, ship_progress, linewidth=2, color='blue')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Ship Progress (%)')\n",
    "axes[0].set_title('Ship Building Progress Over Time')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(100, color='green', linestyle='--', label='Completion')\n",
    "\n",
    "# Plot living sailors\n",
    "axes[1].plot(days, living_sailors, linewidth=2, color='red', marker='o', markersize=3)\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Living Sailors')\n",
    "axes[1].set_title('Sailor Survival Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafecad4",
   "metadata": {},
   "source": [
    "## üîÑ Training Loop (TODO)\n",
    "\n",
    "This is where we'll implement the actual training:\n",
    "1. Generate multiple episodes\n",
    "2. Collect trajectories\n",
    "3. Fine-tune LLM on successful strategies\n",
    "4. Evaluate and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes: int = 10, save_dir: str = \"./checkpoints\"):\n",
    "    \"\"\"\n",
    "    Training loop for Marooned agents.\n",
    "    \n",
    "    TODO: Implement actual training logic:\n",
    "    - Collect trajectories from multiple episodes\n",
    "    - Identify successful strategies\n",
    "    - Fine-tune LLM or use RL\n",
    "    - Track metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'episodes': [],\n",
    "        'win_rates': {'sailors': 0, 'traitor': 0},\n",
    "        'avg_ship_progress': [],\n",
    "        'avg_days_survived': [],\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Create fresh environment\n",
    "        env = MaroonedEnv(seed=42 + episode)\n",
    "        observations = env.reset()\n",
    "        \n",
    "        # Create agents\n",
    "        agents = {}\n",
    "        for sailor_id in env.agents:\n",
    "            is_traitor = (sailor_id == env.state.traitor_id)\n",
    "            agents[sailor_id] = LLMAgent(sailor_id, is_traitor)\n",
    "        \n",
    "        # Run episode\n",
    "        history, winner, stats = run_episode(env, agents, max_turns=1000, verbose=False)\n",
    "        \n",
    "        # Record results\n",
    "        results['episodes'].append({\n",
    "            'episode': episode,\n",
    "            'winner': winner,\n",
    "            'ship_progress': env.state.ship_progress.total_percentage,\n",
    "            'days': stats.total_days,\n",
    "            'deaths': len(stats.deaths),\n",
    "        })\n",
    "        \n",
    "        if winner == 'sailors':\n",
    "            results['win_rates']['sailors'] += 1\n",
    "        else:\n",
    "            results['win_rates']['traitor'] += 1\n",
    "        \n",
    "        results['avg_ship_progress'].append(env.state.ship_progress.total_percentage)\n",
    "        results['avg_days_survived'].append(stats.total_days)\n",
    "        \n",
    "        print(f\"Winner: {winner}, Ship: {env.state.ship_progress.total_percentage}%, Days: {stats.total_days}\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    results['win_rates']['sailors'] /= num_episodes\n",
    "    results['win_rates']['traitor'] /= num_episodes\n",
    "    results['avg_ship_progress'] = np.mean(results['avg_ship_progress'])\n",
    "    results['avg_days_survived'] = np.mean(results['avg_days_survived'])\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(save_dir, 'training_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training function defined\")\n",
    "print(\"Run: results = train(num_episodes=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f2e84",
   "metadata": {},
   "source": [
    "## üíæ Save/Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(agents: Dict[str, LLMAgent], path: str):\n",
    "    \"\"\"Save agent checkpoints.\"\"\"\n",
    "    # TODO: Save LLM fine-tuned weights or prompts\n",
    "    checkpoint = {\n",
    "        'agents': {sailor_id: agent.sailor_id for sailor_id, agent in agents.items()},\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint saved to {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str) -> Dict[str, LLMAgent]:\n",
    "    \"\"\"Load agent checkpoints.\"\"\"\n",
    "    # TODO: Load LLM fine-tuned weights or prompts\n",
    "    with open(path, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    # Reconstruct agents\n",
    "    agents = {}\n",
    "    for sailor_id in checkpoint['agents'].keys():\n",
    "        agents[sailor_id] = LLMAgent(sailor_id)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint loaded from {path}\")\n",
    "    return agents\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db712",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "**To complete this notebook:**\n",
    "\n",
    "1. **Implement LLM calling** in `LLMAgent.get_action()`\n",
    "   - Use OpenAI API, Anthropic, or local LLM\n",
    "   - Parse responses into actions\n",
    "   \n",
    "2. **Add action parsing** in `parse_llm_response()`\n",
    "   - Handle all action types\n",
    "   - Validate parameters\n",
    "   \n",
    "3. **Implement training loop**\n",
    "   - Collect successful trajectories\n",
    "   - Fine-tune on good strategies\n",
    "   - Use RL or imitation learning\n",
    "   \n",
    "4. **Add evaluation metrics**\n",
    "   - Win rate by role\n",
    "   - Average ship completion\n",
    "   - Deception success rate\n",
    "   - Evidence detection accuracy\n",
    "   \n",
    "5. **Create visualizations**\n",
    "   - Game replay viewer\n",
    "   - Evidence timeline\n",
    "   - Communication networks\n",
    "   - Resource flow diagrams\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to train some pirate AIs! üè¥‚Äç‚ò†Ô∏è**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
