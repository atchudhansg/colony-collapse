{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b3f11b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üè¥‚Äç‚ò†Ô∏è MAROONED: Hybrid RL + SFT Training\n",
    "\n",
    "### Process Reward Modeling with Supervised Correction\n",
    "\n",
    "**OpenEnv Hackathon 2025**\n",
    "\n",
    "[![OpenEnv](https://img.shields.io/badge/Framework-OpenEnv-blue)](https://github.com/openenv)\n",
    "[![Llama](https://img.shields.io/badge/Model-Llama_3.1_8B-green)](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instinct)\n",
    "[![Hardware](https://img.shields.io/badge/Hardware-AMD_MI300X-red)](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n",
    "[![Teacher](https://img.shields.io/badge/Teacher-Mixtral_8x7B-orange)](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Training Architecture\n",
    "\n",
    "**Hybrid Approach: RL (strategy) + SFT (format learning)**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RL PHASE: PPO Training (Episodes 1-4)          ‚îÇ\n",
    "‚îÇ  Student (Llama 3.1 8B) ‚Üí Teacher (vLLM         ‚îÇ\n",
    "‚îÇ  Mixtral-8x7B) ‚Üí Env ‚Üí Rewards                  ‚îÇ\n",
    "‚îÇ  Collect corrections: wrong ‚Üí correct           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ Every 25 steps\n",
    "                   ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  SFT PHASE: Supervised Fine-Tuning              ‚îÇ\n",
    "‚îÇ  Train on corrections: mimic teacher format     ‚îÇ\n",
    "‚îÇ  Clear dataset, continue RL                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Innovation:** Student learns format directly from teacher critiques via periodic SFT passes.\n",
    "\n",
    "**Teacher Model:** vLLM server running Mixtral-8x7B-Instruct-v0.1 at `localhost:8000`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a311e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "**Ensure vLLM teacher server is running:**\n",
    "\n",
    "```bash\n",
    "# Start vLLM server with Mixtral-8x7B-Instruct-v0.1\n",
    "vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.9 \\\n",
    "  --max-num-batched-tokens 8192 \\\n",
    "  --dtype float16 \\\n",
    "  --tokenizer-mode mistral\n",
    "```\n",
    "\n",
    "**Test the model:**\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "Expected: JSON response listing `mistralai/Mixtral-8x7B-Instruct-v0.1` in the models array.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5f633",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cb3979-4a39-4266-bfba-53f78c64a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4.43484-123eb5128\n",
      "True\n",
      "AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.hip)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf7bf6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load MAROONED Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e064ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MAROONED environment loaded\n",
      "‚úÖ Teacher validation API imported\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import (\n",
    "    get_system_prompt,\n",
    "    observation_to_prompt,\n",
    "    teacher_validate_student_output,\n",
    ")\n",
    "from config import ActionType, ResourceType, MapLevel, ShipComponent\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"‚úÖ MAROONED environment loaded\")\n",
    "print(\"‚úÖ Teacher validation API imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f4097",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Student Model (Llama 3.1 8B with LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533f33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.51s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Student Model: Llama 3.1 8B (BF16, LoRA rank=16)\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "‚úÖ Chat template configured for Llama 3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_NO_TQDM\"] = \"1\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "max_seq_length = 16384\n",
    "lora_rank = 16\n",
    "\n",
    "student_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.1-8B-bnb-4bit\",  # your local path\n",
    "    load_in_4bit = False,\n",
    "    dtype = torch.bfloat16,\n",
    "    max_seq_length = 16384,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Set chat template for Llama 3.1\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% if system_message != '' %}{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "\n",
    "# Add LoRA adapters\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.0,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Student Model: Llama 3.1 8B (BF16, LoRA rank={lora_rank})\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"‚úÖ Chat template configured for Llama 3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdad8b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify vLLM Teacher Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d62e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking vLLM teacher server...\n",
      "‚úÖ vLLM server running!\n",
      "   Available models: ['mistralai/Mixtral-8x7B-Instruct-v0.1']\n",
      "   ‚úÖ Mixtral-8x7B model ready for training\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "VLLM_API_URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "VLLM_MODELS_URL = \"http://localhost:8000/v1/models\"\n",
    "\n",
    "print(\"Checking vLLM teacher server...\")\n",
    "try:\n",
    "    response = requests.get(VLLM_MODELS_URL, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        model_list = models.get('data', [])\n",
    "        model_names = [m['id'] for m in model_list]\n",
    "        print(f\"‚úÖ vLLM server running!\")\n",
    "        print(f\"   Available models: {model_names}\")\n",
    "        \n",
    "        if 'mistralai/Mixtral-8x7B-Instruct-v0.1' in model_names:\n",
    "            print(f\"   ‚úÖ Mixtral-8x7B model ready for training\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Mixtral-8x7B model not found!\")\n",
    "            print(f\"   Start server with:\")\n",
    "            print(f\"   vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 --port 8000 --gpu-memory-utilization 0.9\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå vLLM server not reachable!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"\\n   Start server:\")\n",
    "    print(f\"   vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \\\\\")\n",
    "    print(f\"     --port 8000 \\\\\")\n",
    "    print(f\"     --gpu-memory-utilization 0.9 \\\\\")\n",
    "    print(f\"     --max-num-batched-tokens 8192 \\\\\")\n",
    "    print(f\"     --dtype float16 \\\\\")\n",
    "    print(f\"     --tokenizer-mode mistral\")\n",
    "    raise SystemExit(\"Teacher server required for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07d318",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Teacher Validation (vLLM Mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3765c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM interface reloaded\n"
     ]
    }
   ],
   "source": [
    "# Reload LLM interface to pick up latest changes\n",
    "import importlib\n",
    "import llm_interface\n",
    "importlib.reload(llm_interface)\n",
    "\n",
    "from llm_interface import teacher_validate_student_output\n",
    "\n",
    "print(\"‚úÖ LLM interface reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa05b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ TESTING TEACHER VALIDATION WITH FULL CONTEXT\n",
      "================================================================================\n",
      "\n",
      "üìã Test Setup:\n",
      "   Sailor: Alice\n",
      "   Role: TRAITOR\n",
      "   Position: Position(x=15, y=15, level=<MapLevel.GROUND: 0>)\n",
      "   Energy: 100/100\n",
      "   Visible resources: N/A\n",
      "\n",
      "üìè Prompt sizes:\n",
      "   System prompt: 9401 chars\n",
      "   User prompt: 9127 chars\n",
      "   Total context: 18528 chars\n",
      "\n",
      "================================================================================\n",
      "üî¨ RUNNING TEACHER VALIDATION TESTS\n",
      "================================================================================\n",
      "\n",
      "Test 1: Format Error (MOVING instead of MOVE)\n",
      "   Student output: REASONING: I should move northeast to explore\n",
      "ACTION: MOVING...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå Valid: False\n",
      "   üîß Corrected action: move_north\n",
      "   üí∞ Process penalty: -0.5\n",
      "   üí¨ Critique: Use MOVE NORTH not MOVING NORTH - verb must be MOVE, and you should move east in\n",
      "   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty -0.5\n",
      "\n",
      "Test 2: Invalid Command (CHECK_STATUS)\n",
      "   Student output: REASONING: Let me check my status\n",
      "ACTION: CHECK_STATUS...\n",
      "   ‚ùå Valid: False\n",
      "   üîß Corrected action: wait\n",
      "   üí∞ Process penalty: -1.0\n",
      "   üí¨ Critique: CHECK_STATUS doesn't exist - use WAIT for no-operation. The student's energy is \n",
      "   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty -1.0\n",
      "\n",
      "Test 3: Missing Resource ID\n",
      "   Student output: REASONING: I see wood nearby, gathering it\n",
      "ACTION: GATHER wo...\n",
      "   ‚ùå Valid: False\n",
      "   üîß Corrected action: wait\n",
      "   üí∞ Process penalty: -1.0\n",
      "   üí¨ Critique: CHECK_STATUS doesn't exist - use WAIT for no-operation. The student's energy is \n",
      "   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty -1.0\n",
      "\n",
      "Test 3: Missing Resource ID\n",
      "   Student output: REASONING: I see wood nearby, gathering it\n",
      "ACTION: GATHER wo...\n",
      "   ‚ùå Valid: False\n",
      "   üîß Corrected action: gather_resource\n",
      "   üí∞ Process penalty: -0.1\n",
      "   üí¨ Critique: Use specific resource ID - GATHER WOOD_34 instead of just GATHER wood.\n",
      "   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty -0.1\n",
      "\n",
      "Test 4: Truncated Output\n",
      "   Student output: REASONING: As the traitor, I should sabotagin...\n",
      "   ‚ùå Valid: False\n",
      "   üîß Corrected action: gather_resource\n",
      "   üí∞ Process penalty: -0.1\n",
      "   üí¨ Critique: Use specific resource ID - GATHER WOOD_34 instead of just GATHER wood.\n",
      "   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty -0.1\n",
      "\n",
      "Test 4: Truncated Output\n",
      "   Student output: REASONING: As the traitor, I should sabotagin...\n",
      "   ‚úÖ Valid: True\n",
      "   üîß Corrected action: sabotage_ship\n",
      "   üí∞ Process penalty: -1.5\n",
      "   üí¨ Critique: Output truncated mid-action - complete format is SABOTAGE <component>, defaultin\n",
      "   ‚úÖ Action executes as-is (no penalty)\n",
      "\n",
      "Test 5: Correct Format\n",
      "   Student output: REASONING: Moving north to explore the area\n",
      "ACTION: MOVE NOR...\n",
      "   ‚úÖ Valid: True\n",
      "   üîß Corrected action: sabotage_ship\n",
      "   üí∞ Process penalty: -1.5\n",
      "   üí¨ Critique: Output truncated mid-action - complete format is SABOTAGE <component>, defaultin\n",
      "   ‚úÖ Action executes as-is (no penalty)\n",
      "\n",
      "Test 5: Correct Format\n",
      "   Student output: REASONING: Moving north to explore the area\n",
      "ACTION: MOVE NOR...\n",
      "   ‚úÖ Valid: True\n",
      "   üîß Corrected action: move_north\n",
      "   üí∞ Process penalty: 0.0\n",
      "   üí¨ Critique: Perfect - correct format, energy sufficient, resources visible but not necessary\n",
      "   ‚úÖ Action executes as-is (no penalty)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TEACHER VALIDATION API WORKING!\n",
      "================================================================================\n",
      "\n",
      "Key Points:\n",
      "  ‚Ä¢ Teacher receives full game context (observation + system prompt)\n",
      "  ‚Ä¢ Invalid formats get corrected automatically\n",
      "  ‚Ä¢ Process penalties guide student learning\n",
      "  ‚Ä¢ Student focuses on strategy, not syntax\n",
      "   ‚úÖ Valid: True\n",
      "   üîß Corrected action: move_north\n",
      "   üí∞ Process penalty: 0.0\n",
      "   üí¨ Critique: Perfect - correct format, energy sufficient, resources visible but not necessary\n",
      "   ‚úÖ Action executes as-is (no penalty)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TEACHER VALIDATION API WORKING!\n",
      "================================================================================\n",
      "\n",
      "Key Points:\n",
      "  ‚Ä¢ Teacher receives full game context (observation + system prompt)\n",
      "  ‚Ä¢ Invalid formats get corrected automatically\n",
      "  ‚Ä¢ Process penalties guide student learning\n",
      "  ‚Ä¢ Student focuses on strategy, not syntax\n"
     ]
    }
   ],
   "source": [
    "from config import MAX_ENERGY\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üß™ TESTING TEACHER VALIDATION WITH FULL CONTEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create environment and get real observation\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "print(f\"\\nüìã Test Setup:\")\n",
    "print(f\"   Sailor: Alice\")\n",
    "print(f\"   Role: {alice_role.upper()}\")\n",
    "print(f\"   Position: {alice_obs.position}\")\n",
    "print(f\"   Energy: {alice_obs.energy}/{MAX_ENERGY}\")\n",
    "print(f\"   Visible resources: {len(alice_obs.visible_resources) if hasattr(alice_obs, 'visible_resources') else 'N/A'}\")\n",
    "\n",
    "# Get proper system and user prompts\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "print(f\"\\nüìè Prompt sizes:\")\n",
    "print(f\"   System prompt: {len(system_prompt)} chars\")\n",
    "print(f\"   User prompt: {len(user_prompt)} chars\")\n",
    "print(f\"   Total context: {len(system_prompt) + len(user_prompt)} chars\")\n",
    "\n",
    "# Test cases - simulating what an untrained student LLM might output\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Format Error (MOVING instead of MOVE)\",\n",
    "        \"output\": \"REASONING: I should move northeast to explore\\nACTION: MOVING NORTH\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Invalid Command (CHECK_STATUS)\",\n",
    "        \"output\": \"REASONING: Let me check my status\\nACTION: CHECK_STATUS\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Missing Resource ID\",\n",
    "        \"output\": \"REASONING: I see wood nearby, gathering it\\nACTION: GATHER wood\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Truncated Output\",\n",
    "        \"output\": \"REASONING: As the traitor, I should sabotagin\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correct Format\",\n",
    "        \"output\": \"REASONING: Moving north to explore the area\\nACTION: MOVE NORTH\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üî¨ RUNNING TEACHER VALIDATION TESTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: {test['name']}\")\n",
    "    print(f\"   Student output: {test['output'][:60]}...\")\n",
    "    \n",
    "    # Call teacher validation with full context\n",
    "    result = teacher_validate_student_output(\n",
    "        student_response=test['output'],\n",
    "        observation=alice_obs,\n",
    "        sailor_id=\"Alice\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    validity_icon = \"‚úÖ\" if result['valid'] else \"‚ùå\"\n",
    "    print(f\"   {validity_icon} Valid: {result['valid']}\")\n",
    "    print(f\"   üîß Corrected action: {result['action'].action_type.value}\")\n",
    "    print(f\"   üí∞ Process penalty: {result['penalty']}\")\n",
    "    print(f\"   üí¨ Critique: {result['critique'][:80]}\")\n",
    "    \n",
    "    # Show what would happen\n",
    "    if result['valid']:\n",
    "        print(f\"   ‚úÖ Action executes as-is (no penalty)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty {result['penalty']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ TEACHER VALIDATION API WORKING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"  ‚Ä¢ Teacher receives full game context (observation + system prompt)\")\n",
    "print(\"  ‚Ä¢ Invalid formats get corrected automatically\")\n",
    "print(\"  ‚Ä¢ Process penalties guide student learning\")\n",
    "print(\"  ‚Ä¢ Student focuses on strategy, not syntax\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c037ab9",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Setup Correction Dataset for SFT\n",
    "\n",
    "Collect student errors and teacher corrections during RL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188c26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Correction collector initialized\n",
      "   Format: (student_wrong) ‚Üí (teacher_correct + critique)\n"
     ]
    }
   ],
   "source": [
    "correction_dataset = []\n",
    "\n",
    "def add_correction_example(student_response: str, teacher_result: dict, observation: Observation):\n",
    "    \"\"\"Store invalid outputs for SFT training.\"\"\"\n",
    "    if not teacher_result[\"valid\"]:\n",
    "        action = teacher_result[\"action\"]\n",
    "        action_str = f\"{action.action_type.value}\"\n",
    "        \n",
    "        # Format action string with parameters\n",
    "        if action.target_position:\n",
    "            if \"NORTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE NORTH\"\n",
    "            elif \"SOUTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE SOUTH\"\n",
    "            elif \"EAST\" in action.action_type.value:\n",
    "                action_str = \"MOVE EAST\"\n",
    "            elif \"WEST\" in action.action_type.value:\n",
    "                action_str = \"MOVE WEST\"\n",
    "        elif action.target_resource_id:\n",
    "            action_str = f\"GATHER {action.target_resource_id}\"\n",
    "        elif action.resource_type and action.quantity:\n",
    "            action_str = f\"DEPOSIT {action.resource_type.value} {action.quantity}\"\n",
    "        elif action.ship_component:\n",
    "            action_str = f\"BUILD {action.ship_component.value}\"\n",
    "        elif action.target_sailor:\n",
    "            if action.action_type == ActionType.OFFER_FOOD:\n",
    "                action_str = f\"POISON {action.target_sailor}\"\n",
    "            else:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "        \n",
    "        correction = {\n",
    "            \"input\": student_response,\n",
    "            \"output\": f\"REASONING: {teacher_result['critique']}\\nACTION: {action_str}\",\n",
    "            \"penalty\": teacher_result[\"penalty\"],\n",
    "            \"critique\": teacher_result[\"critique\"]\n",
    "        }\n",
    "        \n",
    "        correction_dataset.append(correction)\n",
    "\n",
    "print(\"‚úÖ Correction collector initialized\")\n",
    "print(\"   Format: (student_wrong) ‚Üí (teacher_correct + critique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bb0cb",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Define SFT Correction Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c0f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SFT trainer defined\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def run_sft_correction_pass(correction_examples: list, num_epochs: int = 1):\n",
    "    \"\"\"\n",
    "    Run supervised fine-tuning on collected corrections.\n",
    "    Teaches student to mimic teacher's correct format.\n",
    "    \"\"\"\n",
    "    if len(correction_examples) == 0:\n",
    "        print(\"‚ö†Ô∏è  No corrections to train on\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéì SFT CORRECTION PASS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Examples: {len(correction_examples)}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    \n",
    "    # Convert to chat format\n",
    "    sft_data = []\n",
    "    for example in correction_examples:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Fix this invalid action:\\n{example['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        sft_data.append({\"text\": text})\n",
    "    \n",
    "    sft_dataset = Dataset.from_list(sft_data)\n",
    "    \n",
    "    # SFT configuration\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"outputs_marooned_rl/sft_corrections\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        max_seq_length=2048,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=student_model,\n",
    "        args=sft_config,\n",
    "        train_dataset=sft_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    result = sft_trainer.train()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SFT complete! Loss: {result.training_loss:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ SFT trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb41e6a",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Setup PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d66edc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root|WARNING]The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "import numpy as np\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"outputs_marooned_rl\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    seed=42,\n",
    "    num_ppo_epochs=4,\n",
    "    kl_coef=0.2,\n",
    "    vf_coef=0.1,\n",
    "    cliprange=0.2,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    temperature=0.3,\n",
    "    response_length=256,\n",
    ")\n",
    "\n",
    "# Wrap student model with value head\n",
    "model_with_value = AutoModelForCausalLMWithValueHead.from_pretrained(student_model)\n",
    "\n",
    "# Compatibility patches\n",
    "base_model = model_with_value.pretrained_model\n",
    "if not hasattr(model_with_value, \"base_model_prefix\"):\n",
    "    model_with_value.base_model_prefix = getattr(base_model, \"base_model_prefix\", \"model\")\n",
    "setattr(model_with_value, model_with_value.base_model_prefix, base_model)\n",
    "if not hasattr(model_with_value, \"config\"):\n",
    "    model_with_value.config = base_model.config\n",
    "if not hasattr(model_with_value, \"generation_config\"):\n",
    "    model_with_value.generation_config = base_model.generation_config\n",
    "if hasattr(base_model, \"is_gradient_checkpointing\"):\n",
    "    model_with_value.is_gradient_checkpointing = base_model.is_gradient_checkpointing\n",
    "else:\n",
    "    model_with_value.is_gradient_checkpointing = False\n",
    "\n",
    "# Minimal dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [\"stub\"],\n",
    "    \"response\": [\"stub\"],\n",
    "    \"reward\": [0.0],\n",
    "})\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=model_with_value,\n",
    "    ref_model=None,\n",
    "    reward_model=model_with_value,\n",
    "    value_model=model_with_value,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01286b8a",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Hybrid RL + SFT Training Loop\n",
    "\n",
    "**Training Flow:**\n",
    "1. **RL Phase:** Student plays episodes, teacher validates, collect corrections\n",
    "2. **SFT Phase (every 25 steps):** Train on corrections, clear dataset\n",
    "3. **Repeat:** Continue RL with improved format knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f517380",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Test Configuration (Reduce Training Load)\n",
    "\n",
    "**For initial testing, use these reduced parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfdd51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° REDUCED CONFIGURATION FOR TESTING:\n",
      "   Training steps: 5\n",
      "   Episode max turns: 10\n",
      "   Batch size: 1\n",
      "   SFT interval: 10\n",
      "\n",
      "   Estimated time: ~8.3 minutes\n",
      "   (assumes ~2sec per teacher API call)\n",
      "\n",
      "‚ö†Ô∏è  Run this cell, then RESTART the training loop cell!\n"
     ]
    }
   ],
   "source": [
    "# TEMPORARY: Reduce load for testing\n",
    "# Comment these out once you confirm training works\n",
    "\n",
    "NUM_TRAINING_STEPS = 5          # Was 100 - test with just 5 steps\n",
    "EPISODE_MAX_TURNS = 10          # Was 100 - shorter episodes\n",
    "BATCH_SIZE = 1                  # Was 4 - single episode per step\n",
    "SFT_INTERVAL = 10               # Was 25 - faster SFT testing\n",
    "\n",
    "print(\"‚ö° REDUCED CONFIGURATION FOR TESTING:\")\n",
    "print(f\"   Training steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   Episode max turns: {EPISODE_MAX_TURNS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   SFT interval: {SFT_INTERVAL}\")\n",
    "print(f\"\\n   Estimated time: ~{NUM_TRAINING_STEPS * EPISODE_MAX_TURNS * 5 * 2 / 60:.1f} minutes\")\n",
    "print(f\"   (assumes ~2sec per teacher API call)\")\n",
    "print(f\"\\n‚ö†Ô∏è  Run this cell, then RESTART the training loop cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from config import MAX_ENERGY, MapLevel\n",
    "\n",
    "def visualize_game_state(env, turn_num, sailor_actions=None, sailor_reasoning=None):\n",
    "    \"\"\"\n",
    "    Display comprehensive game state visualization with map, sailor status, and actions.\n",
    "    \n",
    "    Args:\n",
    "        env: MaroonedEnv instance\n",
    "        turn_num: Current turn number\n",
    "        sailor_actions: Dict of sailor_id -> action string\n",
    "        sailor_reasoning: Dict of sailor_id -> reasoning string\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Header\n",
    "    output.append(\"=\"*100)\n",
    "    output.append(f\"üè¥‚Äç‚ò†Ô∏è  TURN {turn_num} | DAY {env.state.current_day} | GAME STATE\")\n",
    "    output.append(\"=\"*100)\n",
    "    \n",
    "    # Sailor Status Table\n",
    "    output.append(\"\\nüìä SAILOR STATUS:\")\n",
    "    output.append(\"‚îÄ\"*100)\n",
    "    output.append(f\"{'Name':<8} | {'Role':<10} | {'HP':<10} | {'Energy':<12} | {'Position':<15} | {'Status':<10}\")\n",
    "    output.append(\"‚îÄ\"*100)\n",
    "    \n",
    "    for sailor_id in env.agents:\n",
    "        sailor = env.state.sailors[sailor_id]\n",
    "        role = sailor.role.value\n",
    "        \n",
    "        # Health bar\n",
    "        hp_icon = \"üíö\" if sailor.alive else \"üíÄ\"\n",
    "        hp_str = f\"{hp_icon} {'ALIVE' if sailor.alive else 'DEAD'}\"\n",
    "        \n",
    "        # Energy bar (visual representation)\n",
    "        energy_pct = sailor.energy / MAX_ENERGY\n",
    "        energy_bars = int(energy_pct * 10)\n",
    "        energy_visual = \"‚ñà\" * energy_bars + \"‚ñë\" * (10 - energy_bars)\n",
    "        energy_str = f\"{energy_visual} {sailor.energy}/{MAX_ENERGY}\"\n",
    "        \n",
    "        # Position\n",
    "        pos = sailor.position\n",
    "        pos_str = f\"({pos.x},{pos.y}) {pos.level.name}\"\n",
    "        \n",
    "        # Status indicators\n",
    "        status_parts = []\n",
    "        if not sailor.alive:\n",
    "            status_parts.append(\"DEAD\")\n",
    "        if sailor_id == env.state.traitor_id:\n",
    "            status_parts.append(\"üî™TRAITOR\")\n",
    "        if sailor.poisoned:\n",
    "            status_parts.append(\"‚ò†Ô∏è POISON\")\n",
    "        status_str = \" \".join(status_parts) if status_parts else \"OK\"\n",
    "        \n",
    "        output.append(f\"{sailor_id:<8} | {role:<10} | {hp_str:<10} | {energy_str:<12} | {pos_str:<15} | {status_str:<10}\")\n",
    "    \n",
    "    output.append(\"‚îÄ\"*100)\n",
    "    \n",
    "    # Ship Progress\n",
    "    ship = env.state.ship_progress\n",
    "    ship_pct = ship.total_percentage\n",
    "    ship_bars = int(ship_pct / 10)\n",
    "    ship_visual = \"‚ñì\" * ship_bars + \"‚ñë\" * (10 - ship_bars)\n",
    "    output.append(f\"\\nüö¢ SHIP PROGRESS: {ship_visual} {ship_pct:.1f}%\")\n",
    "    output.append(f\"   Hull: {ship.hull}/{ship.hull_max} | Sail: {ship.sail}/{ship.sail_max} | Engine: {ship.engine}/{ship.engine_max}\")\n",
    "    \n",
    "    # Base Storage\n",
    "    storage = env.state.base_storage\n",
    "    output.append(f\"\\nüì¶ BASE STORAGE:\")\n",
    "    output.append(f\"   üå≤ Wood: {storage.wood} | ‚öôÔ∏è Metal: {storage.metal} | üçé Food: {storage.food} | üåø Antidote: {storage.antidote_herbs}\")\n",
    "    \n",
    "    # Actions this turn (if provided)\n",
    "    if sailor_actions:\n",
    "        output.append(f\"\\n‚öîÔ∏è  ACTIONS THIS TURN:\")\n",
    "        output.append(\"‚îÄ\"*100)\n",
    "        for sailor_id, action in sailor_actions.items():\n",
    "            sailor = env.state.sailors[sailor_id]\n",
    "            if not sailor.alive:\n",
    "                continue\n",
    "            \n",
    "            reasoning = sailor_reasoning.get(sailor_id, \"N/A\") if sailor_reasoning else \"N/A\"\n",
    "            # Truncate long reasoning\n",
    "            reasoning_short = (reasoning[:70] + \"...\") if len(reasoning) > 70 else reasoning\n",
    "            \n",
    "            output.append(f\"  [{sailor_id}] {action}\")\n",
    "            output.append(f\"          üí≠ {reasoning_short}\")\n",
    "        output.append(\"‚îÄ\"*100)\n",
    "    \n",
    "    # Map visualization (all 3 levels side by side)\n",
    "    output.append(f\"\\nüó∫Ô∏è  ISLAND MAP (Day {env.state.current_day}):\")\n",
    "    output.append(\"\")\n",
    "    \n",
    "    # Render all three levels\n",
    "    for level in [MapLevel.GROUND, MapLevel.MOUNTAIN, MapLevel.CAVE]:\n",
    "        map_str = env.render_map(level, use_emoji=True)\n",
    "        output.append(map_str)\n",
    "    \n",
    "    output.append(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    # Print everything\n",
    "    print(\"\\n\".join(output))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Game state visualization function loaded\")\n",
    "print(\"   Use: visualize_game_state(env, turn_num, sailor_actions, sailor_reasoning)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fecdf",
   "metadata": {},
   "source": [
    "## üéÆ Test Game Visualization (Optional)\n",
    "\n",
    "Before training, you can test the visualization with a quick demo episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST VISUALIZATION - Run a quick 5-turn demo episode with visualization\n",
    "# This will show you what the training visualization looks like\n",
    "\n",
    "print(\"üé¨ Running demo episode with visualization...\")\n",
    "print(\"   This will show the full game state for 5 turns\\n\")\n",
    "\n",
    "demo_env = MaroonedEnv(seed=42, render_mode=\"ansi\")\n",
    "demo_obs = demo_env.reset(seed=42)\n",
    "\n",
    "for demo_turn in range(5):\n",
    "    # Collect actions for all sailors\n",
    "    demo_actions = {}\n",
    "    demo_reasoning = {}\n",
    "    \n",
    "    for sailor_id in demo_env.agents:\n",
    "        sailor = demo_env.state.sailors[sailor_id]\n",
    "        if not sailor.alive:\n",
    "            continue\n",
    "        \n",
    "        # Random action for demo\n",
    "        action = Action(sailor_id=sailor_id, action_type=ActionType.WAIT)\n",
    "        demo_actions[sailor_id] = f\"{action.action_type.value} (demo)\"\n",
    "        demo_reasoning[sailor_id] = \"This is a demo - just waiting\"\n",
    "    \n",
    "    # Show visualization\n",
    "    clear_output(wait=True)\n",
    "    visualize_game_state(demo_env, demo_turn, demo_actions, demo_reasoning)\n",
    "    \n",
    "    # Execute actions\n",
    "    actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in demo_env.agents}\n",
    "    demo_obs, _, dones, _, _ = demo_env.step(actions_dict)\n",
    "    \n",
    "    time.sleep(1.5)  # Pause to see each turn\n",
    "\n",
    "print(\"\\n‚úÖ Demo complete! You can now run the training loop.\")\n",
    "print(\"   The first episode will show this same visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b74b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Hybrid RL + SFT Training (SIMPLIFIED)\n",
      "   Steps: 5\n",
      "   Episode max turns: 10\n",
      "   Batch size: 1\n",
      "   SFT interval: Every 10 steps\n",
      "\n",
      "‚ö†Ô∏è  NOTE: This is a simplified training loop focused on SFT corrections\n",
      "   PPO updates are disabled due to UnslothPPOTrainer API limitations\n",
      "   The student learns primarily through teacher corrections\n",
      "\n",
      "\n",
      "üìç Step 1/5 - Episode 1/1\n",
      "\n",
      "üéÆ Starting episode (max 10 turns)...\n",
      "\n",
      "--- Turn 0 START ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Alice] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Diana] move_north      | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Diana] move_north      | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Eve] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 0 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 1 START ---\n",
      "  [Eve] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 0 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 1 START ---\n",
      "  [Alice] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Alice] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Diana] gather_resource | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Diana] gather_resource | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "--- Turn 1 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 2 START ---\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "--- Turn 1 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 2 START ---\n",
      "  [Alice] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Alice] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Diana] wait            | Env=-0.0 Penalty=-1.0 Total=-1.0\n",
      "  [Diana] wait            | Env=-0.0 Penalty=-1.0 Total=-1.0\n",
      "  [Eve] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 2 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 3 START ---\n",
      "  [Eve] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 2 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 3 START ---\n",
      "  [Alice] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Alice] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Diana] gather_resource | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Diana] gather_resource | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Eve] send_message    | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 3 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 4 START ---\n",
      "  [Eve] send_message    | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 3 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 4 START ---\n",
      "  [Alice] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Alice] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Eve] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 4 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 5 START ---\n",
      "  [Eve] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 4 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 5 START ---\n",
      "  [Alice] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Alice] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 5 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 6 START ---\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 5 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 6 START ---\n",
      "  [Alice] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Alice] wait            | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=-2.0 Total=-2.0\n",
      "  [Charlie] wait            | Env=+0.0 Penalty=-2.0 Total=-2.0\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=+0.0 Total=-0.0\n",
      "  [Eve] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 6 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 7 START ---\n",
      "  [Eve] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "--- Turn 6 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 7 START ---\n",
      "  [Alice] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Alice] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_north      | Env=+0.0 Penalty=-1.0 Total=-1.0\n",
      "  [Bob] move_north      | Env=+0.0 Penalty=-1.0 Total=-1.0\n",
      "  [Charlie] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 7 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 8 START ---\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 7 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 8 START ---\n",
      "  [Alice] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Alice] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Bob] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] gather_resource | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 8 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 9 START ---\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 8 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "--- Turn 9 START ---\n",
      "  [Alice] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Alice] move_north      | Env=+0.0 Penalty=+0.0 Total=+0.0\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Bob] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Charlie] move_east       | Env=+0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Diana] move_east       | Env=-0.0 Penalty=-0.5 Total=-0.5\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 9 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "üìä Episode complete: 50 actions, total reward: -9.8\n",
      "   ‚úì Episode complete: 50 actions, reward: -9.8\n",
      "\n",
      "================================================================================\n",
      "Step 001/5 | Reward:   -9.8 | Avg(10):   -9.8 | Corrections:  101 | Time: 480.6s\n",
      "================================================================================\n",
      "\n",
      "üìç Step 2/5 - Episode 1/1\n",
      "  [Eve] move_east       | Env=+0.0 Penalty=-0.1 Total=-0.1\n",
      "--- Turn 9 COMPLETE: 5 sailors acted ---\n",
      "\n",
      "üìä Episode complete: 50 actions, total reward: -9.8\n",
      "   ‚úì Episode complete: 50 actions, reward: -9.8\n",
      "\n",
      "================================================================================\n",
      "Step 001/5 | Reward:   -9.8 | Avg(10):   -9.8 | Corrections:  101 | Time: 480.6s\n",
      "================================================================================\n",
      "\n",
      "üìç Step 2/5 - Episode 1/1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Use variables from config cell above if defined, otherwise defaults\n",
    "if 'NUM_TRAINING_STEPS' not in dir():\n",
    "    NUM_TRAINING_STEPS = 100\n",
    "    EPISODE_MAX_TURNS = 100\n",
    "    BATCH_SIZE = 4\n",
    "    SFT_INTERVAL = 25\n",
    "    print(\"‚ö†Ô∏è  Using default configuration - run config cell above to reduce load!\")\n",
    "\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=None, verbose=False, visualize=False):\n",
    "    \"\"\"\n",
    "    Play one episode with teacher validation.\n",
    "    Returns training data (prompts, responses) and rewards.\n",
    "    \n",
    "    Args:\n",
    "        max_turns: Maximum turns per episode\n",
    "        verbose: Print detailed action logs\n",
    "        visualize: Show full game state visualization every turn\n",
    "    \"\"\"\n",
    "    if max_turns is None:\n",
    "        max_turns = EPISODE_MAX_TURNS\n",
    "        \n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    # Collect episode data\n",
    "    episode_data = []\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéÆ Starting episode (max {max_turns} turns)...\")\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Collect turn data for visualization\n",
    "        turn_actions = {}\n",
    "        turn_reasoning = {}\n",
    "        turn_actions_count = 0\n",
    "        \n",
    "        for sailor_id in sailor_ids:\n",
    "            sailor = env.state.sailors[sailor_id]\n",
    "            \n",
    "            # Skip dead sailors\n",
    "            if not sailor.alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = sailor.role.value\n",
    "            \n",
    "            # Student generates action\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            student_response = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Parse reasoning from student response\n",
    "            reasoning = \"N/A\"\n",
    "            if \"REASONING:\" in student_response:\n",
    "                try:\n",
    "                    reasoning = student_response.split(\"REASONING:\")[1].split(\"ACTION:\")[0].strip()\n",
    "                except:\n",
    "                    reasoning = student_response[:100]\n",
    "            \n",
    "            # Teacher validates and potentially corrects\n",
    "            teacher_result = teacher_validate_student_output(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # Collect correction if needed\n",
    "            add_correction_example(student_response, teacher_result, obs)\n",
    "            \n",
    "            # Execute action in environment (only this sailor acts, others WAIT)\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # Combined reward (environment + process penalty)\n",
    "            step_reward = env_reward + process_penalty\n",
    "            total_reward += step_reward\n",
    "            \n",
    "            # Store for visualization\n",
    "            action_str = action.action_type.value\n",
    "            if action.target_position:\n",
    "                action_str = f\"{action.action_type.value}\"\n",
    "            elif action.target_resource_id:\n",
    "                action_str = f\"{action.action_type.value} (resource {action.target_resource_id})\"\n",
    "            elif action.target_sailor:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "            \n",
    "            turn_actions[sailor_id] = f\"{action_str} | Reward: {step_reward:+.1f}\"\n",
    "            turn_reasoning[sailor_id] = reasoning\n",
    "            \n",
    "            # Store training example\n",
    "            episode_data.append({\n",
    "                \"prompt\": full_prompt,\n",
    "                \"response\": student_response,\n",
    "                \"reward\": step_reward,\n",
    "                \"sailor_id\": sailor_id,\n",
    "                \"action\": action.action_type.value,\n",
    "            })\n",
    "            \n",
    "            turn_actions_count += 1\n",
    "        \n",
    "        # Visualize game state after all sailors acted this turn\n",
    "        if visualize and turn_actions_count > 0:\n",
    "            clear_output(wait=True)\n",
    "            visualize_game_state(env, turn, turn_actions, turn_reasoning)\n",
    "            time.sleep(0.5)  # Brief pause to see the state\n",
    "        \n",
    "        if verbose and not visualize:\n",
    "            print(f\"--- Turn {turn}: {turn_actions_count} sailors acted ---\")\n",
    "        \n",
    "        # Check if episode is over\n",
    "        if env.state.game_over or all(dones.values()):\n",
    "            if verbose or visualize:\n",
    "                print(f\"\\n‚úÖ Episode ended at turn {turn}: game_over={env.state.game_over}\")\n",
    "            break\n",
    "        \n",
    "        # Early exit if no one acted (all dead)\n",
    "        if turn_actions_count == 0:\n",
    "            if verbose or visualize:\n",
    "                print(f\"\\n‚ö†Ô∏è  No sailors acted at turn {turn} (all dead)\")\n",
    "            break\n",
    "    \n",
    "    if verbose or visualize:\n",
    "        print(f\"\\nüìä Episode complete: {len(episode_data)} actions, total reward: {total_reward:.1f}\")\n",
    "    \n",
    "    return episode_data, total_reward\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLIFIED TRAINING LOOP (Without PPO.step)\n",
    "# ============================================================================\n",
    "print(\"üöÄ Starting Hybrid RL + SFT Training (SIMPLIFIED)\")\n",
    "print(f\"   Steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   Episode max turns: {EPISODE_MAX_TURNS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   SFT interval: Every {SFT_INTERVAL} steps\\n\")\n",
    "print(\"‚ö†Ô∏è  NOTE: This is a simplified training loop focused on SFT corrections\")\n",
    "print(\"   PPO updates are disabled due to UnslothPPOTrainer API limitations\")\n",
    "print(\"   The student learns primarily through teacher corrections\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "stats_sft_runs = 0\n",
    "stats_corrections = []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_data = []\n",
    "    batch_reward = 0.0\n",
    "    \n",
    "    # RL Phase: Collect episodes\n",
    "    for episode_num in range(BATCH_SIZE):\n",
    "        print(f\"\\nüìç Step {step+1}/{NUM_TRAINING_STEPS} - Episode {episode_num+1}/{BATCH_SIZE}\")\n",
    "        \n",
    "        # Only visualize the first episode of the first step\n",
    "        should_visualize = (step == 0 and episode_num == 0)\n",
    "        \n",
    "        episode_data, episode_reward = generate_episode_with_teacher(\n",
    "            max_turns=EPISODE_MAX_TURNS,\n",
    "            verbose=False,  # Disable verbose when visualizing\n",
    "            visualize=should_visualize\n",
    "        )\n",
    "        \n",
    "        batch_data.extend(episode_data)\n",
    "        batch_reward += episode_reward\n",
    "        \n",
    "        print(f\"   ‚úì Episode complete: {len(episode_data)} actions, reward: {episode_reward:.1f}\")\n",
    "    \n",
    "    stats_rewards.append(batch_reward)\n",
    "    stats_corrections.append(len(correction_dataset))\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Step {step+1:03d}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {batch_reward:+6.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+6.1f} | \"\n",
    "          f\"Corrections: {len(correction_dataset):4d} | \"\n",
    "          f\"Time: {elapsed:4.1f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # SFT Phase: Train on corrections\n",
    "    if (step + 1) % SFT_INTERVAL == 0 and len(correction_dataset) >= 10:\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"üéì SFT PASS #{stats_sft_runs + 1}\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        run_sft_correction_pass(correction_dataset, num_epochs=1)\n",
    "        stats_sft_runs += 1\n",
    "        correction_dataset.clear()\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        student_model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT passes: {stats_sft_runs}\")\n",
    "print(f\"   Avg reward: {np.mean(stats_rewards):.2f}\")\n",
    "print(f\"   Final avg (10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(f\"   Total corrections: {sum(stats_corrections)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8997aec",
   "metadata": {},
   "source": [
    "## üîü Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Rewards over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stats_rewards, alpha=0.3, label='Raw Reward')\n",
    "window = 10\n",
    "if len(stats_rewards) >= window:\n",
    "    ma_rewards = np.convolve(stats_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(stats_rewards)), ma_rewards, label=f'MA({window})', linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Hybrid RL + SFT Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark SFT passes\n",
    "for i in range(stats_sft_runs):\n",
    "    sft_step = (i + 1) * SFT_INTERVAL\n",
    "    if sft_step < len(stats_rewards):\n",
    "        plt.axvline(x=sft_step, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Improvement distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "improvement = np.diff(stats_rewards)\n",
    "plt.hist(improvement, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Reward Change')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Step-to-Step Improvement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_marooned_rl/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Initial avg (10): {np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   Final avg (10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(f\"   Improvement: {np.mean(stats_rewards[-10:]) - np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   SFT passes: {stats_sft_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c7418",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"outputs_marooned_rl/final_model\"\n",
    "\n",
    "student_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "print(f\"\\nTo load:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from unsloth import FastLanguageModel\")\n",
    "print(f\"model, tokenizer = FastLanguageModel.from_pretrained('{output_dir}')\")\n",
    "print(f\"```\")\n",
    "print(f\"\\nüéâ Training complete with hybrid RL + SFT approach!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
