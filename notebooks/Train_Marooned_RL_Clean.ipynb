{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b3f11b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üè¥‚Äç‚ò†Ô∏è MAROONED: Hybrid RL + SFT Training\n",
    "\n",
    "### Process Reward Modeling with Supervised Correction\n",
    "\n",
    "**OpenEnv Hackathon 2025**\n",
    "\n",
    "[![OpenEnv](https://img.shields.io/badge/Framework-OpenEnv-blue)](https://github.com/openenv)\n",
    "[![Llama](https://img.shields.io/badge/Model-Llama_3.1_8B-green)](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instinct)\n",
    "[![Hardware](https://img.shields.io/badge/Hardware-AMD_MI300X-red)](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Training Architecture\n",
    "\n",
    "**Hybrid Approach: RL (strategy) + SFT (format learning)**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RL PHASE: PPO Training (Episodes 1-4)          ‚îÇ\n",
    "‚îÇ  Student ‚Üí Teacher (vLLM) ‚Üí Env ‚Üí Rewards       ‚îÇ\n",
    "‚îÇ  Collect corrections: wrong ‚Üí correct           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ Every 25 steps\n",
    "                   ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  SFT PHASE: Supervised Fine-Tuning              ‚îÇ\n",
    "‚îÇ  Train on corrections: mimic teacher format     ‚îÇ\n",
    "‚îÇ  Clear dataset, continue RL                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Innovation:** Student learns format directly from teacher critiques via periodic SFT passes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a311e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "**Start vLLM teacher server in a separate terminal:**\n",
    "\n",
    "```bash\n",
    "pip install vllm\n",
    "\n",
    "vllm serve unsloth/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --port 8000\n",
    "```\n",
    "\n",
    "**Verify:**\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "Expected: `{\"data\": [{\"id\": \"unsloth/Meta-Llama-3.1-8B-Instruct\", ...}]}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5f633",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0a454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: No virtual environment found; run `\u001b[32muv venv\u001b[39m` to create an environment, or pass `\u001b[32m--system\u001b[39m` to install into a non-virtual environment\n",
      "‚úÖ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cb3979-4a39-4266-bfba-53f78c64a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0.51831-a3e329ad8\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.hip)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf7bf6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load MAROONED Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e064ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MAROONED environment loaded\n",
      "‚úÖ Teacher validation API imported\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import (\n",
    "    get_system_prompt,\n",
    "    observation_to_prompt,\n",
    "    teacher_validate_student_output,\n",
    ")\n",
    "from config import ActionType, ResourceType, MapLevel, ShipComponent\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"‚úÖ MAROONED environment loaded\")\n",
    "print(\"‚úÖ Teacher validation API imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f4097",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Student Model (Llama 3.1 8B with LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533f33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.9.0a0+git1c57644 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-29 20:31:41 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    AMD GPU Device. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cfd2b86d364c229b5af27e0208b0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.11 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Student Model: Llama 3.1 8B (BF16, LoRA rank=16)\n",
      "   GPU: \n",
      "   VRAM: 191.7 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_NO_TQDM\"] = \"1\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "max_seq_length = 16384\n",
    "lora_rank = 16\n",
    "\n",
    "student_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/root/llama3_8b\",  # your local path\n",
    "    load_in_4bit = False,\n",
    "    dtype = torch.bfloat16,\n",
    "    max_seq_length = 16384,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.0,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Student Model: Llama 3.1 8B (BF16, LoRA rank={lora_rank})\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdad8b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify vLLM Teacher Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking vLLM teacher server...\n",
      "‚úÖ vLLM server running!\n",
      "   Model: ['Unsloth/Llama-3.3-70B-Instruct']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "VLLM_API_URL = \"http://localhost:8001/v1/chat/completions\"\n",
    "VLLM_MODELS_URL = \"http://localhost:8001/v1/models\"\n",
    "\n",
    "print(\"Checking vLLM teacher server...\")\n",
    "try:\n",
    "    response = requests.get(VLLM_MODELS_URL, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(f\"‚úÖ vLLM server running!\")\n",
    "        print(f\"   Model: {[m['id'] for m in models.get('data', [])]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå vLLM server not reachable!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"\\n   Start server in separate terminal:\")\n",
    "    print(f\"   vllm serve unsloth/Meta-Llama-3.1-8B-Instruct --dtype bfloat16 --port 8001\")\n",
    "    raise SystemExit(\"Teacher server required for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07d318",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Teacher Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ TESTING TEACHER VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Sending test cases to teacher...\n",
      "\n",
      "Test 1: MOVING NORTH\n",
      "‚ö†Ô∏è  Teacher API error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bff7f7ce180>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "   Valid: False\n",
      "   Corrected: wait\n",
      "   Penalty: -2.0\n",
      "   Critique: Teacher API unavailable - defaulting to WAIT...\n",
      "\n",
      "Test 2: CHECK_STATUS\n",
      "‚ö†Ô∏è  Teacher API error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bff7f7ce720>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "   Valid: False\n",
      "   Corrected: wait\n",
      "   Penalty: -2.0\n",
      "   Critique: Teacher API unavailable - defaulting to WAIT...\n",
      "\n",
      "Test 3: GATHER wood\n",
      "‚ö†Ô∏è  Teacher API error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bff7f7cd940>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "   Valid: False\n",
      "   Corrected: wait\n",
      "   Penalty: -2.0\n",
      "   Critique: Teacher API unavailable - defaulting to WAIT...\n",
      "\n",
      "Test 4: MOVE NORTH\n",
      "‚ö†Ô∏è  Teacher API error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bff7f7ceff0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "   Valid: False\n",
      "   Corrected: wait\n",
      "   Penalty: -2.0\n",
      "   Critique: Teacher API unavailable - defaulting to WAIT...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Teacher API working correctly!\n"
     ]
    }
   ],
   "source": [
    "from config import MAX_ENERGY\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üß™ TESTING TEACHER VALIDATION WITH FULL CONTEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create environment and get real observation\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "print(f\"\\nüìã Test Setup:\")\n",
    "print(f\"   Sailor: Alice\")\n",
    "print(f\"   Role: {alice_role.upper()}\")\n",
    "print(f\"   Position: {alice_obs.position}\")\n",
    "print(f\"   Energy: {alice_obs.energy}/{MAX_ENERGY}\")\n",
    "print(f\"   Visible resources: {len(alice_obs.visible_resources) if hasattr(alice_obs, 'visible_resources') else 'N/A'}\")\n",
    "\n",
    "# Get proper system and user prompts\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "print(f\"\\nüìè Prompt sizes:\")\n",
    "print(f\"   System prompt: {len(system_prompt)} chars\")\n",
    "print(f\"   User prompt: {len(user_prompt)} chars\")\n",
    "print(f\"   Total context: {len(system_prompt) + len(user_prompt)} chars\")\n",
    "\n",
    "# Test cases - simulating what an untrained student LLM might output\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Format Error (MOVING instead of MOVE)\",\n",
    "        \"output\": \"REASONING: I should move northeast to explore\\nACTION: MOVING NORTH\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Invalid Command (CHECK_STATUS)\",\n",
    "        \"output\": \"REASONING: Let me check my status\\nACTION: CHECK_STATUS\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Missing Resource ID\",\n",
    "        \"output\": \"REASONING: I see wood nearby, gathering it\\nACTION: GATHER wood\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Truncated Output\",\n",
    "        \"output\": \"REASONING: As the traitor, I should sabotagin\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correct Format\",\n",
    "        \"output\": \"REASONING: Moving north to explore the area\\nACTION: MOVE NORTH\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üî¨ RUNNING TEACHER VALIDATION TESTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: {test['name']}\")\n",
    "    print(f\"   Student output: {test['output'][:60]}...\")\n",
    "    \n",
    "    # Call teacher validation with full context\n",
    "    result = teacher_validate_student_output(\n",
    "        student_response=test['output'],\n",
    "        observation=alice_obs,\n",
    "        sailor_id=\"Alice\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    validity_icon = \"‚úÖ\" if result['valid'] else \"‚ùå\"\n",
    "    print(f\"   {validity_icon} Valid: {result['valid']}\")\n",
    "    print(f\"   üîß Corrected action: {result['action'].action_type.value}\")\n",
    "    print(f\"   üí∞ Process penalty: {result['penalty']}\")\n",
    "    print(f\"   üí¨ Critique: {result['critique'][:80]}\")\n",
    "    \n",
    "    # Show what would happen\n",
    "    if result['valid']:\n",
    "        print(f\"   ‚úÖ Action executes as-is (no penalty)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Teacher corrected ‚Üí student gets penalty {result['penalty']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ TEACHER VALIDATION API WORKING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"  ‚Ä¢ Teacher receives full game context (observation + system prompt)\")\n",
    "print(\"  ‚Ä¢ Invalid formats get corrected automatically\")\n",
    "print(\"  ‚Ä¢ Process penalties guide student learning\")\n",
    "print(\"  ‚Ä¢ Student focuses on strategy, not syntax\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c037ab9",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Setup Correction Dataset for SFT\n",
    "\n",
    "Collect student errors and teacher corrections during RL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_dataset = []\n",
    "\n",
    "def add_correction_example(student_response: str, teacher_result: dict, observation: Observation):\n",
    "    \"\"\"Store invalid outputs for SFT training.\"\"\"\n",
    "    if not teacher_result[\"valid\"]:\n",
    "        action = teacher_result[\"action\"]\n",
    "        action_str = f\"{action.action_type.value}\"\n",
    "        \n",
    "        # Format action string with parameters\n",
    "        if action.target_position:\n",
    "            if \"NORTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE NORTH\"\n",
    "            elif \"SOUTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE SOUTH\"\n",
    "            elif \"EAST\" in action.action_type.value:\n",
    "                action_str = \"MOVE EAST\"\n",
    "            elif \"WEST\" in action.action_type.value:\n",
    "                action_str = \"MOVE WEST\"\n",
    "        elif action.target_resource_id:\n",
    "            action_str = f\"GATHER {action.target_resource_id}\"\n",
    "        elif action.resource_type and action.quantity:\n",
    "            action_str = f\"DEPOSIT {action.resource_type.value} {action.quantity}\"\n",
    "        elif action.ship_component:\n",
    "            action_str = f\"BUILD {action.ship_component.value}\"\n",
    "        elif action.target_sailor:\n",
    "            if action.action_type == ActionType.OFFER_FOOD:\n",
    "                action_str = f\"POISON {action.target_sailor}\"\n",
    "            else:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "        \n",
    "        correction = {\n",
    "            \"input\": student_response,\n",
    "            \"output\": f\"REASONING: {teacher_result['critique']}\\nACTION: {action_str}\",\n",
    "            \"penalty\": teacher_result[\"penalty\"],\n",
    "            \"critique\": teacher_result[\"critique\"]\n",
    "        }\n",
    "        \n",
    "        correction_dataset.append(correction)\n",
    "\n",
    "print(\"‚úÖ Correction collector initialized\")\n",
    "print(\"   Format: (student_wrong) ‚Üí (teacher_correct + critique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bb0cb",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Define SFT Correction Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def run_sft_correction_pass(correction_examples: list, num_epochs: int = 1):\n",
    "    \"\"\"\n",
    "    Run supervised fine-tuning on collected corrections.\n",
    "    Teaches student to mimic teacher's correct format.\n",
    "    \"\"\"\n",
    "    if len(correction_examples) == 0:\n",
    "        print(\"‚ö†Ô∏è  No corrections to train on\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéì SFT CORRECTION PASS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Examples: {len(correction_examples)}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    \n",
    "    # Convert to chat format\n",
    "    sft_data = []\n",
    "    for example in correction_examples:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Fix this invalid action:\\n{example['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        sft_data.append({\"text\": text})\n",
    "    \n",
    "    sft_dataset = Dataset.from_list(sft_data)\n",
    "    \n",
    "    # SFT configuration\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"outputs_marooned_rl/sft_corrections\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        max_seq_length=2048,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=student_model,\n",
    "        args=sft_config,\n",
    "        train_dataset=sft_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    result = sft_trainer.train()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SFT complete! Loss: {result.training_loss:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ SFT trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb41e6a",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Setup PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66edc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "import numpy as np\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"outputs_marooned_rl\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    seed=42,\n",
    "    num_ppo_epochs=4,\n",
    "    kl_coef=0.2,\n",
    "    vf_coef=0.1,\n",
    "    cliprange=0.2,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    temperature=0.3,\n",
    "    response_length=256,\n",
    ")\n",
    "\n",
    "# Wrap student model with value head\n",
    "model_with_value = AutoModelForCausalLMWithValueHead.from_pretrained(student_model)\n",
    "\n",
    "# Compatibility patches\n",
    "base_model = model_with_value.pretrained_model\n",
    "if not hasattr(model_with_value, \"base_model_prefix\"):\n",
    "    model_with_value.base_model_prefix = getattr(base_model, \"base_model_prefix\", \"model\")\n",
    "setattr(model_with_value, model_with_value.base_model_prefix, base_model)\n",
    "if not hasattr(model_with_value, \"config\"):\n",
    "    model_with_value.config = base_model.config\n",
    "if not hasattr(model_with_value, \"generation_config\"):\n",
    "    model_with_value.generation_config = base_model.generation_config\n",
    "if hasattr(base_model, \"is_gradient_checkpointing\"):\n",
    "    model_with_value.is_gradient_checkpointing = base_model.is_gradient_checkpointing\n",
    "else:\n",
    "    model_with_value.is_gradient_checkpointing = False\n",
    "\n",
    "# Minimal dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [\"stub\"],\n",
    "    \"response\": [\"stub\"],\n",
    "    \"reward\": [0.0],\n",
    "})\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=model_with_value,\n",
    "    ref_model=None,\n",
    "    reward_model=model_with_value,\n",
    "    value_model=model_with_value,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01286b8a",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Hybrid RL + SFT Training Loop\n",
    "\n",
    "**Training Flow:**\n",
    "1. **RL Phase:** Student plays episodes, teacher validates, collect corrections\n",
    "2. **SFT Phase (every 25 steps):** Train on corrections, clear dataset\n",
    "3. **Repeat:** Continue RL with improved format knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b74b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPISODE_MAX_SEQ_LENGTH = 16384\n",
    "SFT_INTERVAL = 25\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=100, verbose=False):\n",
    "    \"\"\"Play episode with teacher validation and correction collection.\"\"\"\n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    query_tensors, response_tensors, rewards_list = [], [], []\n",
    "    \n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        for sailor_id in sailor_ids:\n",
    "            if not env.state.sailors[sailor_id].alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = env.state.sailors[sailor_id].role.value\n",
    "            \n",
    "            # Student generates\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            query_tensor = inputs[\"input_ids\"][0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                response_tensor = outputs[0]\n",
    "            \n",
    "            student_response = tokenizer.decode(response_tensor[len(query_tensor):], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Teacher validates\n",
    "            teacher_result = teacher_validate_student_output(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # Collect correction\n",
    "            add_correction_example(student_response, teacher_result, obs)\n",
    "            \n",
    "            # Environment executes\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # Combined reward\n",
    "            total_reward = env_reward + process_penalty\n",
    "            \n",
    "            query_tensors.append(query_tensor)\n",
    "            response_tensors.append(response_tensor[len(query_tensor):])\n",
    "            rewards_list.append(torch.tensor(total_reward, dtype=torch.float32))\n",
    "            \n",
    "            if verbose and turn % 20 == 0:\n",
    "                print(f\"Turn {turn:03d} | {sailor_id}: {action.action_type.value:<12} | \"\n",
    "                      f\"Env={env_reward:+.1f} Penalty={process_penalty:+.1f} Total={total_reward:+.1f}\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                return query_tensors, response_tensors, rewards_list\n",
    "    \n",
    "    return query_tensors, response_tensors, rewards_list\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print(\"üöÄ Starting Hybrid RL + SFT Training\")\n",
    "print(f\"   Steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT interval: Every {SFT_INTERVAL} steps\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "stats_sft_runs = 0\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_queries, batch_responses, batch_rewards = [], [], []\n",
    "    \n",
    "    # RL Phase\n",
    "    for _ in range(ppo_config.batch_size):\n",
    "        queries, responses, rewards = generate_episode_with_teacher(\n",
    "            max_turns=100,\n",
    "            verbose=(step % 50 == 0 and _ == 0)\n",
    "        )\n",
    "        batch_queries.extend(queries)\n",
    "        batch_responses.extend(responses)\n",
    "        batch_rewards.extend(rewards)\n",
    "    \n",
    "    # PPO update\n",
    "    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)\n",
    "    \n",
    "    episode_reward = sum([r.item() for r in batch_rewards])\n",
    "    stats_rewards.append(episode_reward)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"Step {step+1:03d}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {episode_reward:+6.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+6.1f} | \"\n",
    "          f\"Corrections: {len(correction_dataset):4d} | \"\n",
    "          f\"Time: {elapsed:4.1f}s\")\n",
    "    \n",
    "    # SFT Phase\n",
    "    if (step + 1) % SFT_INTERVAL == 0 and len(correction_dataset) >= 10:\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"üéì SFT PASS #{stats_sft_runs + 1}\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        run_sft_correction_pass(correction_dataset, num_epochs=1)\n",
    "        stats_sft_runs += 1\n",
    "        correction_dataset.clear()\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        ppo_trainer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint ‚Üí {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT passes: {stats_sft_runs}\")\n",
    "print(f\"   Avg reward: {np.mean(stats_rewards):.2f}\")\n",
    "print(f\"   Final (10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8997aec",
   "metadata": {},
   "source": [
    "## üîü Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Rewards over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stats_rewards, alpha=0.3, label='Raw Reward')\n",
    "window = 10\n",
    "if len(stats_rewards) >= window:\n",
    "    ma_rewards = np.convolve(stats_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(stats_rewards)), ma_rewards, label=f'MA({window})', linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Hybrid RL + SFT Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark SFT passes\n",
    "for i in range(stats_sft_runs):\n",
    "    sft_step = (i + 1) * SFT_INTERVAL\n",
    "    if sft_step < len(stats_rewards):\n",
    "        plt.axvline(x=sft_step, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Improvement distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "improvement = np.diff(stats_rewards)\n",
    "plt.hist(improvement, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Reward Change')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Step-to-Step Improvement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_marooned_rl/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Initial avg (10): {np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   Final avg (10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(f\"   Improvement: {np.mean(stats_rewards[-10:]) - np.mean(stats_rewards[:10]):.2f}\")\n",
    "print(f\"   SFT passes: {stats_sft_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c7418",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"outputs_marooned_rl/final_model\"\n",
    "\n",
    "student_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "print(f\"\\nTo load:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from unsloth import FastLanguageModel\")\n",
    "print(f\"model, tokenizer = FastLanguageModel.from_pretrained('{output_dir}')\")\n",
    "print(f\"```\")\n",
    "print(f\"\\nüéâ Training complete with hybrid RL + SFT approach!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
