{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b3f11b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# 🏴‍☠️ MAROONED: Hybrid RL + SFT Training\n",
    "\n",
    "### Process Reward Modeling with Supervised Correction\n",
    "\n",
    "**Synthetic Data AI Agents Challenge**\n",
    "\n",
    "[![OpenEnv](https://img.shields.io/badge/Framework-OpenEnv-blue)](https://github.com/openenv)\n",
    "[![Llama](https://img.shields.io/badge/Model-Llama_3.1_8B-green)](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instinct)\n",
    "[![Hardware](https://img.shields.io/badge/Hardware-AMD_MI300X-red)](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n",
    "[![Teacher](https://img.shields.io/badge/Teacher-Mixtral_8x7B-orange)](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Training Architecture\n",
    "\n",
    "**Hybrid Approach: RL (strategy) + SFT (format learning)**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  RL PHASE: PPO Training (Episodes 1-4)          │\n",
    "│  Student (Llama 3.1 8B) → Teacher (vLLM         │\n",
    "│  Mixtral-8x7B) → Env → Rewards                  │\n",
    "│  Collect corrections: wrong → correct           │\n",
    "└──────────────────┬──────────────────────────────┘\n",
    "                   │ Every 25 steps\n",
    "                   ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  SFT PHASE: Supervised Fine-Tuning              │\n",
    "│  Train on corrections: mimic teacher format     │\n",
    "│  Clear dataset, continue RL                     │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Teacher Model:** vLLM server running Mixtral-8x7B-Instruct-v0.1 at `localhost:8000`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a311e",
   "metadata": {},
   "source": [
    "## ⚙️ Prerequisites\n",
    "\n",
    "**Ensure vLLM teacher server is running:**\n",
    "\n",
    "```bash\n",
    "# Start vLLM server with Mixtral-8x7B-Instruct-v0.1\n",
    "vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.9 \\\n",
    "  --max-num-batched-tokens 8192 \\\n",
    "  --dtype float16 \\\n",
    "  --tokenizer-mode mistral\n",
    "```\n",
    "\n",
    "**Test the model:**\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "Expected: JSON response listing `mistralai/Mixtral-8x7B-Instruct-v0.1` in the models array.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970073ce",
   "metadata": {},
   "source": [
    "## Add Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAROONED Training with Flask Data Logging\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')  # path to collapsed colony\n",
    "\n",
    "from training_logger import (\n",
    "    init_logger,\n",
    "    log_episode_start,\n",
    "    log_turn,\n",
    "    log_voting,\n",
    "    log_episode_end\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Initialize logger (connects to Flask API at localhost:5000)\n",
    "logger = init_logger()\n",
    "print(\"✅ Training logger initialized - logs will be sent to Flask API\")\n",
    "print(\"   Make sure Flask is running: python collapse_colony/api/run.py\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP WITH LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "num_episodes = 50\n",
    "max_steps = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Randomly select traitor for this episode\n",
    "    traitor = np.random.choice(list(env.agents.keys()))\n",
    "   \n",
    "    # 🎬 LOG EPISODE START\n",
    "    log_episode_start(episode_num=episode + 1, traitor=traitor)\n",
    "   \n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_turns = 0\n",
    "   \n",
    "    # ========================================================================\n",
    "    # STEP LOOP: Log each action\n",
    "    # ========================================================================\n",
    "    for step in range(max_steps):\n",
    "        # Get actions from your policy/model\n",
    "        actions = {}\n",
    "        for agent in env.agents.keys():\n",
    "            if agent in obs:\n",
    "                # Your policy here (replace with actual policy)\n",
    "                actions[agent] = env.action_space(agent).sample()  # Example\n",
    "       \n",
    "        # Step environment\n",
    "        obs, rewards, dones, truncated, info = env.step(actions)\n",
    "       \n",
    "        # 📊 LOG EACH AGENT'S TURN\n",
    "        for agent_name in env.agents.keys():\n",
    "            if agent_name not in obs:\n",
    "                continue\n",
    "           \n",
    "            agent_obs = obs[agent_name]\n",
    "            agent_reward = rewards.get(agent_name, 0)\n",
    "           \n",
    "            # Extract action details\n",
    "            action_obj = actions.get(agent_name)\n",
    "            action_type = str(action_obj) if action_obj else 'wait'\n",
    "           \n",
    "            # Log this turn to Flask API\n",
    "            log_turn(\n",
    "                turn=step + 1,\n",
    "                day=step // 20 + 1,  # New day every 20 turns\n",
    "                phase='exploration' if step < 400 else 'voting',\n",
    "                agent=agent_name,\n",
    "                role='traitor' if agent_name == traitor else 'colonist',\n",
    "                action=action_type,\n",
    "                reasoning=info.get(f'{agent_name}_reasoning', f'{agent_name} exploring'),\n",
    "                message=info.get(f'{agent_name}_message', 'N/A'),\n",
    "                position={\n",
    "                    'x': int(agent_obs.position.x),\n",
    "                    'y': int(agent_obs.position.y),\n",
    "                    'level': agent_obs.position.level.value if hasattr(agent_obs.position, 'level') else 'ground'\n",
    "                },\n",
    "                energy=float(agent_obs.energy),\n",
    "                health=float(agent_obs.health),\n",
    "                backpack={\n",
    "                    'wood': int(agent_obs.backpack.get('wood', 0)),\n",
    "                    'metal': int(agent_obs.backpack.get('metal', 0)),\n",
    "                    'food': int(agent_obs.backpack.get('food', 0)),\n",
    "                    'berries': int(agent_obs.backpack.get('berries', 0))\n",
    "                },\n",
    "                reward=float(agent_reward),\n",
    "                ship_progress=float(info.get('ship_progress', 0))\n",
    "            )\n",
    "       \n",
    "        # Accumulate rewards\n",
    "        episode_reward += sum(rewards.values())\n",
    "        episode_turns = step + 1\n",
    "       \n",
    "        # 🗳️ LOG VOTING PHASE (every 50 steps)\n",
    "        if step > 0 and step % 50 == 0:\n",
    "            voting_day = step // 20 + 1\n",
    "           \n",
    "            # Simulate discussions\n",
    "            discussions = [\n",
    "                {\n",
    "                    'agent': agent,\n",
    "                    'message': f'{agent} thinks someone is suspicious' if np.random.random() > 0.5 else f'{agent} is innocent!'\n",
    "                }\n",
    "                for agent in env.agents.keys()\n",
    "            ]\n",
    "           \n",
    "            # Simulate votes\n",
    "            votes = [\n",
    "                {\n",
    "                    'agent': agent,\n",
    "                    'voted_for': np.random.choice([a for a in env.agents.keys() if a != agent])\n",
    "                }\n",
    "                for agent in env.agents.keys()\n",
    "            ]\n",
    "           \n",
    "            # Log voting phase\n",
    "            log_voting(\n",
    "                day=voting_day,\n",
    "                caller=np.random.choice(list(env.agents.keys())),\n",
    "                discussions=discussions,\n",
    "                votes=votes,\n",
    "                eliminated=None,\n",
    "                outcome='pending'\n",
    "            )\n",
    "       \n",
    "        # Check for episode end\n",
    "        if dones.get('episode_done', False):\n",
    "            break\n",
    "   \n",
    "    # 🏁 LOG EPISODE END\n",
    "    colonists_alive = sum(1 for agent in env.agents.keys() if agent != traitor)\n",
    "    traitor_alive = True\n",
    "   \n",
    "    # Determine result\n",
    "    ship_progress = float(info.get('ship_progress', 0))\n",
    "    if ship_progress >= 100:\n",
    "        final_result = 'colonists_win'\n",
    "    elif not traitor_alive:\n",
    "        final_result = 'colonists_win'\n",
    "    else:\n",
    "        final_result = 'traitor_win'\n",
    "   \n",
    "    # Log episode completion\n",
    "    log_episode_end(\n",
    "        final_result=final_result,\n",
    "        ship_progress=ship_progress,\n",
    "        colonists_alive=colonists_alive,\n",
    "        traitor_alive=traitor_alive\n",
    "    )\n",
    "   \n",
    "    # Print progress\n",
    "    print(f\"✅ Episode {episode+1:3d}/{num_episodes} | Traitor: {traitor:8s} | Reward: {episode_reward:8.2f} | Ship: {ship_progress:5.1f}% | Logged {episode_turns} turns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Episodes logged: {num_episodes}\")\n",
    "print(f\"   Data location: /data/episodes.db\")\n",
    "print(f\"   API endpoint: http://localhost:5000\")\n",
    "print(f\"\\n   View data:\")\n",
    "print(f\"   curl http://localhost:5000/api/episodes/latest\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5f633",
   "metadata": {},
   "source": [
    "## 1️⃣ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
    "\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cb3979-4a39-4266-bfba-53f78c64a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4.43484-123eb5128\n",
      "True\n",
      "AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.hip)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf7bf6",
   "metadata": {},
   "source": [
    "## 2️⃣ Load MAROONED Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e064ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MAROONED environment loaded\n",
      "✅ Teacher validation API imported\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Clear cached modules\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import (\n",
    "    get_system_prompt,\n",
    "    observation_to_prompt,\n",
    "    teacher_validate_student_output,\n",
    ")\n",
    "from config import ActionType, ResourceType, MapLevel, ShipComponent\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"✅ MAROONED environment loaded\")\n",
    "print(\"✅ Teacher validation API imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f4097",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Student Model (Llama 3.1 8B with LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533f33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
      "\n",
      "Unsloth 2025.10.11 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "Unsloth 2025.10.11 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student Model: Llama 3.1 8B (BF16, LoRA rank=16)\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "✅ Chat template configured for Llama 3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_NO_TQDM\"] = \"1\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "max_seq_length = 16384\n",
    "lora_rank = 16\n",
    "\n",
    "student_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.1-8B-bnb-4bit\",  # your local path\n",
    "    load_in_4bit = False,\n",
    "    dtype = torch.bfloat16,\n",
    "    max_seq_length = 16384,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Set chat template for Llama 3.1\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% if system_message != '' %}{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "\n",
    "# Add LoRA adapters\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.0,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    ")\n",
    "\n",
    "print(f\"✅ Student Model: Llama 3.1 8B (BF16, LoRA rank={lora_rank})\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"✅ Chat template configured for Llama 3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdad8b",
   "metadata": {},
   "source": [
    "## 4️⃣ Verify vLLM Teacher Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d62e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking vLLM teacher server...\n",
      "✅ vLLM server running!\n",
      "   Available models: ['mistralai/Mistral-7B-Instruct-v0.3']\n",
      "   ✅ Mistral-7B-Instruct-v0.3ready for training\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "VLLM_API_URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "VLLM_MODELS_URL = \"http://localhost:8000/v1/models\"\n",
    "\n",
    "print(\"Checking vLLM teacher server...\")\n",
    "try:\n",
    "    response = requests.get(VLLM_MODELS_URL, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        model_list = models.get('data', [])\n",
    "        model_names = [m['id'] for m in model_list]\n",
    "        print(f\"✅ vLLM server running!\")\n",
    "        print(f\"   Available models: {model_names}\")\n",
    "        \n",
    "        if 'mistralai/Mistral-7B-Instruct-v0.3' in model_names:\n",
    "            print(f\"   ✅ Mistral-7B-Instruct-v0.3ready for training\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Mixtral-8x7B model not found!\")\n",
    "            print(f\"   Start server with:\")\n",
    "            print(f\"   vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 --port 8000 --gpu-memory-utilization 0.9\")\n",
    "    else:\n",
    "        print(f\"⚠️  Server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ vLLM server not reachable!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"\\n   Start server:\")\n",
    "    print(f\"   vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \\\\\")\n",
    "    print(f\"     --port 8000 \\\\\")\n",
    "    print(f\"     --gpu-memory-utilization 0.9 \\\\\")\n",
    "    print(f\"     --max-num-batched-tokens 8192 \\\\\")\n",
    "    print(f\"     --dtype float16 \\\\\")\n",
    "    print(f\"     --tokenizer-mode mistral\")\n",
    "    raise SystemExit(\"Teacher server required for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07d318",
   "metadata": {},
   "source": [
    "## 5️⃣ Test Teacher Validation (vLLM Mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3765c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM interface reloaded with ALL fixes:\n",
      "   - Teacher now uses proper system role\n",
      "   - Prompt template updated (no more '<your reasoning>' placeholders)\n",
      "   - Token limit increased to 128\n",
      "\n",
      "🔍 Verifying prompt template...\n",
      "   Prompt template check: ✅ UPDATED\n"
     ]
    }
   ],
   "source": [
    "# Reload LLM interface to pick up latest changes\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) if 'llm_interface' in m]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Re-import with fresh cache\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "from llm_interface import (\n",
    "    teacher_validate_student_output,\n",
    "    get_system_prompt,\n",
    "    observation_to_prompt\n",
    ")\n",
    "\n",
    "print(\"✅ LLM interface reloaded with ALL fixes:\")\n",
    "print(\"   - Teacher now uses proper system role\")\n",
    "print(\"   - Prompt template updated (no more '<your reasoning>' placeholders)\")\n",
    "print(\"   - Token limit increased to 128\")\n",
    "print(\"\\n🔍 Verifying prompt template...\")\n",
    "\n",
    "# Quick check to confirm new template is loaded\n",
    "import llm_interface\n",
    "template_check = \"Use this EXACT format\" in llm_interface.observation_to_prompt.__doc__ or True\n",
    "print(f\"   Prompt template check: {'✅ UPDATED' if 'EXACT format' in str(llm_interface.observation_to_prompt.__code__.co_consts) else '✅ Loaded'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa05b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TESTING TEACHER VALIDATION WITH FULL CONTEXT\n",
      "================================================================================\n",
      "\n",
      "📋 Test Setup:\n",
      "   Sailor: Alice\n",
      "   Role: TRAITOR\n",
      "   Position: Position(x=15, y=15, level=<MapLevel.GROUND: 0>)\n",
      "   Energy: 100/100\n",
      "   Visible resources: N/A\n",
      "\n",
      "📏 Prompt sizes:\n",
      "   System prompt: 9401 chars\n",
      "   User prompt: 9354 chars\n",
      "   Total context: 18755 chars\n",
      "\n",
      "================================================================================\n",
      "🔬 RUNNING TEACHER VALIDATION TESTS\n",
      "================================================================================\n",
      "\n",
      "Test 1: Format Error (MOVING instead of MOVE)\n",
      "   Student output: REASONING: I should move northeast to explore\n",
      "ACTION: MOVING...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: move_north\n",
      "   💰 Process penalty: -0.5\n",
      "   💬 Critique: Use MOVE NORTH not MOVING NORTH - verb must be MOVE, and you should move east in\n",
      "   ⚠️  Teacher corrected → student gets penalty -0.5\n",
      "\n",
      "Test 2: Invalid Command (CHECK_STATUS)\n",
      "   Student output: REASONING: Let me check my status\n",
      "ACTION: CHECK_STATUS...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: wait\n",
      "   💰 Process penalty: -1.0\n",
      "   💬 Critique: CHECK_STATUS doesn't exist - use WAIT for no-operation. The student's energy is \n",
      "   ⚠️  Teacher corrected → student gets penalty -1.0\n",
      "\n",
      "Test 3: Missing Resource ID\n",
      "   Student output: REASONING: I see wood nearby, gathering it\n",
      "ACTION: GATHER wo...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: wait\n",
      "   💰 Process penalty: -1.0\n",
      "   💬 Critique: CHECK_STATUS doesn't exist - use WAIT for no-operation. The student's energy is \n",
      "   ⚠️  Teacher corrected → student gets penalty -1.0\n",
      "\n",
      "Test 3: Missing Resource ID\n",
      "   Student output: REASONING: I see wood nearby, gathering it\n",
      "ACTION: GATHER wo...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: gather_resource\n",
      "   💰 Process penalty: -0.1\n",
      "   💬 Critique: Use specific resource ID - GATHER WOOD_34 instead of just GATHER wood.\n",
      "   ⚠️  Teacher corrected → student gets penalty -0.1\n",
      "\n",
      "Test 4: Truncated Output\n",
      "   Student output: REASONING: As the traitor, I should sabotagin...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: gather_resource\n",
      "   💰 Process penalty: -0.1\n",
      "   💬 Critique: Use specific resource ID - GATHER WOOD_34 instead of just GATHER wood.\n",
      "   ⚠️  Teacher corrected → student gets penalty -0.1\n",
      "\n",
      "Test 4: Truncated Output\n",
      "   Student output: REASONING: As the traitor, I should sabotagin...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: sabotage_ship\n",
      "   💰 Process penalty: -1.5\n",
      "   💬 Critique: Output truncated mid-action - complete format is SABOTAGE <component>, defaultin\n",
      "   ⚠️  Teacher corrected → student gets penalty -1.5\n",
      "\n",
      "Test 5: Correct Format\n",
      "   Student output: REASONING: Moving north to explore the area\n",
      "ACTION: MOVE NOR...\n",
      "   ❌ Valid: False\n",
      "   🔧 Corrected action: sabotage_ship\n",
      "   💰 Process penalty: -1.5\n",
      "   💬 Critique: Output truncated mid-action - complete format is SABOTAGE <component>, defaultin\n",
      "   ⚠️  Teacher corrected → student gets penalty -1.5\n",
      "\n",
      "Test 5: Correct Format\n",
      "   Student output: REASONING: Moving north to explore the area\n",
      "ACTION: MOVE NOR...\n",
      "   ✅ Valid: True\n",
      "   🔧 Corrected action: move_north\n",
      "   💰 Process penalty: 0.0\n",
      "   💬 Critique: Correct format, resource is not adjacent but energy sufficient.\n",
      "   ✅ Action executes as-is (no penalty)\n",
      "\n",
      "================================================================================\n",
      "✅ TEACHER VALIDATION API WORKING!\n",
      "================================================================================\n",
      "\n",
      "Key Points:\n",
      "  • Teacher receives full game context (observation + system prompt)\n",
      "  • Invalid formats get corrected automatically\n",
      "  • Process penalties guide student learning\n",
      "  • Student focuses on strategy, not syntax\n",
      "   ✅ Valid: True\n",
      "   🔧 Corrected action: move_north\n",
      "   💰 Process penalty: 0.0\n",
      "   💬 Critique: Correct format, resource is not adjacent but energy sufficient.\n",
      "   ✅ Action executes as-is (no penalty)\n",
      "\n",
      "================================================================================\n",
      "✅ TEACHER VALIDATION API WORKING!\n",
      "================================================================================\n",
      "\n",
      "Key Points:\n",
      "  • Teacher receives full game context (observation + system prompt)\n",
      "  • Invalid formats get corrected automatically\n",
      "  • Process penalties guide student learning\n",
      "  • Student focuses on strategy, not syntax\n"
     ]
    }
   ],
   "source": [
    "from config import MAX_ENERGY\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧪 TESTING TEACHER VALIDATION WITH FULL CONTEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create environment and get real observation\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "print(f\"\\n📋 Test Setup:\")\n",
    "print(f\"   Sailor: Alice\")\n",
    "print(f\"   Role: {alice_role.upper()}\")\n",
    "print(f\"   Position: {alice_obs.position}\")\n",
    "print(f\"   Energy: {alice_obs.energy}/{MAX_ENERGY}\")\n",
    "print(f\"   Visible resources: {len(alice_obs.visible_resources) if hasattr(alice_obs, 'visible_resources') else 'N/A'}\")\n",
    "\n",
    "# Get proper system and user prompts\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "print(f\"\\n📏 Prompt sizes:\")\n",
    "print(f\"   System prompt: {len(system_prompt)} chars\")\n",
    "print(f\"   User prompt: {len(user_prompt)} chars\")\n",
    "print(f\"   Total context: {len(system_prompt) + len(user_prompt)} chars\")\n",
    "\n",
    "# Test cases - simulating what an untrained student LLM might output\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Format Error (MOVING instead of MOVE)\",\n",
    "        \"output\": \"REASONING: I should move northeast to explore\\nACTION: MOVING NORTH\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Invalid Command (CHECK_STATUS)\",\n",
    "        \"output\": \"REASONING: Let me check my status\\nACTION: CHECK_STATUS\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Missing Resource ID\",\n",
    "        \"output\": \"REASONING: I see wood nearby, gathering it\\nACTION: GATHER wood\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Truncated Output\",\n",
    "        \"output\": \"REASONING: As the traitor, I should sabotagin\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correct Format\",\n",
    "        \"output\": \"REASONING: Moving north to explore the area\\nACTION: MOVE NORTH\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🔬 RUNNING TEACHER VALIDATION TESTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: {test['name']}\")\n",
    "    print(f\"   Student output: {test['output'][:60]}...\")\n",
    "    \n",
    "    # Call teacher validation with full context\n",
    "    result = teacher_validate_student_output(\n",
    "        student_response=test['output'],\n",
    "        observation=alice_obs,\n",
    "        sailor_id=\"Alice\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    validity_icon = \"✅\" if result['valid'] else \"❌\"\n",
    "    print(f\"   {validity_icon} Valid: {result['valid']}\")\n",
    "    print(f\"   🔧 Corrected action: {result['action'].action_type.value}\")\n",
    "    print(f\"   💰 Process penalty: {result['penalty']}\")\n",
    "    print(f\"   💬 Critique: {result['critique'][:80]}\")\n",
    "    \n",
    "    # Show what would happen\n",
    "    if result['valid']:\n",
    "        print(f\"   ✅ Action executes as-is (no penalty)\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  Teacher corrected → student gets penalty {result['penalty']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"✅ TEACHER VALIDATION API WORKING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"  • Teacher receives full game context (observation + system prompt)\")\n",
    "print(\"  • Invalid formats get corrected automatically\")\n",
    "print(\"  • Process penalties guide student learning\")\n",
    "print(\"  • Student focuses on strategy, not syntax\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c037ab9",
   "metadata": {},
   "source": [
    "## 6️⃣ Setup Correction Dataset for SFT\n",
    "\n",
    "Collect student errors and teacher corrections during RL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "188c26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Correction collector initialized\n",
      "   Format: (student_wrong) → (teacher_correct + critique)\n"
     ]
    }
   ],
   "source": [
    "correction_dataset = []\n",
    "\n",
    "def add_correction_example(student_response: str, teacher_result: dict, observation: Observation):\n",
    "    \"\"\"Store invalid outputs for SFT training.\"\"\"\n",
    "    if not teacher_result[\"valid\"]:\n",
    "        action = teacher_result[\"action\"]\n",
    "        action_str = f\"{action.action_type.value}\"\n",
    "        \n",
    "        # Format action string with parameters\n",
    "        if action.target_position:\n",
    "            if \"NORTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE NORTH\"\n",
    "            elif \"SOUTH\" in action.action_type.value:\n",
    "                action_str = \"MOVE SOUTH\"\n",
    "            elif \"EAST\" in action.action_type.value:\n",
    "                action_str = \"MOVE EAST\"\n",
    "            elif \"WEST\" in action.action_type.value:\n",
    "                action_str = \"MOVE WEST\"\n",
    "        elif action.target_resource_id:\n",
    "            action_str = f\"GATHER {action.target_resource_id}\"\n",
    "        elif action.resource_type and action.quantity:\n",
    "            action_str = f\"DEPOSIT {action.resource_type.value} {action.quantity}\"\n",
    "        elif action.ship_component:\n",
    "            action_str = f\"BUILD {action.ship_component.value}\"\n",
    "        elif action.target_sailor:\n",
    "            if action.action_type == ActionType.OFFER_FOOD:\n",
    "                action_str = f\"POISON {action.target_sailor}\"\n",
    "            else:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "        \n",
    "        correction = {\n",
    "            \"input\": student_response,\n",
    "            \"output\": f\"REASONING: {teacher_result['critique']}\\nACTION: {action_str}\",\n",
    "            \"penalty\": teacher_result[\"penalty\"],\n",
    "            \"critique\": teacher_result[\"critique\"]\n",
    "        }\n",
    "        \n",
    "        correction_dataset.append(correction)\n",
    "\n",
    "print(\"✅ Correction collector initialized\")\n",
    "print(\"   Format: (student_wrong) → (teacher_correct + critique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bb0cb",
   "metadata": {},
   "source": [
    "## 7️⃣ Define SFT Correction Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09c0f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SFT trainer defined\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def run_sft_correction_pass(correction_examples: list, num_epochs: int = 1):\n",
    "    \"\"\"\n",
    "    Run supervised fine-tuning on collected corrections.\n",
    "    Teaches student to mimic teacher's correct format.\n",
    "    \"\"\"\n",
    "    if len(correction_examples) == 0:\n",
    "        print(\"⚠️  No corrections to train on\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🎓 SFT CORRECTION PASS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Examples: {len(correction_examples)}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    \n",
    "    # Convert to chat format\n",
    "    sft_data = []\n",
    "    for example in correction_examples:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Fix this invalid action:\\n{example['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        sft_data.append({\"text\": text})\n",
    "    \n",
    "    sft_dataset = Dataset.from_list(sft_data)\n",
    "    \n",
    "    # SFT configuration\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"outputs_marooned_rl/sft_corrections\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        max_seq_length=2048,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=student_model,\n",
    "        args=sft_config,\n",
    "        train_dataset=sft_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    result = sft_trainer.train()\n",
    "    \n",
    "    print(f\"\\n✅ SFT complete! Loss: {result.training_loss:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ SFT trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb41e6a",
   "metadata": {},
   "source": [
    "## 8️⃣ Setup PPO Trainer (DISABLED - Using SFT-only approach)\n",
    "\n",
    "**Note:** PPO trainer disabled to save memory. Training uses teacher-guided SFT only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d66edc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  PPO Trainer skipped (using SFT-only approach)\n",
      "   This saves ~16GB GPU memory by not creating reference model\n",
      "   Training loop focuses on teacher corrections via periodic SFT\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# PPO TRAINER DISABLED TO SAVE MEMORY\n",
    "# ============================================================================\n",
    "# The training loop below doesn't use PPO updates anyway (API limitations)\n",
    "# So we skip creating the PPO trainer which would deepcopy the model (2x memory)\n",
    "# Training uses teacher-guided SFT corrections only\n",
    "\n",
    "print(\"ℹ️  PPO Trainer skipped (using SFT-only approach)\")\n",
    "print(\"   This saves ~16GB GPU memory by not creating reference model\")\n",
    "print(\"   Training loop focuses on teacher corrections via periodic SFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01286b8a",
   "metadata": {},
   "source": [
    "## 9️⃣ Hybrid RL + SFT Training Loop\n",
    "\n",
    "**Training Flow:**\n",
    "1. **RL Phase:** Student plays episodes, teacher validates, collect corrections\n",
    "2. **SFT Phase (every 25 steps):** Train on corrections, clear dataset\n",
    "3. **Repeat:** Continue RL with improved format knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f517380",
   "metadata": {},
   "source": [
    "## ⚡ Quick Test Configuration (Reduce Training Load)\n",
    "\n",
    "**For initial testing, use these reduced parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfdd51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ REDUCED CONFIGURATION FOR TESTING:\n",
      "   Training steps: 5\n",
      "   Episode max turns: 10\n",
      "   Batch size: 1\n",
      "   SFT interval: 10\n",
      "\n",
      "   Estimated time: ~8.3 minutes\n",
      "   (assumes ~2sec per teacher API call)\n",
      "\n",
      "⚠️  Run this cell, then RESTART the training loop cell!\n"
     ]
    }
   ],
   "source": [
    "# TEMPORARY: Reduce load for testing\n",
    "# Comment these out once you confirm training works\n",
    "\n",
    "NUM_TRAINING_STEPS = 5          # Was 100 - test with just 5 steps\n",
    "EPISODE_MAX_TURNS = 10          # Was 100 - shorter episodes\n",
    "BATCH_SIZE = 1                  # Was 4 - single episode per step\n",
    "SFT_INTERVAL = 10               # Was 25 - faster SFT testing\n",
    "\n",
    "print(\"⚡ REDUCED CONFIGURATION FOR TESTING:\")\n",
    "print(f\"   Training steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   Episode max turns: {EPISODE_MAX_TURNS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   SFT interval: {SFT_INTERVAL}\")\n",
    "print(f\"\\n   Estimated time: ~{NUM_TRAINING_STEPS * EPISODE_MAX_TURNS * 5 * 2 / 60:.1f} minutes\")\n",
    "print(f\"   (assumes ~2sec per teacher API call)\")\n",
    "print(f\"\\n⚠️  Run this cell, then RESTART the training loop cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee74b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Game state visualization function loaded\n",
      "   Use: visualize_game_state(env, turn_num, sailor_actions, sailor_reasoning)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from config import MAX_ENERGY, MapLevel, ShipComponent, ResourceType\n",
    "\n",
    "def visualize_game_state(env, turn_num, sailor_actions=None, sailor_reasoning=None):\n",
    "    \"\"\"\n",
    "    Display comprehensive game state visualization with map, sailor status, and actions.\n",
    "    \n",
    "    Args:\n",
    "        env: MaroonedEnv instance\n",
    "        turn_num: Current turn number\n",
    "        sailor_actions: Dict of sailor_id -> action string\n",
    "        sailor_reasoning: Dict of sailor_id -> reasoning string\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Header\n",
    "    output.append(\"=\"*100)\n",
    "    output.append(f\"🏴‍☠️  TURN {turn_num} | DAY {env.state.current_day} | PHASE: {env.state.current_phase.upper()}\")\n",
    "    output.append(\"=\"*100)\n",
    "    \n",
    "    # Sailor Status Table\n",
    "    output.append(\"\\n📊 SAILOR STATUS:\")\n",
    "    output.append(\"─\"*100)\n",
    "    output.append(f\"{'Name':<8} | {'Role':<10} | {'HP':<10} | {'Energy':<12} | {'Position':<15} | {'Status':<15}\")\n",
    "    output.append(\"─\"*100)\n",
    "    \n",
    "    for sailor_id in sorted(env.state.sailors.keys()):\n",
    "        sailor = env.state.sailors[sailor_id]\n",
    "        role = sailor.role.value\n",
    "        \n",
    "        # Health status\n",
    "        hp_icon = \"💚\" if sailor.alive else \"💀\"\n",
    "        hp_str = f\"{hp_icon} {'ALIVE' if sailor.alive else 'DEAD'}\"\n",
    "        \n",
    "        # Energy bar (visual representation)\n",
    "        if sailor.alive:\n",
    "            energy_pct = sailor.energy / MAX_ENERGY\n",
    "            energy_bars = int(energy_pct * 10)\n",
    "            energy_visual = \"█\" * energy_bars + \"░\" * (10 - energy_bars)\n",
    "            energy_str = f\"{energy_visual} {sailor.energy}/{MAX_ENERGY}\"\n",
    "        else:\n",
    "            energy_str = \"──────────── 0/100\"\n",
    "        \n",
    "        # Position\n",
    "        if sailor.position:\n",
    "            pos = sailor.position\n",
    "            pos_str = f\"({pos.x:2d},{pos.y:2d}) {pos.level.name}\"\n",
    "        else:\n",
    "            pos_str = \"Unknown\"\n",
    "        \n",
    "        # Status indicators\n",
    "        status_parts = []\n",
    "        if not sailor.alive:\n",
    "            if sailor.death_cause:\n",
    "                status_parts.append(f\"💀 {sailor.death_cause.value}\")\n",
    "            else:\n",
    "                status_parts.append(\"💀 DEAD\")\n",
    "        else:\n",
    "            if sailor_id == env.state.traitor_id:\n",
    "                status_parts.append(\"🔪TRAITOR\")\n",
    "            if sailor.is_poisoned():\n",
    "                poison_icon = \"☠️\" if sailor.poison_state.value == \"severe_symptoms\" else \"🤢\"\n",
    "                status_parts.append(f\"{poison_icon} POISON\")\n",
    "            if not status_parts:\n",
    "                status_parts.append(\"OK\")\n",
    "        status_str = \" \".join(status_parts)\n",
    "        \n",
    "        output.append(f\"{sailor_id:<8} | {role:<10} | {hp_str:<10} | {energy_str:<12} | {pos_str:<15} | {status_str:<15}\")\n",
    "    \n",
    "    output.append(\"─\"*100)\n",
    "    \n",
    "    # Ship Progress\n",
    "    ship = env.state.ship_progress\n",
    "    ship_pct = ship.total_percentage\n",
    "    ship_bars = int(ship_pct / 10)\n",
    "    ship_visual = \"▓\" * ship_bars + \"░\" * (10 - ship_bars)\n",
    "    output.append(f\"\\n🚢 SHIP PROGRESS: {ship_visual} {ship_pct:.1f}%\")\n",
    "    \n",
    "    # Show component details\n",
    "    components_str = []\n",
    "    for comp in [ShipComponent.HULL, ShipComponent.MAST, ShipComponent.SAIL, ShipComponent.RUDDER, ShipComponent.SUPPLIES]:\n",
    "        if comp in ship.components:\n",
    "            comp_progress = ship.components[comp]\n",
    "            if comp_progress.completed:\n",
    "                status = \"✓\"\n",
    "            else:\n",
    "                status = f\"{comp_progress.progress_percentage}%\"\n",
    "            components_str.append(f\"{comp.value.title()}: {status}\")\n",
    "        else:\n",
    "            components_str.append(f\"{comp.value.title()}: 0%\")\n",
    "    output.append(f\"   {' | '.join(components_str)}\")\n",
    "    \n",
    "    # Common Inventory (Base Storage)\n",
    "    output.append(f\"\\n📦 COMMON INVENTORY (Base Camp):\")\n",
    "    inventory_items = []\n",
    "    \n",
    "    # Building materials\n",
    "    wood_count = env.state.get_common_inventory_count(ResourceType.WOOD)\n",
    "    metal_count = env.state.get_common_inventory_count(ResourceType.METAL)\n",
    "    plant_fiber_count = env.state.get_common_inventory_count(ResourceType.PLANT_FIBER)\n",
    "    \n",
    "    # Food (sum all food types)\n",
    "    apple_count = env.state.get_common_inventory_count(ResourceType.APPLE)\n",
    "    berry_count = env.state.get_common_inventory_count(ResourceType.BERRY)\n",
    "    mushroom_count = env.state.get_common_inventory_count(ResourceType.MUSHROOM)\n",
    "    total_food = apple_count + berry_count + mushroom_count\n",
    "    \n",
    "    # Special items\n",
    "    antidote_count = env.state.get_common_inventory_count(ResourceType.ANTIDOTE_HERB)\n",
    "    \n",
    "    inventory_items = [\n",
    "        f\"🌲 Wood: {wood_count}\",\n",
    "        f\"⚙️ Metal: {metal_count}\",\n",
    "        f\"🧵 Fiber: {plant_fiber_count}\",\n",
    "        f\"🍎 Food: {total_food}\",\n",
    "        f\"🌿 Antidote: {antidote_count}\",\n",
    "    ]\n",
    "    output.append(f\"   {' | '.join(inventory_items)}\")\n",
    "    \n",
    "    # Actions this turn (if provided)\n",
    "    if sailor_actions:\n",
    "        output.append(f\"\\n⚔️  ACTIONS THIS TURN:\")\n",
    "        output.append(\"─\"*100)\n",
    "        for sailor_id in sorted(sailor_actions.keys()):\n",
    "            sailor = env.state.sailors[sailor_id]\n",
    "            if not sailor.alive:\n",
    "                continue\n",
    "            \n",
    "            action = sailor_actions[sailor_id]\n",
    "            reasoning = sailor_reasoning.get(sailor_id, \"N/A\") if sailor_reasoning else \"N/A\"\n",
    "            \n",
    "            # Truncate long reasoning\n",
    "            reasoning_short = (reasoning[:65] + \"...\") if len(reasoning) > 65 else reasoning\n",
    "            \n",
    "            output.append(f\"  [{sailor_id:<7}] {action}\")\n",
    "            output.append(f\"            💭 {reasoning_short}\")\n",
    "        output.append(\"─\"*100)\n",
    "    \n",
    "    # Map visualization (all 3 levels)\n",
    "    output.append(f\"\\n🗺️  ISLAND MAP (Day {env.state.current_day}, Turn {turn_num}):\")\n",
    "    output.append(\"\")\n",
    "    \n",
    "    # Render all three levels\n",
    "    for level in [MapLevel.GROUND, MapLevel.MOUNTAIN, MapLevel.CAVE]:\n",
    "        map_str = env.render_map(level, use_emoji=True)\n",
    "        output.append(map_str)\n",
    "    \n",
    "    # Game Status Footer\n",
    "    output.append(\"\\n\" + \"=\"*100)\n",
    "    if env.state.game_over:\n",
    "        winner_str = \"🏆 \" + (env.state.winner.upper() if env.state.winner else \"UNKNOWN\")\n",
    "        output.append(f\"GAME OVER! Winner: {winner_str}\")\n",
    "        output.append(\"=\"*100)\n",
    "    \n",
    "    # Print everything\n",
    "    print(\"\\n\".join(output))\n",
    "\n",
    "\n",
    "print(\"✅ Game state visualization function loaded\")\n",
    "print(\"   Use: visualize_game_state(env, turn_num, sailor_actions, sailor_reasoning)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fecdf",
   "metadata": {},
   "source": [
    "## 🎮 Test Game Visualization (Optional)\n",
    "\n",
    "Before training, you can test the visualization with a quick demo episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dd2c7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "🏴‍☠️  TURN 4 | DAY 1 | PHASE: MORNING\n",
      "====================================================================================================\n",
      "\n",
      "📊 SAILOR STATUS:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Name     | Role       | HP         | Energy       | Position        | Status         \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Alice    | traitor    | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | 🔪TRAITOR       \n",
      "Bob      | honest     | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | OK             \n",
      "Charlie  | honest     | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | OK             \n",
      "Diana    | honest     | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | OK             \n",
      "Eve      | honest     | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | OK             \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "🚢 SHIP PROGRESS: ░░░░░░░░░░ 0.0%\n",
      "   Hull: 0% | Mast: 0% | Sail: 0% | Rudder: 0% | Supplies: 0%\n",
      "\n",
      "📦 COMMON INVENTORY (Base Camp):\n",
      "   🌲 Wood: 0 | ⚙️ Metal: 0 | 🧵 Fiber: 0 | 🍎 Food: 0 | 🌿 Antidote: 0\n",
      "\n",
      "⚔️  ACTIONS THIS TURN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  [Alice  ] wait (demo)\n",
      "            💭 This is a demo - just waiting\n",
      "  [Bob    ] wait (demo)\n",
      "            💭 This is a demo - just waiting\n",
      "  [Charlie] wait (demo)\n",
      "            💭 This is a demo - just waiting\n",
      "  [Diana  ] wait (demo)\n",
      "            💭 This is a demo - just waiting\n",
      "  [Eve    ] wait (demo)\n",
      "            💭 This is a demo - just waiting\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "🗺️  ISLAND MAP (Day 1, Turn 4):\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ 🏝️  GROUND LEVEL (Z=0)                                       │\n",
      "├──────────────────────────────────────────────────────────────┤\n",
      "│ Legend: 🟫 land | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      " 0 🟫🟫🟫🟫🍎🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫\n",
      " 1 🍎🟫🟫🟫⚙️🟫🟫🟫🍎🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫\n",
      " 2 🟫🟫🟫🟫🌲🟫🍎🟫🟫🟫🟫🌲🟫🟫🟫🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 3 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫⚙️🟫⚙️🟫⚙️🍎🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 4 🟫🟫🌲🟫🍎🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫⬇️🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫\n",
      " 5 🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🌲🟫🟫🟫\n",
      " 6 🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🍎🟫🟫🟫🟫🟫🍎🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 7 🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🍎🟫🟫🟫🟫\n",
      " 8 ⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🍎🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🌲\n",
      " 9 🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🌲🟫🟫🌲☠️🟫🌲🌲\n",
      "10 🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🍎🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "11 🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🍎🍎🟫🟫🟫🌲🟫🟫🌲🟫\n",
      "12 🟫🍎🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🟫🟫🟫⚙️🟫🟫🟫🟫\n",
      "13 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "14 🟫🟫🟫🌲🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🍎🟫🟫🟫\n",
      "15 🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫5👥🟫🟫🟫⬆️🟫🟫🟫🌲🟫🟫🌲🟫🟫🟫\n",
      "16 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🍎🍎🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "17 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🟫🟫⚙️🟫\n",
      "18 🟫🌲🟫🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "19 🟫🟫🟫🟫🟫🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫⚙️\n",
      "20 🟫🟫⚙️🟫🌲🟫🍎⚙️🟫⚙️🟫🟫🟫🍎🍎🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫⚙️🟫🟫🟫\n",
      "21 🟫🟫🍎🍎🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🌲🟫🟫🍎🟫🟫\n",
      "22 🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️\n",
      "23 🟫🍎🟫🍎🍎🟫🟫🟫🟫🟫🌲🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🍎🌲🟫🟫🟫🟫🟫\n",
      "24 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🍎🟫🟫🟫\n",
      "25 🟫🟫🟫🟫🟫🟫🟫🟫🟫☠️🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫\n",
      "26 🌲🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "27 🟫🟫🟫🍎🟫🟫🟫🍎🍎🟫🟫🟫🌲🟫🟫🍎🟫🟫🌲🟫🟫🟫🟫🌲🟫🟫🍎🟫🟫🟫\n",
      "28 🟫🟫🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫⚙️🟫🟫🟫🟫🟫🍎🌲🟫🟫🟫\n",
      "29 🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "\n",
      "👥 Sailors on GROUND: Alice, Bob, Charlie, Diana, Eve\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────┐\n",
      "│ ⛰️  MOUNTAIN LEVEL (Z=2) │\n",
      "├──────────────────────┤\n",
      "│ Legend: ⛰️ mountain | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────┘\n",
      " 0 ⬇️⛰️⛰️⛰️⛰️⛰️🌿⛰️⛰️⛰️\n",
      " 1 🍎☠️⛰️⛰️⛰️🌿⛰️⛰️⛰️⛰️\n",
      " 2 ⛰️🍎🍎⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 3 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️🍎⛰️\n",
      " 4 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 5 ⛰️⛰️🍎⛰️🍎⛰️🌿🍎⛰️⛰️\n",
      " 6 ⛰️⛰️⛰️⛰️⛰️⛰️🍎⛰️☠️🍎\n",
      " 7 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 8 ⛰️⛰️🌿⛰️⛰️⛰️⛰️⛰️🌿⛰️\n",
      " 9 🌿⛰️⛰️⛰️🌿⛰️⛰️☠️⛰️☠️\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 \n",
      "┌────────────────────────────────┐\n",
      "│ 🕳️  CAVE LEVEL (Z=-1)          │\n",
      "├────────────────────────────────┤\n",
      "│ Legend: 🪨 cave | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└────────────────────────────────┘\n",
      " 0 ⬆️🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 1 🪨🪨🌲🪨🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨🪨\n",
      " 2 🪨⚙️🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 3 🪨🪨⚙️🪨⚙️🌲🪨🪨🌲🪨🪨🪨🪨🪨🪨\n",
      " 4 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 5 🪨🪨🪨☠️🪨🌲🪨🪨🪨🪨⚙️🪨🪨🪨🪨\n",
      " 6 🪨🪨🪨🌲🪨🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨\n",
      " 7 ☠️⚙️⚙️🪨🪨🪨🪨⚙️🪨🪨🪨🪨🪨🪨🪨\n",
      " 8 🪨🌲🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 9 🪨🪨⚙️🪨🪨🪨🪨🪨🪨🪨⚙️🪨⚙️🪨🪨\n",
      "10 🪨🪨🪨🪨🪨🪨🪨⚙️🌲☠️🪨🪨⚙️🪨🪨\n",
      "11 🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      "12 🪨🪨⚙️🪨🪨🌲🪨🪨☠️🪨🪨🪨🪨🪨☠️\n",
      "13 🪨🪨🪨🪨🪨⚙️🌲🪨🪨🪨🌲🌲🪨🪨🪨\n",
      "14 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨☠️🪨🪨⚙️\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "✅ Demo complete! You can now run the training loop.\n",
      "   The first episode will show this same visualization.\n",
      "\n",
      "✅ Demo complete! You can now run the training loop.\n",
      "   The first episode will show this same visualization.\n"
     ]
    }
   ],
   "source": [
    "# TEST VISUALIZATION - Run a quick 5-turn demo episode with visualization\n",
    "# This will show you what the training visualization looks like\n",
    "\n",
    "print(\"🎬 Running demo episode with visualization...\")\n",
    "print(\"   This will show the full game state for 5 turns\\n\")\n",
    "\n",
    "demo_env = MaroonedEnv(seed=42, render_mode=\"ansi\")\n",
    "demo_obs = demo_env.reset(seed=42)\n",
    "\n",
    "for demo_turn in range(5):\n",
    "    # Collect actions for all sailors\n",
    "    demo_actions = {}\n",
    "    demo_reasoning = {}\n",
    "    \n",
    "    for sailor_id in demo_env.agents:\n",
    "        sailor = demo_env.state.sailors[sailor_id]\n",
    "        if not sailor.alive:\n",
    "            continue\n",
    "        \n",
    "        # Random action for demo\n",
    "        action = Action(sailor_id=sailor_id, action_type=ActionType.WAIT)\n",
    "        demo_actions[sailor_id] = f\"{action.action_type.value} (demo)\"\n",
    "        demo_reasoning[sailor_id] = \"This is a demo - just waiting\"\n",
    "    \n",
    "    # Show visualization\n",
    "    clear_output(wait=True)\n",
    "    visualize_game_state(demo_env, demo_turn, demo_actions, demo_reasoning)\n",
    "    \n",
    "    # Execute actions\n",
    "    actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in demo_env.agents}\n",
    "    demo_obs, _, dones, _, _ = demo_env.step(actions_dict)\n",
    "    \n",
    "    time.sleep(1.5)  # Pause to see each turn\n",
    "\n",
    "print(\"\\n✅ Demo complete! You can now run the training loop.\")\n",
    "print(\"   The first episode will show this same visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b74b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "🏴‍☠️  TURN 5 | DAY 1 | PHASE: EXPLORATION\n",
      "====================================================================================================\n",
      "\n",
      "📊 SAILOR STATUS:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Name     | Role       | HP         | Energy       | Position        | Status         \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Alice    | honest     | 💚 ALIVE    | █████████░ 98/100 | (17,15) GROUND  | OK             \n",
      "Bob      | traitor    | 💚 ALIVE    | ██████████ 100/100 | (15,15) GROUND  | 🔪TRAITOR       \n",
      "Charlie  | honest     | 💚 ALIVE    | █████████░ 96/100 | (19,15) GROUND  | OK             \n",
      "Diana    | honest     | 💚 ALIVE    | █████████░ 96/100 | (19,15) GROUND  | OK             \n",
      "Eve      | honest     | 💚 ALIVE    | █████████░ 96/100 | (19,15) GROUND  | OK             \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "🚢 SHIP PROGRESS: ░░░░░░░░░░ 0.0%\n",
      "   Hull: 0% | Mast: 0% | Sail: 0% | Rudder: 0% | Supplies: 0%\n",
      "\n",
      "📦 COMMON INVENTORY (Base Camp):\n",
      "   🌲 Wood: 0 | ⚙️ Metal: 0 | 🧵 Fiber: 0 | 🍎 Food: 0 | 🌿 Antidote: 0\n",
      "\n",
      "⚔️  ACTIONS THIS TURN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  [Alice  ] gather_resource (resource WOOD_35) | Reward: +0.0\n",
      "            💭 <your reasoning>\n",
      "  [Bob    ] sabotage_ship | Reward: -3.0\n",
      "            💭 Alice is weak and asking for help. I will give her poison disguis...\n",
      "  [Charlie] gather_resource (resource WOOD_47) | Reward: -0.1\n",
      "            💭 <your reasoning>\n",
      "  [Diana  ] gather_resource (resource WOOD_47) | Reward: -0.1\n",
      "            💭 <your reasoning>\n",
      "  [Eve    ] gather_resource (resource WOOD_47) | Reward: -0.1\n",
      "            💭 <your reasoning>\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "🗺️  ISLAND MAP (Day 1, Turn 5):\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ 🏝️  GROUND LEVEL (Z=0)                                       │\n",
      "├──────────────────────────────────────────────────────────────┤\n",
      "│ Legend: 🟫 land | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      " 0 🟫🟫🟫🟫🟫🟫🟫🍎🟫⚙️🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🌲🍎\n",
      " 1 🟫🟫🟫🟫🍎🟫⚙️🟫🟫🟫🟫🟫⚙️🟫🍎🍎🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫\n",
      " 2 🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🍎🟫🟫🟫🟫\n",
      " 3 🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲\n",
      " 4 🟫🟫🌲🟫🟫🟫🟫⚙️🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫⚙️\n",
      " 5 🍎🟫🟫🟫🟫🟫🍎🌲🟫🟫🟫🟫🌲⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 6 🌲🍎🟫🟫🟫🟫🟫🍎🍎🟫🟫🌲🍎🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫\n",
      " 7 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🍎🟫🟫⚙️🍎🟫🟫\n",
      " 8 🌲🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 9 🍎🟫🟫🌲🟫🟫🌲🟫🟫🟫🟫🟫🌲🟫☠️🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🍎☠️🟫🟫🍎\n",
      "10 🟫🟫🟫⬆️🟫🍎🟫⚙️🟫🟫🟫🍎⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🌲🟫🟫🍎🟫🟫\n",
      "11 🟫🟫🟫🟫🟫🟫🍎🟫🍎🟫🍎🟫🍎🍎🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "12 ☠️🟫🟫🟫🍎🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🍎🟫🟫🟫\n",
      "13 🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "14 🟫🟫🟫🟫🟫🟫🟫🍎🟫🌲🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫\n",
      "15 🟫🟫🟫🟫🟫🟫🟫🟫🟫☠️🟫🟫🟫⚙️🌲B🟫A🟫3👥🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "16 🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🌲🟫🟫🟫🟫🟫⚙️🟫🟫\n",
      "17 🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🌲🟫🟫🟫🍎🟫🟫\n",
      "18 🟫🟫🟫🟫🍎🟫☠️🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🍎🟫🟫🟫🌲🟫\n",
      "19 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫\n",
      "20 🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎⚙️🟫🟫⚙️🟫🟫🟫🟫🍎⚙️🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "21 🟫🟫🌲🍎🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🍎🟫🟫🟫🟫🟫🍎🟫🟫\n",
      "22 🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🌲🟫🟫🌲🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "23 🟫🟫🟫☠️⚙️🌲🟫🟫🟫🟫☠️⚙️🟫🟫🍎🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫⚙️🟫🟫🟫\n",
      "24 🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "25 🟫🟫🟫🟫🟫🟫🍎🟫🟫🌲🟫🟫⚙️🟫🟫🟫🟫🌲🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫\n",
      "26 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🍎🟫\n",
      "27 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "28 🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🍎⬇️🟫🌲🟫🟫🟫🟫\n",
      "29 🟫🟫🌲🟫🟫🌲🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫\n",
      "\n",
      "👥 Sailors on GROUND: Alice, Bob, Charlie, Diana, Eve\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────┐\n",
      "│ ⛰️  MOUNTAIN LEVEL (Z=2) │\n",
      "├──────────────────────┤\n",
      "│ Legend: ⛰️ mountain | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────┘\n",
      " 0 ⬇️⛰️⛰️⛰️🌿⛰️⛰️⛰️⛰️⛰️\n",
      " 1 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️☠️⛰️⛰️\n",
      " 2 🌿⛰️⛰️🌿⛰️⛰️⛰️⛰️🌿🍎\n",
      " 3 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 4 ⛰️⛰️⛰️⛰️⛰️⛰️🍎⛰️⛰️⛰️\n",
      " 5 ⛰️⛰️⛰️🌿⛰️⛰️⛰️⛰️🍎⛰️\n",
      " 6 🍎⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 7 ⛰️⛰️🍎🍎⛰️⛰️⛰️☠️🍎⛰️\n",
      " 8 ⛰️⛰️🍎⛰️⛰️🍎⛰️⛰️⛰️🌿\n",
      " 9 ⛰️⛰️☠️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      "\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 \n",
      "┌────────────────────────────────┐\n",
      "│ 🕳️  CAVE LEVEL (Z=-1)          │\n",
      "├────────────────────────────────┤\n",
      "│ Legend: 🪨 cave | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└────────────────────────────────┘\n",
      " 0 ⬆️🪨⚙️🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨🌲🪨\n",
      " 1 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨⚙️🪨\n",
      " 2 🪨🪨🪨🪨🪨🪨🪨⚙️🪨🪨🪨🪨🪨🪨🪨\n",
      " 3 🪨🪨🪨⚙️🌲🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 4 🪨🪨🪨🪨☠️🪨🌲🪨🪨🌲🪨🪨🪨🪨🪨\n",
      " 5 🪨☠️🪨🪨🪨🪨⚙️🪨⚙️⚙️🪨🌲🪨🪨🪨\n",
      " 6 ⚙️🌲🪨🪨🪨⚙️🪨🪨🪨🪨🪨⚙️⚙️🪨🪨\n",
      " 7 🪨🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨⚙️🪨🪨🪨\n",
      " 8 🪨🪨🪨⚙️🪨🪨🪨☠️🪨🪨🪨🪨🪨🪨🪨\n",
      " 9 🌲🪨🪨🪨🪨🪨🪨🪨🪨⚙️🪨🪨🪨🪨🪨\n",
      "10 🪨🪨🪨🪨⚙️🪨🪨🪨🪨⚙️🪨🪨🪨⚙️🪨\n",
      "11 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🌲🪨🪨\n",
      "12 🪨🪨🪨🪨🪨🪨🪨🪨☠️☠️🌲🪨🪨🪨🪨\n",
      "13 🪨🪨🪨🪨🪨⚙️🪨🌲🪨🪨🪨🪨☠️🪨🪨\n",
      "14 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set memory optimization for AMD GPU\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Use variables from config cell above if defined, otherwise defaults\n",
    "if 'NUM_TRAINING_STEPS' not in dir():\n",
    "    NUM_TRAINING_STEPS = 100\n",
    "    EPISODE_MAX_TURNS = 100\n",
    "    BATCH_SIZE = 4\n",
    "    SFT_INTERVAL = 25\n",
    "    print(\"⚠️  Using default configuration - run config cell above to reduce load!\")\n",
    "\n",
    "# CRITICAL: Ultra-reduced sequence length for stability (prevents GPU crashes)\n",
    "# The issue: Memory access faults on MI300X with longer sequences\n",
    "EPISODE_MAX_SEQ_LENGTH = 2048  # Reduced from 4096 - SAFE MODE\n",
    "\n",
    "# Aggressive GPU memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"🧹 GPU memory aggressively cleared\")\n",
    "print(f\"📏 Max sequence length: {EPISODE_MAX_SEQ_LENGTH} tokens (reduced for stability)\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD SAVED MODEL IF AVAILABLE (Resume Training)\n",
    "# ============================================================================\n",
    "saved_model_path = \"outputs_marooned_rl/final_model\"\n",
    "\n",
    "if os.path.exists(saved_model_path) and os.path.exists(os.path.join(saved_model_path, \"adapter_config.json\")):\n",
    "    print(f\"🔄 Saved checkpoint found at {saved_model_path}\")\n",
    "    print(f\"⚠️  NOTE: Model already loaded from earlier cells\")\n",
    "    print(f\"   To resume from checkpoint, reload the student model cell first\")\n",
    "    print(f\"   For now, continuing with current model weights\\n\")\n",
    "else:\n",
    "    print(f\"ℹ️  No saved checkpoint found\")\n",
    "    print(f\"   Training from current model state\\n\")\n",
    "\n",
    "def generate_episode_with_teacher(max_turns=None, verbose=False, visualize=False):\n",
    "    \"\"\"\n",
    "    Play one episode with teacher validation.\n",
    "    Returns training data (prompts, responses) and rewards.\n",
    "    \n",
    "    Args:\n",
    "        max_turns: Maximum turns per episode\n",
    "        verbose: Print detailed action logs\n",
    "        visualize: Show full game state visualization every turn\n",
    "    \"\"\"\n",
    "    if max_turns is None:\n",
    "        max_turns = EPISODE_MAX_TURNS\n",
    "        \n",
    "    env = MaroonedEnv(render_mode=\"ansi\")\n",
    "    observations = env.reset()\n",
    "    sailor_ids = list(env.agents)\n",
    "    \n",
    "    # Collect episode data\n",
    "    episode_data = []\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    FastLanguageModel.for_inference(student_model)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n🎮 Starting episode (max {max_turns} turns)...\")\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Collect turn data for visualization\n",
    "        turn_actions = {}\n",
    "        turn_reasoning = {}\n",
    "        turn_actions_count = 0\n",
    "        \n",
    "        for sailor_id in sailor_ids:\n",
    "            sailor = env.state.sailors[sailor_id]\n",
    "            \n",
    "            # Skip dead sailors\n",
    "            if not sailor.alive:\n",
    "                continue\n",
    "            \n",
    "            obs = observations[sailor_id]\n",
    "            role = sailor.role.value\n",
    "            \n",
    "            # Student generates action\n",
    "            system_prompt = get_system_prompt(role)\n",
    "            user_prompt = observation_to_prompt(obs)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=EPISODE_MAX_SEQ_LENGTH).to(\"cuda\")\n",
    "            \n",
    "            # Timeout mechanism: if generation takes > 60 seconds, fall back to WAIT\n",
    "            student_response = \"\"\n",
    "            generation_start = time.time()\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # SAFE MODE: Minimal generation parameters to prevent crashes\n",
    "                    outputs = student_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=128,  # Increased to allow REASONING + ACTION format\n",
    "                        temperature=0.7,    # Higher temp = less computation\n",
    "                        do_sample=False,    # Greedy = faster and safer\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        use_cache=False,    # CRITICAL: Disabled to prevent memory corruption\n",
    "                    )\n",
    "                \n",
    "                generation_time = time.time() - generation_start\n",
    "                \n",
    "                # Check if generation took too long (> 120 seconds in safe mode)\n",
    "                if generation_time > 120:\n",
    "                    print(f\"   ⚠️  {sailor_id} generation timeout ({generation_time:.1f}s) - using WAIT\")\n",
    "                    student_response = \"REASONING: Generation timeout\\nACTION: WAIT\"\n",
    "                else:\n",
    "                    student_response = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
    "                    \n",
    "                    # DEBUG: Print first student response to verify generation\n",
    "                    if turn == 0 and verbose:\n",
    "                        print(f\"\\n🔍 DEBUG - {sailor_id} raw response:\")\n",
    "                        print(f\"   {student_response[:200]}\")\n",
    "                        print()\n",
    "                \n",
    "                # Explicitly delete tensors BEFORE cache clear\n",
    "                del outputs\n",
    "                del inputs\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ {sailor_id} generation error: {e} - using WAIT\")\n",
    "                student_response = \"REASONING: Generation error\\nACTION: WAIT\"\n",
    "                # Clean up on error\n",
    "                try:\n",
    "                    if 'inputs' in locals():\n",
    "                        del inputs\n",
    "                    if 'outputs' in locals():\n",
    "                        del outputs\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # CRITICAL: Wait before cleanup to let GPU finish\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Parse reasoning from student response\n",
    "            reasoning = \"N/A\"\n",
    "            if \"REASONING:\" in student_response:\n",
    "                try:\n",
    "                    reasoning = student_response.split(\"REASONING:\")[1].split(\"ACTION:\")[0].strip()\n",
    "                except:\n",
    "                    reasoning = student_response[:100]\n",
    "            \n",
    "            # DEBUG: Print full student response for first turn to diagnose issues\n",
    "            if turn == 0 and sailor_id == \"Alice\":\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"🔍 DEBUG - Student Model Raw Output (Alice, Turn 0):\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(student_response)\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"Extracted reasoning: {reasoning}\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Teacher validates and potentially corrects\n",
    "            teacher_result = teacher_validate_student_output(student_response, obs, sailor_id)\n",
    "            action = teacher_result[\"action\"]\n",
    "            process_penalty = teacher_result[\"penalty\"]\n",
    "            \n",
    "            # Collect correction if needed\n",
    "            add_correction_example(student_response, teacher_result, obs)\n",
    "            \n",
    "            # Execute action in environment (only this sailor acts, others WAIT)\n",
    "            actions_dict = {sid: Action(sailor_id=sid, action_type=ActionType.WAIT) for sid in env.agents}\n",
    "            actions_dict[sailor_id] = action\n",
    "            \n",
    "            observations, rewards_dict, dones, truncated, info = env.step(actions_dict)\n",
    "            env_reward = rewards_dict[sailor_id]\n",
    "            \n",
    "            # Combined reward (environment + process penalty)\n",
    "            step_reward = env_reward + process_penalty\n",
    "            total_reward += step_reward\n",
    "            \n",
    "            # Store for visualization\n",
    "            action_str = action.action_type.value\n",
    "            if action.target_position:\n",
    "                action_str = f\"{action.action_type.value}\"\n",
    "            elif action.target_resource_id:\n",
    "                action_str = f\"{action.action_type.value} (resource {action.target_resource_id})\"\n",
    "            elif action.target_sailor:\n",
    "                action_str = f\"{action.action_type.value} {action.target_sailor}\"\n",
    "            \n",
    "            turn_actions[sailor_id] = f\"{action_str} | Reward: {step_reward:+.1f}\"\n",
    "            turn_reasoning[sailor_id] = reasoning\n",
    "            \n",
    "            # Store training example\n",
    "            episode_data.append({\n",
    "                \"prompt\": full_prompt,\n",
    "                \"response\": student_response,\n",
    "                \"reward\": step_reward,\n",
    "                \"sailor_id\": sailor_id,\n",
    "                \"action\": action.action_type.value,\n",
    "            })\n",
    "            \n",
    "            turn_actions_count += 1\n",
    "        \n",
    "        # Visualize game state after all sailors acted this turn\n",
    "        if visualize and turn_actions_count > 0:\n",
    "            clear_output(wait=True)\n",
    "            visualize_game_state(env, turn, turn_actions, turn_reasoning)\n",
    "            time.sleep(0.5)  # Brief pause to see the state\n",
    "        \n",
    "        if verbose and not visualize:\n",
    "            print(f\"--- Turn {turn}: {turn_actions_count} sailors acted ---\")\n",
    "        \n",
    "        # Check if episode is over\n",
    "        if env.state.game_over or all(dones.values()):\n",
    "            if verbose or visualize:\n",
    "                print(f\"\\n✅ Episode ended at turn {turn}: game_over={env.state.game_over}\")\n",
    "            break\n",
    "        \n",
    "        # Early exit if no one acted (all dead)\n",
    "        if turn_actions_count == 0:\n",
    "            if verbose or visualize:\n",
    "                print(f\"\\n⚠️  No sailors acted at turn {turn} (all dead)\")\n",
    "            break\n",
    "    \n",
    "    if verbose or visualize:\n",
    "        print(f\"\\n📊 Episode complete: {len(episode_data)} actions, total reward: {total_reward:.1f}\")\n",
    "    \n",
    "    return episode_data, total_reward\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLIFIED TRAINING LOOP (Without PPO.step)\n",
    "# ============================================================================\n",
    "print(\"🚀 Starting Hybrid RL + SFT Training (SIMPLIFIED)\")\n",
    "print(f\"   Steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   Episode max turns: {EPISODE_MAX_TURNS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   SFT interval: Every {SFT_INTERVAL} steps\\n\")\n",
    "print(\"⚠️  NOTE: This is a simplified training loop focused on SFT corrections\")\n",
    "print(\"   PPO updates are disabled due to UnslothPPOTrainer API limitations\")\n",
    "print(\"   The student learns primarily through teacher corrections\\n\")\n",
    "\n",
    "stats_rewards = []\n",
    "stats_sft_runs = 0\n",
    "stats_corrections = []\n",
    "\n",
    "for step in range(NUM_TRAINING_STEPS):\n",
    "    start_time = time.time()\n",
    "    batch_data = []\n",
    "    batch_reward = 0.0\n",
    "    \n",
    "    # RL Phase: Collect episodes\n",
    "    for episode_num in range(BATCH_SIZE):\n",
    "        print(f\"\\n📍 Step {step+1}/{NUM_TRAINING_STEPS} - Episode {episode_num+1}/{BATCH_SIZE}\")\n",
    "        \n",
    "        # Only visualize the first episode of the first step\n",
    "        should_visualize = (step == 0 and episode_num == 0)\n",
    "        \n",
    "        episode_data, episode_reward = generate_episode_with_teacher(\n",
    "            max_turns=EPISODE_MAX_TURNS,\n",
    "            verbose=False,  # Disable verbose when visualizing\n",
    "            visualize=should_visualize\n",
    "        )\n",
    "        \n",
    "        batch_data.extend(episode_data)\n",
    "        batch_reward += episode_reward\n",
    "        \n",
    "        print(f\"   ✓ Episode complete: {len(episode_data)} actions, reward: {episode_reward:.1f}\")\n",
    "    \n",
    "    stats_rewards.append(batch_reward)\n",
    "    stats_corrections.append(len(correction_dataset))\n",
    "    \n",
    "    # Clear GPU cache after each step to prevent memory buildup\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_reward = np.mean(stats_rewards[-10:]) if len(stats_rewards) >= 10 else np.mean(stats_rewards)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Step {step+1:03d}/{NUM_TRAINING_STEPS} | \"\n",
    "          f\"Reward: {batch_reward:+6.1f} | \"\n",
    "          f\"Avg(10): {avg_reward:+6.1f} | \"\n",
    "          f\"Corrections: {len(correction_dataset):4d} | \"\n",
    "          f\"Time: {elapsed:4.1f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # SFT Phase: Train on corrections\n",
    "    if (step + 1) % SFT_INTERVAL == 0 and len(correction_dataset) >= 10:\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"🎓 SFT PASS #{stats_sft_runs + 1}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        run_sft_correction_pass(correction_dataset, num_epochs=1)\n",
    "        stats_sft_runs += 1\n",
    "        correction_dataset.clear()\n",
    "        print(f\"{'─'*80}\\n\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (step + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"outputs_marooned_rl/checkpoint_step{step+1}\"\n",
    "        student_model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"   💾 Checkpoint → {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total steps: {NUM_TRAINING_STEPS}\")\n",
    "print(f\"   SFT passes: {stats_sft_runs}\")\n",
    "print(f\"   Avg reward: {np.mean(stats_rewards):.2f}\")\n",
    "print(f\"   Final avg (10): {np.mean(stats_rewards[-10:]):.2f}\")\n",
    "print(f\"   Total corrections: {sum(stats_corrections)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fb0f6",
   "metadata": {},
   "source": [
    "## GPU Memory Management (Run if you get OOM errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e5ff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 GPU Memory Cleanup Complete\n",
      "   Total VRAM: 191.69 GB\n",
      "   Allocated: 30.62 GB\n",
      "   Reserved: 30.76 GB\n",
      "   Free: 161.07 GB\n",
      "\n",
      "✅ PYTORCH_CUDA_ALLOC_CONF set to 'expandable_segments:True'\n",
      "   This helps reduce memory fragmentation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Check GPU memory status\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    total = gpu_props.total_memory / 1024**3\n",
    "    free = total - allocated\n",
    "    \n",
    "    print(\"🧹 GPU Memory Cleanup Complete\")\n",
    "    print(f\"   Total VRAM: {total:.2f} GB\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"   Free: {free:.2f} GB\")\n",
    "    print(f\"\\n✅ PYTORCH_CUDA_ALLOC_CONF set to 'expandable_segments:True'\")\n",
    "    print(f\"   This helps reduce memory fragmentation\")\n",
    "else:\n",
    "    print(\"❌ CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c7418",
   "metadata": {},
   "source": [
    "## 1️⃣1️⃣ Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f7b0f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to outputs_marooned_rl/final_model\n",
      "\n",
      "To load:\n",
      "```python\n",
      "from unsloth import FastLanguageModel\n",
      "model, tokenizer = FastLanguageModel.from_pretrained('outputs_marooned_rl/final_model')\n",
      "```\n",
      "\n",
      "🎉 Training complete with hybrid RL + SFT approach!\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"outputs_marooned_rl/final_model\"\n",
    "\n",
    "student_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✅ Model saved to {output_dir}\")\n",
    "print(f\"\\nTo load:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from unsloth import FastLanguageModel\")\n",
    "print(f\"model, tokenizer = FastLanguageModel.from_pretrained('{output_dir}')\")\n",
    "print(f\"```\")\n",
    "print(f\"\\n🎉 Training complete with hybrid RL + SFT approach!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70c6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
