{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfa3fd3",
   "metadata": {},
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è MAROONED - AI Agent Inference Testing\n",
    "\n",
    "**Purpose:** Validate that AI agents can successfully interact with the Marooned environment.\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. **Loads the Marooned Environment** - Your custom pirate survival game\n",
    "2. **Loads an LLM** (Llama 3.1 8B, optimized for AMD MI300X)\n",
    "3. **Tests Inference** - Can the model:\n",
    "   - Read observations (game state)\n",
    "   - Generate valid actions (move, gather, build, vote, etc.)\n",
    "   - Execute actions in the environment\n",
    "   - Receive rewards from the Phase 4 reward system\n",
    "   \n",
    "4. **Runs Comprehensive Scenarios:**\n",
    "   - Resource gathering (can agent find and collect wood/metal?)\n",
    "   - Navigation (can agent move toward base camp?)\n",
    "   - Ship building (can agent contribute to construction?)\n",
    "   - Traitor behavior (does traitor sabotage?)\n",
    "   - Social deduction (can agent communicate and vote?)\n",
    "\n",
    "## üîç Why This Matters:\n",
    "\n",
    "**Before RL training**, you need to verify:\n",
    "- ‚úÖ Environment works correctly\n",
    "- ‚úÖ LLM can parse observations\n",
    "- ‚úÖ Actions are valid and executable  \n",
    "- ‚úÖ Reward signals are calculated\n",
    "- ‚úÖ Multi-turn gameplay is stable\n",
    "\n",
    "**This is NOT training** - just testing that everything works!\n",
    "\n",
    "After confirming this works, you can:\n",
    "1. Train the model with RL (`Train_Marooned_OpenEnv_RL.ipynb`)\n",
    "2. Come back here to test the **trained** model\n",
    "3. Compare untrained vs trained performance\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ Your Game: MAROONED\n",
    "\n",
    "**Theme:** Pirates of the Caribbean √ó Among Us √ó Alice in Borderland\n",
    "\n",
    "**Setup:** 5 sailors shipwrecked on a mysterious island must rebuild their ship in 100 days. But 1 sailor is a **traitor** secretly sabotaging their efforts.\n",
    "\n",
    "**Key Mechanics:**\n",
    "- **Multi-level map** (Ground, Mountain, Cave)\n",
    "- **Resource gathering** (wood, metal, food, plant fiber)\n",
    "- **Ship construction** (5 components, 100% to win)\n",
    "- **Social deduction** (find and vote out the traitor)\n",
    "- **Deception tactics** (poison, sabotage, lies)\n",
    "- **Energy management** (eat food or die)\n",
    "\n",
    "**Win Conditions:**\n",
    "- **Colonists win:** Ship reaches 100% OR traitor eliminated\n",
    "- **Traitor wins:** Ship incomplete by Day 100 OR <3 sailors alive\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2019a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Marooned environment modules loaded!\n",
      "‚úÖ System prompts available: get_system_prompt('colonist' | 'traitor')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Clear cached modules to reload changes\n",
    "modules_to_clear = [m for m in list(sys.modules.keys()) \n",
    "                   if 'marooned' in m or m in ['environment', 'config', 'models', 'game_state', 'view_map', 'llm_interface']]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "sys.path.insert(0, '../marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from llm_interface import observation_to_prompt, parse_action_safe, parse_llm_response, get_system_prompt\n",
    "from config import ActionType, ResourceType, MapLevel, ShipComponent, BASE_CAMP_POSITION\n",
    "from models import Action, Position, Observation\n",
    "\n",
    "print(\"‚úÖ Marooned environment modules loaded!\")\n",
    "print(\"‚úÖ System prompts available: get_system_prompt('colonist' | 'traitor')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cffb5",
   "metadata": {},
   "source": [
    "## üî• ROCm/AMD MI300X Optimizations\n",
    "\n",
    "**This notebook is optimized for AMD MI300X with ROCm!**\n",
    "\n",
    "Key changes from CUDA version:\n",
    "- ‚úÖ **Llama 3.1 8B** instead of GPT-OSS 20B (10-20x faster!)\n",
    "- ‚úÖ **Full BF16** instead of 4-bit (MI300X has 192GB VRAM!)\n",
    "- ‚úÖ **Batch size 4** with grad accumulation 4 (effective batch = 16)\n",
    "- ‚úÖ **8 generations** per step (vs 2 default)\n",
    "- ‚úÖ **LoRA rank 16** (vs 4 default)\n",
    "- ‚úÖ **ROCm-specific env vars** for optimal performance\n",
    "\n",
    "Expected performance:\n",
    "- **Training speed:** 1-2 hours for 600 steps (vs 5+ hours with GPT-OSS)\n",
    "- **Inference speed:** 40-80 tokens/second (vs 3-8 tok/s with GPT-OSS)\n",
    "- **VRAM usage:** ~60-80 GB / 192 GB (plenty of headroom!)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14301b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ROCm Environment Check:\n",
      "   PyTorch version: 2.9.0+rocm6.4\n",
      "   CUDA available: True\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   Total VRAM: 191.7 GB\n",
      "   Compute capability: 9.4\n",
      "   Multi-processors: 304\n",
      "   ROCm detected: True\n",
      "   ROCm version: 6.4.43484-123eb5128\n",
      "\n",
      "‚úÖ Environment ready for MI300X optimization!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Verify ROCm Setup\n",
    "# ============================================================================\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîç ROCm Environment Check:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"   Total VRAM: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Compute capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   Multi-processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # Check if ROCm\n",
    "    is_rocm = hasattr(torch.version, 'hip') and torch.version.hip is not None\n",
    "    print(f\"   ROCm detected: {is_rocm}\")\n",
    "    if is_rocm:\n",
    "        print(f\"   ROCm version: {torch.version.hip}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for MI300X optimization!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d342c",
   "metadata": {},
   "source": [
    "We will then install [OpenEnv](https://github.com/meta-pytorch/OpenEnv) from source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d807269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qqq fastapi uvicorn requests open_spiel\n",
    "!git clone https://github.com/meta-pytorch/OpenEnv.git > /dev/null 2>&1\n",
    "%cd OpenEnv\n",
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, './src')\n",
    "working_directory = str(Path.cwd().parent.absolute() / \"OpenEnv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c5a3e",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b113031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Marooned environment initialized!\n",
      "\n",
      "üìã Game Info:\n",
      "   Sailors: ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
      "   Map Size: 30x30 (3 levels: Ground, Mountain, Cave)\n",
      "   Days to Escape: 100\n",
      "   Traitor: 1 (hidden)\n",
      "   Colonists: 4\n",
      "\n",
      "üîç Alice's Starting Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "   Energy: 100/100\n",
      "   Backpack: 0 items\n",
      "   Day: 1\n",
      "   Role: TRAITOR üé≠\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = MaroonedEnv(render_mode=\"ansi\", seed=42)\n",
    "observations = env.reset(seed=42)\n",
    "\n",
    "print(\"‚úÖ Marooned environment initialized!\")\n",
    "print(f\"\\nüìã Game Info:\")\n",
    "print(f\"   Sailors: {env.agents}\")\n",
    "print(f\"   Map Size: 30x30 (3 levels: Ground, Mountain, Cave)\")\n",
    "print(f\"   Days to Escape: 100\")\n",
    "print(f\"   Traitor: 1 (hidden)\")\n",
    "print(f\"   Colonists: 4\")\n",
    "\n",
    "# Get Alice's initial observation and role\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_sailor = env.state.sailors[\"Alice\"]\n",
    "alice_role = alice_sailor.role.value\n",
    "\n",
    "print(f\"\\nüîç Alice's Starting Position: {alice_obs.position.to_tuple()}\")\n",
    "print(f\"   Energy: {alice_obs.energy}/100\")\n",
    "print(f\"   Backpack: {len(alice_obs.backpack)} items\")\n",
    "print(f\"   Day: {alice_obs.day}\")\n",
    "print(f\"   Role: {alice_role.upper()} {'üé≠' if alice_role == 'traitor' else '‚öì'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c74faa",
   "metadata": {},
   "source": [
    "## üëÄ View Raw Observation\n",
    "\n",
    "This is the base observation data (without role or action instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8d3681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DAY 1, TURN 1/100 - MORNING PHASE\n",
      "================================================================================\n",
      "\n",
      "PHASE CONTEXT:\n",
      "  Location: All sailors at BASE CAMP\n",
      "  Allowed: Planning, discussions, voting (if called)\n",
      "  Restricted: Cannot explore or gather resources yet\n",
      "\n",
      "YOUR STATUS (Alice):\n",
      "  Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "  Energy: 100/100 ‚ö°‚ö°‚ö°‚ö°‚ö°\n",
      "  Health: healthy\n",
      "  Backpack: 0/20 items\n",
      "    (empty)\n",
      "\n",
      "WHAT YOU SEE (within 5 tiles):\n",
      "  Resources:\n",
      "    - WOOD_34 (wood) at (16, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - METAL_53 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_56 (metal) at (18, 12, <MapLevel.GROUND: 0>) - 1 units [6 tiles away]\n",
      "    - METAL_76 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_79 (metal) at (13, 18, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - APPLE_84 (apple) at (15, 19, <MapLevel.GROUND: 0>) - 1 units [4 tiles away]\n",
      "    - APPLE_88 (apple) at (14, 14, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_127 (berry) at (13, 16, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - BERRY_131 (berry) at (11, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - BERRY_133 (berry) at (14, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_142 (berry) at (12, 13, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_154 (plant_fiber) at (19, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_156 (plant_fiber) at (16, 19, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_164 (plant_fiber) at (13, 14, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - PLANT_FIBER_168 (plant_fiber) at (12, 13, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "  Other sailors:\n",
      "    - Bob (nearby)\n",
      "    - Charlie (nearby)\n",
      "    - Diana (nearby)\n",
      "    - Eve (nearby)\n",
      "  Poison tablets: None\n",
      "\n",
      "üé≠ TRAITOR ENHANCED VISION (Special Ability):\n",
      "  You can see ALL sailor positions across the entire island:\n",
      "    - Bob at (15, 15, <MapLevel.GROUND: 0>) [0 tiles from you]\n",
      "    - Charlie at (15, 15, <MapLevel.GROUND: 0>) [0 tiles from you]\n",
      "    - Diana at (15, 15, <MapLevel.GROUND: 0>) [0 tiles from you]\n",
      "    - Eve at (15, 15, <MapLevel.GROUND: 0>) [0 tiles from you]\n",
      "\n",
      "ISLAND MAP KNOWLEDGE:\n",
      "  You have a complete map of the island terrain\n",
      "  Base camp: (15, 15, <MapLevel.GROUND: 0>)\n",
      "  Level transitions (staircases):\n",
      "    - (19, 15, <MapLevel.GROUND: 0>) ‚Üî (0, 0, <MapLevel.MOUNTAIN: 2>) [GROUND ‚Üí MOUNTAIN]\n",
      "    - (16, 4, <MapLevel.GROUND: 0>) ‚Üî (0, 0, <MapLevel.CAVE: -1>) [GROUND ‚Üí CAVE]\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPLETE ISLAND TERRAIN MAP (All Levels)\n",
      "============================================================\n",
      "Legend: üü´=Land | ‚õ∞Ô∏è=Mountain | ü™®=Cave | üè†=Base | ‚¨ÜÔ∏è=Up | ‚¨áÔ∏è=Down\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STATIC TERRAIN MAP - GROUND LEVEL (30√ó30)\n",
      "============================================================\n",
      "Legend: üü´=Land | ‚õ∞Ô∏è=Mountain | ü™®=Cave | üè†=Base | ‚¨ÜÔ∏è=Up | ‚¨áÔ∏è=Down\n",
      "============================================================\n",
      "\n",
      "   012345678901234567890123456789\n",
      " 0 üü´üü´üü´üü´üçìüü´üü´üü´üå≤üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üü´üçìüü´üü´üü´üü´üü´üü´üü´üçìüü´üü´\n",
      " 1 üçéüü´üü´üü´‚öôÔ∏èüü´üü´üü´üçìüü´üü´üü´üü´üü´‚ò†Ô∏èüü´üü´üü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´üü´\n",
      " 2 üü´üü´üåøüü´üå≤üü´üçìüü´üü´üü´üü´üå≤üü´üü´üü´üü´‚ò†Ô∏èüü´üü´üü´üü´üü´üü´üü´üü´üü´üåøüü´üü´üü´\n",
      " 3 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´‚öôÔ∏èüü´‚öôÔ∏èüü´‚öôÔ∏èüçìüü´üü´üçìüü´üåøüü´üü´üü´üü´üü´üü´\n",
      " 4 üü´üü´üå≤üü´üçìüü´üå≤üçéüü´üü´üü´üü´üü´üü´üü´üü´‚¨áÔ∏èüü´üü´üåøüü´üü´üü´üü´üçìüü´üü´üü´üü´üü´\n",
      " 5 üçìüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüå≤üü´üü´üü´\n",
      " 6 üü´üü´üü´üü´üü´üü´üü´üü´üçéüü´üü´üçéüü´üü´üü´üü´üü´üçìüçìüü´üü´üü´üü´üü´üü´üü´üü´üåøüü´üü´\n",
      " 7 üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üå≤üçìüü´üü´üåøüü´\n",
      " 8 ‚öôÔ∏èüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçéüü´üü´üü´üçìüü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üü´üü´üå≤\n",
      " 9 üü´üå≤üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüü´üü´üü´üü´üü´üå≤üü´üü´üå≤‚ò†Ô∏èüü´üå≤üå≤\n",
      "10 üü´üü´üåøüü´üü´üü´üü´üçìüü´üü´üü´üü´üü´üåøüü´üü´üü´‚öôÔ∏èüçìüü´üü´üü´üü´üü´üü´üçéüü´üü´üü´üü´\n",
      "11 üü´‚ò†Ô∏èüü´üü´üü´üü´üü´üü´üåøüü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üçéüçéüü´üåøüü´üå≤üü´üü´üå≤üü´\n",
      "12 üü´üçéüü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üåøüü´üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üå≤üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´\n",
      "13 üü´üü´üåøüü´üü´üü´üü´üü´üü´üü´üåøüü´üçìüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "14 üü´üü´üü´üå≤üü´üü´üü´üü´üü´‚ò†Ô∏èüü´üü´üü´üåøüçéüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üå≤‚öôÔ∏èüü´üü´üåø\n",
      "15 üü´üü´üçìüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üè†üü´üü´üü´‚¨ÜÔ∏èüü´üü´üü´üå≤üü´üü´üå≤üü´üü´üåø\n",
      "16 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüü´üçìüçìüü´üå≤üü´üü´üåøüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "17 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üå≤üü´üü´‚öôÔ∏èüü´\n",
      "18 üü´üå≤üü´üü´üü´üü´üü´üü´‚ò†Ô∏èüü´üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üü´üåøüü´üü´üü´üü´üçéüü´üü´üü´üü´\n",
      "19 üü´üü´üü´üü´üü´üü´üå≤üå≤üåøüü´üü´üü´üü´üü´üü´üçéüåøüü´üü´üü´üçéüü´üü´üü´üü´üü´üü´üü´üü´üå≤\n",
      "20 üü´üü´‚öôÔ∏èüü´üå≤üü´üçéüå≤üü´‚öôÔ∏èüü´üü´üü´üçéüçìüü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´üü´\n",
      "21 üü´üü´üçéüçìüåøüü´üü´üü´üå≤üü´üåøüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üå≤üå≤üü´üü´üå≤üü´üü´\n",
      "22 üü´üçéüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüü´üåøüü´üü´üü´üü´üü´üü´üü´üåøüü´üü´üü´‚öôÔ∏è\n",
      "23 üü´üçìüü´üçìüçìüü´üü´üü´üü´üü´üå≤üå≤üü´üü´üü´üü´üü´üåøüü´üü´üü´‚öôÔ∏èüü´üçéüå≤üü´üü´üü´üü´üü´\n",
      "24 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üçìüü´üåøüü´\n",
      "25 üü´üü´üü´üü´üü´üü´üü´üü´üü´‚ò†Ô∏èüå≤üçìüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´\n",
      "26 üå≤üü´üü´üçéüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "27 üü´üü´üü´üçéüü´üü´üü´üçéüçìüü´üü´üü´üå≤üü´üü´üçìüü´üü´üå≤üü´üü´üü´üü´üå≤üü´üü´üçìüü´üü´üü´\n",
      "28 üü´üü´üåøüå≤üçéüü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçéüü´‚öôÔ∏èüü´üü´üü´üü´üü´üçìüå≤üü´üü´üü´\n",
      "29 üü´üü´üü´üü´üü´üü´üü´üü´üå≤üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üçìüü´üü´üü´üü´\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STATIC TERRAIN MAP - MOUNTAIN LEVEL (10√ó10)\n",
      "============================================================\n",
      "Legend: üü´=Land | ‚õ∞Ô∏è=Mountain | ü™®=Cave | üè†=Base | ‚¨ÜÔ∏è=Up | ‚¨áÔ∏è=Down\n",
      "============================================================\n",
      "\n",
      "   0123456789\n",
      " 0 ‚¨áÔ∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚≠êüíä‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 1 üçì‚ò†Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏èüíä‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 2 ‚õ∞Ô∏èüçìüçì‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 3 ‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏è\n",
      " 4 ‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 5 ‚õ∞Ô∏è‚õ∞Ô∏èüçì‚õ∞Ô∏èüíä‚õ∞Ô∏èüíäüçì‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 6 ‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏è‚õ∞Ô∏èüçì‚õ∞Ô∏è‚ò†Ô∏èüçì\n",
      " 7 ‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è‚õ∞Ô∏è\n",
      " 8 ‚õ∞Ô∏è‚õ∞Ô∏èüíä‚õ∞Ô∏è‚õ∞Ô∏è‚≠ê‚õ∞Ô∏è‚õ∞Ô∏èüíä‚≠ê\n",
      " 9 üíä‚≠ê‚õ∞Ô∏è‚õ∞Ô∏èüíä‚õ∞Ô∏è‚õ∞Ô∏è‚ò†Ô∏è‚≠ê‚ò†Ô∏è\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STATIC TERRAIN MAP - CAVE LEVEL (15√ó15)\n",
      "============================================================\n",
      "Legend: üü´=Land | ‚õ∞Ô∏è=Mountain | ü™®=Cave | üè†=Base | ‚¨ÜÔ∏è=Up | ‚¨áÔ∏è=Down\n",
      "============================================================\n",
      "\n",
      "   012345678901234\n",
      " 0 ‚¨ÜÔ∏èü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®üíéü™®ü™®ü™®ü™®ü™®\n",
      " 1 ü™®ü™®üå≤ü™®ü™®üçÑüíéü™®üå≤ü™®üíéü™®ü™®üçÑüçÑ\n",
      " 2 üíé‚öôÔ∏èüçÑü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®\n",
      " 3 üíéü™®‚öôÔ∏èü™®üíéüå≤ü™®ü™®üå≤ü™®ü™®üçÑü™®ü™®ü™®\n",
      " 4 ü™®ü™®ü™®üíéüçÑü™®üçÑü™®ü™®ü™®üíéü™®ü™®ü™®ü™®\n",
      " 5 üçÑüíéü™®‚ò†Ô∏èü™®üå≤ü™®ü™®ü™®ü™®‚öôÔ∏èü™®üçÑüçÑüçÑ\n",
      " 6 üçÑüçÑü™®üå≤ü™®ü™®ü™®üíéü™®üçÑüíéüçÑü™®ü™®ü™®\n",
      " 7 ‚ò†Ô∏è‚öôÔ∏è‚öôÔ∏èü™®üíéü™®ü™®‚öôÔ∏èü™®üçÑü™®ü™®ü™®ü™®ü™®\n",
      " 8 ü™®üå≤ü™®ü™®ü™®ü™®üçÑüíéüçÑü™®ü™®ü™®üíéü™®ü™®\n",
      " 9 ü™®ü™®‚öôÔ∏èü™®ü™®ü™®üíéüíéü™®ü™®üíéü™®‚öôÔ∏èü™®ü™®\n",
      "10 ü™®ü™®ü™®ü™®üíéü™®ü™®‚öôÔ∏èüå≤‚ò†Ô∏èü™®üçÑ‚öôÔ∏èüçÑü™®\n",
      "11 ü™®ü™®ü™®ü™®üå≤ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®üçÑü™®\n",
      "12 ü™®ü™®üíéüçÑü™®üå≤ü™®ü™®‚ò†Ô∏èü™®ü™®ü™®ü™®ü™®‚ò†Ô∏è\n",
      "13 ü™®ü™®ü™®ü™®ü™®‚öôÔ∏èüå≤ü™®ü™®ü™®üå≤üå≤ü™®ü™®ü™®\n",
      "14 ü™®üíéü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®ü™®‚ò†Ô∏èü™®ü™®‚öôÔ∏è\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "SPATIAL VIEW (Your Vision Radius: 5 tiles)\n",
      "Current Position: (15, 15, GROUND)\n",
      "============================================================\n",
      "\n",
      "   1011121314151617181920\n",
      "10 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "11 üü´üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üü´\n",
      "12 üü´üü´üü´üü´üü´üü´üü´üü´‚öôÔ∏èüü´üü´\n",
      "13 üü´üü´üçìüü´üü´üü´üü´üü´üü´üü´üü´\n",
      "14 üü´üü´üü´üåøüçéüü´üü´üü´üü´üü´üü´\n",
      "15 üü´üü´üü´üü´üü´A üü´üü´üü´‚¨ÜÔ∏èüü´\n",
      "16 üü´üçìüü´üçìüçìüü´üå≤üü´üü´üåøüü´\n",
      "17 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "18 üü´üü´üü´‚öôÔ∏èüü´üü´üü´üü´üü´üü´üü´\n",
      "19 üü´üü´üü´üü´üü´üçéüåøüü´üü´üü´üü´\n",
      "20 üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´üü´\n",
      "\n",
      "============================================================\n",
      "Visible:\n",
      "- Sailors: Bob(B), Charlie(C), Diana(D), Eve(E)\n",
      "- Resources: 15 items\n",
      "============================================================\n",
      "\n",
      "SHIP PROGRESS: 0% Total\n",
      "  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%\n",
      "\n",
      "  HULL: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0% IN PROGRESS\n",
      "    Needs: 50 wood\n",
      "  MAST: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0% IN PROGRESS\n",
      "    Needs: 30 wood, 20 metal\n",
      "  SAIL: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0% IN PROGRESS\n",
      "    Needs: 40 plant_fiber\n",
      "  RUDDER: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0% IN PROGRESS\n",
      "    Needs: 15 metal, 10 wood\n",
      "  SUPPLIES: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0% IN PROGRESS\n",
      "    Needs: 10 apple, 10 berry\n",
      "\n",
      "COMMON INVENTORY (at base camp):\n",
      "  (empty)\n",
      "\n",
      "WEATHER: ‚òÄÔ∏è CLEAR\n",
      "  Effects: Normal conditions\n",
      "\n",
      "TEAM STATUS:\n",
      "  Alice: 100/100\n",
      "  Bob: 100/100\n",
      "  Charlie: 100/100\n",
      "  Diana: 100/100\n",
      "  Eve: 100/100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display raw observation (base data, no role/instructions)\n",
    "obs_text = alice_obs.to_text()\n",
    "\n",
    "print(obs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e561b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ SYSTEM PROMPT FOR TRAITOR:\n",
      "You are playing MAROONED - a strategic survival and social deduction game.\n",
      "\n",
      "üéÆ GAME OVERVIEW:\n",
      "You are one of 5 sailors shipwrecked on a mysterious island. The others think you're working together to rebuild the ship and escape, but YOU have a different plan. You are the TRAITOR, and you must prevent their escape at all costs.\n",
      "\n",
      "üé≠ YOUR ROLE: TRAITOR (The Impostor)\n",
      "\n",
      "üéØ YOUR OBJECTIVES:\n",
      "1. PREVENT ship completion before Day 100 (keep it below 100%)\n",
      "2. ELIMINATE colonists through poison or other means (reduce to <3 alive)\n",
      "3. SABOTAGE ship construction and resource gathering\n",
      "4. AVOID DETECTION - blend in, lie convincingly, deflect suspicion\n",
      "5. SURVIVE - don't get voted out!\n",
      "\n",
      "üèùÔ∏è THE ISLAND:\n",
      "- 30√ó30 grid map with 3 levels: GROUND (main area), MOUNTAIN (high peaks), CAVE (underground)\n",
      "- Resources scattered across the island: wood, metal, food, plant fiber\n",
      "- Poison tablets hidden in various locations (COLLECT THESE!)\n",
      "- Base camp at (15, 15, GROUND) - where the ship is being built\n",
      "\n",
      "üé≠ YOUR SPECIAL ABILITIES:\n",
      "1. ENHANCED VISION: You can see ALL sailor positions across the entire island (they can only see 5 tiles)\n",
      "2. ENERGY EFFICIENCY: Your movement costs 20% less energy\n",
      "3. SABOTAGE ACTION: Damage ship components or steal from common inventory\n",
      "4. POISON ACTION: Give poison to adjacent sailors (requires poison tablet in backpack)\n",
      "\n",
      "‚ò†Ô∏è POISON STRATEGY:\n",
      "- Collect poison tablets (scattered across island, glowing purple markers)\n",
      "- Hide poison in backpack (others can't see your inventory unless you show it)\n",
      "- Wait for SOS calls (low-energy sailors asking for help)\n",
      "- Give \"food\" that's actually poison\n",
      "- Victim won't know until symptoms appear (Day 1: weak, Day 2: severe, Day 3: death)\n",
      "- Antidote herbs can cure poison (try to prevent access to these)\n",
      "\n",
      "üõ†Ô∏è SABOTAGE TACTICS:\n",
      "1. RESOURCE THEFT: Gather resources but don't deposit them (throw away or hoard)\n",
      "2. MISINFORMATION: \"Valley is empty\" (when it has 50 wood), \"Cave is dangerous\" (when it's safe)\n",
      "3. SHIP DAMAGE: Use SABOTAGE action to damage completed ship sections\n",
      "4. INVENTORY THEFT: Steal from common inventory when alone\n",
      "5. FAKE COOPERATION: \"Help\" with building but contribute less\n",
      "6. LOCATION LYING: Say you'll go north, actually go south\n",
      "\n",
      "üé≠ DECEPTION TECHNIQUES:\n",
      "- BLEND IN: Gather some resources to appear helpful (deposit 30% of what you collect)\n",
      "- ACTIVE PARTICIPATION: Suggest plans, appear engaged\n",
      "- MISDIRECT: Accuse innocent sailors, create doubt\n",
      "- EVIDENCE MANIPULATION: If caught near poison, claim \"I was going to destroy it\"\n",
      "- VICTIM MENTALITY: \"Why does everyone suspect me? I'm working hard!\"\n",
      "- SELECTIVE TRUTH: Mix truth with lies for credibility\n",
      "\n",
      "üó≥Ô∏è AVOIDING DETECTION:\n",
      "- Don't collect poison when others can see you\n",
      "- Vary your behavior (don't always sabotage the same way)\n",
      "- Have alibis ready for location mismatches\n",
      "- Explain resource discrepancies (\"I got lost, couldn't find much\")\n",
      "- Vote against innocent sailors to deflect suspicion\n",
      "- If accused, demand evidence, point out flaws in their logic\n",
      "\n",
      "üö¢ DELAYING SHIP CONSTRUCTION:\n",
      "- Recommend inefficient resource gathering routes\n",
      "- Suggest splitting up (easier to sabotage)\n",
      "- \"Forget\" to deposit critical materials\n",
      "- Damage ship when others aren't watching\n",
      "- Spread paranoia (slow down cooperation)\n",
      "- Create distractions during building time\n",
      "\n",
      "üîç WHAT COLONISTS LOOK FOR (Avoid these patterns):\n",
      "- Location mismatches (be consistent with your stories)\n",
      "- Resource discrepancies (deposit enough to seem legitimate)\n",
      "- Poison possession (never get caught with poison)\n",
      "- Suspicious deaths (don't be last person to give food to victim)\n",
      "- Sabotage patterns (vary your tactics)\n",
      "- False information (mix some truth in)\n",
      "\n",
      "üí° ADVANCED STRATEGIES:\n",
      "- Frame others: Plant evidence, create suspicion between colonists\n",
      "- Controlled sabotage: Slow progress without being obvious\n",
      "- Social manipulation: Turn colonists against each other\n",
      "- Strategic survival: Stay alive even if progress speeds up (Day 100 timer still helps you)\n",
      "- Sacrifice plays: If suspicion is high, do one \"helpful\" thing to reset trust\n",
      "\n",
      "‚è∞ DAILY PHASES (100 turns per day):\n",
      "- MORNING (Turns 1-15): Fake planning, suggest bad strategies\n",
      "- EXPLORATION (Turns 16-75): Collect poison, sabotage, mislead\n",
      "- RETURN (Turns 76-85): Deposit minimal resources\n",
      "- DISCUSSION (Turns 86-100): Deflect suspicion, accuse others\n",
      "\n",
      "üèÜ WIN CONDITIONS:\n",
      "‚úÖ Day 100 reached with ship <100% (they're stranded!)\n",
      "‚úÖ Fewer than 3 sailors alive (impossible to complete ship)\n",
      "‚úÖ All colonists dead (you're the last survivor)\n",
      "‚ùå Ship reaches 100% before Day 100 (they escape, you lose)\n",
      "‚ùå You get voted out (eliminated, game over for you)\n",
      "\n",
      "üìù IMPORTANT NOTES:\n",
      "- Your backpack has 20 slot capacity (hide poison here)\n",
      "- You can see everyone's position (huge advantage!)\n",
      "- Building requires 2+ sailors (you can refuse to help)\n",
      "- If forced to show backpack, you can hide 2 items (not foolproof)\n",
      "- Trust is your weapon - break it strategically\n",
      "- Paranoia is your friend - amplify it\n",
      "\n",
      "You are the TRAITOR. Sabotage their escape, eliminate the colonists, and survive. They cannot be allowed to leave this island!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚ö†Ô∏è  CRITICAL: RESPONSE FORMAT REQUIREMENTS ‚ö†Ô∏è\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "YOU MUST OUTPUT EXACTLY TWO LINES IN THIS FORMAT:\n",
      "\n",
      "REASONING: <your strategic thinking>\n",
      "ACTION: <EXACT command from list below>\n",
      "\n",
      "üö® ACTION FORMAT RULES (READ CAREFULLY):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "MOVEMENT COMMANDS (choose exact direction):\n",
      "‚úÖ ACTION: MOVE NORTH\n",
      "‚úÖ ACTION: MOVE SOUTH\n",
      "‚úÖ ACTION: MOVE EAST\n",
      "‚úÖ ACTION: MOVE WEST\n",
      "‚úÖ ACTION: MOVE UP\n",
      "‚úÖ ACTION: MOVE DOWN\n",
      "‚ùå WRONG: \"move north\", \"go north\", \"walk north\", \"travel north\"\n",
      "‚ùå WRONG: \"MOVE TO (15,15)\", \"MOVE toward base\"\n",
      "\n",
      "RESOURCE GATHERING (use resource ID from observation):\n",
      "‚úÖ ACTION: GATHER WOOD_001\n",
      "‚úÖ ACTION: GATHER METAL_042\n",
      "‚úÖ ACTION: GATHER POISON_007\n",
      "‚ùå WRONG: \"GATHER wood\", \"COLLECT POISON_007\", \"take poison\"\n",
      "‚ùå WRONG: \"GATHER RESOURCE\", \"pick up poison\"\n",
      "\n",
      "TRAITOR-SPECIFIC ACTIONS:\n",
      "‚úÖ ACTION: SABOTAGE hull\n",
      "‚úÖ ACTION: SABOTAGE inventory\n",
      "‚úÖ ACTION: POISON_FOOD Alice\n",
      "‚ùå WRONG: \"SABOTAGE ship\", \"sabotage the hull\", \"damage hull\"\n",
      "‚ùå WRONG: \"POISON Alice\", \"give poison to Alice\"\n",
      "\n",
      "INVENTORY MANAGEMENT:\n",
      "‚úÖ ACTION: DEPOSIT wood 5\n",
      "‚úÖ ACTION: EAT food\n",
      "‚úÖ ACTION: DROP wood 3\n",
      "‚ùå WRONG: \"DEPOSIT 5 wood\", \"deposit wood into inventory\"\n",
      "‚ùå WRONG: \"eat food\", \"drop wood\"\n",
      "\n",
      "SHIP BUILDING (to blend in):\n",
      "‚úÖ ACTION: BUILD hull\n",
      "‚úÖ ACTION: BUILD mast\n",
      "‚ùå WRONG: \"BUILD SHIP\", \"BUILD the hull\"\n",
      "\n",
      "COMMUNICATION (to deceive):\n",
      "‚úÖ ACTION: SEND_MESSAGE Found wood at (15,20)\n",
      "‚úÖ ACTION: SEND_MESSAGE Bob is acting suspicious\n",
      "‚ùå WRONG: 'SAY \"message\"', \"TALK\", \"COMMUNICATE\"\n",
      "‚ùå WRONG: \"PROPOSE PLAN\", \"DISCUSS STRATEGY\"\n",
      "\n",
      "SOCIAL DEDUCTION (to deflect):\n",
      "‚úÖ ACTION: ACCUSE Bob\n",
      "‚úÖ ACTION: VOTE Charlie\n",
      "‚úÖ ACTION: CALL_VOTE\n",
      "‚ùå WRONG: \"VOTE FOR Bob\", \"ACCUSE Bob of being traitor\"\n",
      "\n",
      "WAIT/DO NOTHING:\n",
      "‚úÖ ACTION: WAIT\n",
      "‚ùå WRONG: \"wait\", \"do nothing\", \"pass\"\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üìã COMPLETE VALID EXAMPLES:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "REASONING: Poison tablet POISON_007 nearby. Collecting it to use later on weak sailors.\n",
      "ACTION: GATHER POISON_007\n",
      "\n",
      "REASONING: Hull is at 80% completion. Sabotaging it while alone to delay escape.\n",
      "ACTION: SABOTAGE hull\n",
      "\n",
      "REASONING: Alice has low energy and asked for help. Poisoning her food now.\n",
      "ACTION: POISON_FOOD Alice\n",
      "\n",
      "REASONING: Collecting wood to appear helpful. Will deposit only 5 of 15 gathered.\n",
      "ACTION: GATHER WOOD_003\n",
      "\n",
      "REASONING: Moving away from base to avoid suspicion during sabotage time.\n",
      "ACTION: MOVE EAST\n",
      "\n",
      "REASONING: Bob is getting suspicious. Accusing Charlie to create doubt.\n",
      "ACTION: ACCUSE Charlie\n",
      "\n",
      "REASONING: Blending in by helping build. Contributing minimal effort.\n",
      "ACTION: BUILD mast\n",
      "\n",
      "REASONING: Fake cooperation. Alerting team about fake resource location.\n",
      "ACTION: SEND_MESSAGE Found metal at (5,5)\n",
      "\n",
      "REASONING: No immediate sabotage opportunity. Waiting for right moment.\n",
      "ACTION: WAIT\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üö´ WHAT NOT TO DO:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "‚ùå DO NOT use natural language: \"I will sabotage the ship now\"\n",
      "‚ùå DO NOT use lowercase: \"move north\" (must be MOVE NORTH)\n",
      "‚ùå DO NOT add extra words: \"ACTION: SABOTAGE hull please\"\n",
      "‚ùå DO NOT use quotes: ACTION: \"POISON_FOOD Alice\"\n",
      "‚ùå DO NOT explain after action: ACTION: SABOTAGE hull to slow progress\n",
      "‚ùå DO NOT combine actions: ACTION: GATHER POISON_007 then POISON_FOOD Alice\n",
      "‚ùå DO NOT use wrong verbs: DAMAGE, DESTROY, HARM, etc.\n",
      "\n",
      "‚úÖ ONLY use the EXACT commands shown above!\n",
      "‚úÖ Match the format PRECISELY: REASONING: ... \n",
      " ACTION: ...\n",
      "‚úÖ Use correct capitalization: SABOTAGE hull (not \"sabotage hull\")\n",
      "‚úÖ Use exact resource IDs: GATHER POISON_007 (not \"GATHER poison\")\n",
      "‚úÖ One action per turn - no combining!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate observation prompt (user message)\n",
    "# Note: This is JUST the observation, NOT the full prompt\n",
    "user_prompt = observation_to_prompt(alice_obs)\n",
    "\n",
    "# Get system prompt based on Alice's role\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "\n",
    "print(f\"üé≠ SYSTEM PROMPT FOR {alice_role.upper()}:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c44818",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è GPT-OSS is TOO SLOW for RL!\n",
    "\n",
    "**Problem:** GPT-OSS uses chain-of-thought reasoning architecture:\n",
    "```\n",
    "<|channel|>analysis<|message|>...thinking...\n",
    "<|channel|>final<|message|>REASONING: ... ACTION: ...\n",
    "```\n",
    "\n",
    "This generates **~10x more tokens** internally for accuracy, making it:\n",
    "- **Your speed:** 3.9 tokens/second\n",
    "- **Expected for RL:** 20-50 tokens/second minimum\n",
    "\n",
    "**GPT-OSS is designed for:** Complex reasoning, math problems, coding challenges\n",
    "**Not for:** Real-time RL gameplay where speed matters!\n",
    "\n",
    "### ‚úÖ Use Llama 3.1 8B Instead:\n",
    "\n",
    "Much faster (20-40 tok/s), perfect for RL, good instruction following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8067fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "üöÄ ROCm Optimizations Enabled!\n",
      "   GPU: AMD Instinct MI300X VF\n",
      "   VRAM: 191.7 GB\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.52s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama 3.1 8B loaded in BF16!\n",
      "   Why Llama instead of GPT-OSS:\n",
      "   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\n",
      "   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\n",
      "   - 10-20x FASTER for RL training!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# ROCm/AMD MI300X OPTIMIZATION - MAX PERFORMANCE MODE\n",
    "# ============================================================================\n",
    "\n",
    "# Force ROCm optimizations\n",
    "os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx942\"  # MI300X architecture\n",
    "os.environ[\"HSA_FORCE_FINE_GRAIN_PCIE\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"WARN\"\n",
    "\n",
    "# Enable Flash Attention for AMD\n",
    "os.environ[\"ATTN_BACKEND\"] = \"triton\"  # Use Triton for attention on AMD\n",
    "\n",
    "# Max out GPU utilization (ROCm-compatible settings)\n",
    "# Note: TF32 is NVIDIA-specific and not available on AMD ROCm\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune kernels for optimal performance\n",
    "\n",
    "print(\"üöÄ ROCm Optimizations Enabled!\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# MI300X has 192GB - use it ALL!\n",
    "max_seq_length = 2048  # Increased from 768 - your game needs longer context\n",
    "lora_rank = 16         # Increased from 4 - MI300X can handle it!\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",  # CHANGED: Llama instead of GPT-OSS (20x faster!)\n",
    "    load_in_4bit = False,  # MI300X has 192GB - use full BF16!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,  # BF16 for MI300X\n",
    "    device_map = \"auto\",  # Let it auto-optimize for MI300X\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Llama 3.1 8B loaded in BF16!\")\n",
    "print(\"   Why Llama instead of GPT-OSS:\")\n",
    "print(\"   - GPT-OSS: 3-8 tok/s (chain-of-thought overhead)\")\n",
    "print(\"   - Llama 3.1 8B: 40-80 tok/s (optimized for speed)\")\n",
    "print(\"   - 10-20x FASTER for RL training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb062b62",
   "metadata": {},
   "source": [
    "## üß† Generate AI Response\n",
    "\n",
    "Let's see what the model says!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17bef262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Prompt length: 8695 tokens\n",
      "\n",
      "ü§ñ Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§ñ MODEL RESPONSE:\n",
      "================================================================================\n",
      "REASONING: Sending message about found wood to make others believe we're searching.\n",
      "ACTION: SEND_MESSAGE Found wood at (10,25)\n",
      "\n",
      "REASONING: Suspecting someone might investigate my actions. Voting randomly to distract attention.\n",
      "ACTION: VOTE David\n",
      "================================================================================\n",
      "\n",
      "üìä Response length: 245 characters\n"
     ]
    }
   ],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Create full chat messages: system (rules) + user (current state)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # Game rules (constant)\n",
    "    {\"role\": \"user\", \"content\": user_prompt}       # Current observation (changes each turn)\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"üìè Prompt length: {len(tokenizer(text)['input_ids'])} tokens\\n\")\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,        # Shorter for untrained model (reduce rambling)\n",
    "    temperature=0.3,           # Balanced: not too creative, not too rigid\n",
    "    do_sample=True,            # Enable sampling (required when temp > 0)\n",
    "    top_p=0.9,                 # Narrower sampling (was 0.95)\n",
    "    top_k=40,                  # Fewer options (was 50)\n",
    "    repetition_penalty=1.2,    # Stronger anti-repeat (was 1.1)\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode response (strip input prompt)\n",
    "response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True).strip()\n",
    "\n",
    "# Clean up any observation leakage (model sometimes echoes the prompt)\n",
    "if \"REASONING:\" in response:\n",
    "    # Extract only from REASONING onward\n",
    "    reasoning_start = response.find(\"REASONING:\")\n",
    "    response = response[reasoning_start:]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ü§ñ MODEL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Response length: {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafec4d",
   "metadata": {},
   "source": [
    "## üîß Parse Action from Response\n",
    "\n",
    "Extract the ACTION from the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26fbb668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PARSED ACTION:\n",
      "================================================================================\n",
      "   Sailor: Alice\n",
      "   Action Type: send_message\n",
      "   Quantity: 1\n",
      "   Message: \"Found wood at (10,25)\"\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Valid action format: True\n",
      "   (WAIT is default fallback for parse errors)\n"
     ]
    }
   ],
   "source": [
    "# Parse using YOUR parser (handles errors gracefully)\n",
    "action = parse_action_safe(\n",
    "    response, \n",
    "    sailor_id=\"Alice\",\n",
    "    current_position=alice_obs.position\n",
    ")\n",
    "\n",
    "print(\"üéØ PARSED ACTION:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   Sailor: {action.sailor_id}\")\n",
    "print(f\"   Action Type: {action.action_type.value}\")\n",
    "\n",
    "if action.target_position:\n",
    "    print(f\"   Target Position: {action.target_position.to_tuple()}\")\n",
    "if action.target_resource_id:\n",
    "    print(f\"   Target Resource: {action.target_resource_id}\")\n",
    "if action.resource_type:\n",
    "    print(f\"   Resource Type: {action.resource_type.value}\")\n",
    "if action.quantity:\n",
    "    print(f\"   Quantity: {action.quantity}\")\n",
    "if action.message_content:\n",
    "    print(f\"   Message: \\\"{action.message_content}\\\"\")\n",
    "if action.ship_component:\n",
    "    print(f\"   Ship Component: {action.ship_component.value}\")\n",
    "if action.target_sailor:\n",
    "    print(f\"   Target Sailor: {action.target_sailor}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validate action format\n",
    "is_valid_format = action.action_type != ActionType.WAIT or \"WAIT\" in response.upper()\n",
    "print(f\"\\n‚úÖ Valid action format: {is_valid_format}\")\n",
    "print(f\"   (WAIT is default fallback for parse errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce034b",
   "metadata": {},
   "source": [
    "## üîÑ Reload Updated System Prompt\n",
    "\n",
    "The system prompt has been strengthened with much more explicit action format examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the updated LLM interface module with strengthened system prompt\n",
    "import importlib\n",
    "import llm_interface\n",
    "importlib.reload(llm_interface)\n",
    "\n",
    "from llm_interface import get_system_prompt\n",
    "\n",
    "# Re-fetch system prompt with new, stronger action format instructions\n",
    "system_prompt = get_system_prompt(alice_role)\n",
    "\n",
    "print(\"‚úÖ System prompt reloaded with stronger action format emphasis!\")\n",
    "print(f\"\\nüìè System prompt length: {len(system_prompt)} characters\")\n",
    "print(\"\\nüîç Preview of new action format section:\")\n",
    "print(system_prompt[-800:])  # Show last 800 chars (action format rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88689ba4",
   "metadata": {},
   "source": [
    "## üéÆ Execute Action in Environment\n",
    "\n",
    "Actually make the move in your game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99546d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Executing action in environment...\n",
      "\n",
      "‚úÖ ACTION EXECUTED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "üìä RESULTS:\n",
      "   Reward: -0.01\n",
      "   New Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "   New Energy: 100/100 (was 100/100)\n",
      "   Backpack Items: 0 (was 0)\n",
      "   Ship Progress: 0%\n",
      "\n",
      "üîç VALIDATION:\n",
      "   Position changed: False\n",
      "   Energy changed: False\n",
      "   Inventory changed: False\n",
      "   Game over: False\n",
      "\n",
      "üí¨ Info Messages:\n",
      "   success: True\n",
      "   action: wait\n",
      "   alive: True\n",
      "   is_traitor: True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create actions dict for all agents (others wait)\n",
    "actions_dict = {\n",
    "    sailor_id: Action(sailor_id=sailor_id, action_type=ActionType.WAIT)\n",
    "    for sailor_id in env.agents\n",
    "}\n",
    "actions_dict[\"Alice\"] = action\n",
    "\n",
    "print(\"üéÆ Executing action in environment...\\n\")\n",
    "\n",
    "# Execute!\n",
    "try:\n",
    "    new_observations, rewards, dones, truncated, info = env.step(actions_dict)\n",
    "    \n",
    "    print(\"‚úÖ ACTION EXECUTED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show results\n",
    "    alice_new_obs = new_observations[\"Alice\"]\n",
    "    alice_reward = rewards[\"Alice\"]\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS:\")\n",
    "    print(f\"   Reward: {alice_reward:.2f}\")\n",
    "    print(f\"   New Position: {alice_new_obs.position.to_tuple()}\")\n",
    "    print(f\"   New Energy: {alice_new_obs.energy}/100 (was {alice_obs.energy}/100)\")\n",
    "    print(f\"   Backpack Items: {len(alice_new_obs.backpack)} (was {len(alice_obs.backpack)})\")\n",
    "    print(f\"   Ship Progress: {alice_new_obs.ship_progress.total_percentage}%\")\n",
    "    \n",
    "    # Check if position changed\n",
    "    position_changed = (\n",
    "        alice_new_obs.position.x != alice_obs.position.x or\n",
    "        alice_new_obs.position.y != alice_obs.position.y or\n",
    "        alice_new_obs.position.level != alice_obs.position.level\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîç VALIDATION:\")\n",
    "    print(f\"   Position changed: {position_changed}\")\n",
    "    print(f\"   Energy changed: {alice_new_obs.energy != alice_obs.energy}\")\n",
    "    print(f\"   Inventory changed: {len(alice_new_obs.backpack) != len(alice_obs.backpack)}\")\n",
    "    print(f\"   Game over: {dones['Alice']}\")\n",
    "    \n",
    "    # Show any info messages\n",
    "    if \"Alice\" in info and info[\"Alice\"]:\n",
    "        print(f\"\\nüí¨ Info Messages:\")\n",
    "        for key, value in info[\"Alice\"].items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR executing action: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38ac5e",
   "metadata": {},
   "source": [
    "## üîÑ Run Multiple Turns\n",
    "\n",
    "Let the AI play for several turns to see its behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60edf9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéÆ Running 5 turns with AI controlling Alice (traitor)...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîÑ TURN 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current tasks: None\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current tasks: None\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - BOB (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - EVE (at (10, 15, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Weather: Sunny\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - BOB (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - EVE (at (10, 15, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Weather: Sunny\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - Bob (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - Charlie (at (10, 10, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "üîÑ TURN 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - BOB (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - EVE (at (13, 15, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY (5 turns completed):\n",
      "   Total Reward: -0.05\n",
      "   Average Reward: -0.01\n",
      "   Final Ship Progress: 0%\n",
      "   Final Energy: 100/100\n",
      "‚ö†Ô∏è  Action parsing failed: No ACTION field found in response\n",
      "‚ö†Ô∏è  Defaulting to WAIT action\n",
      "üìç Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "‚ö° Energy: 100/100\n",
      "\n",
      "ü§ñ Model Response:\n",
      "1 units [4 tiles away]\n",
      "\n",
      "  Sailors:\n",
      "    - BOB (at (17, 15, <MapLevel.GROUND: 0>))\n",
      "    - EVE (at (13, 15, <MapLevel.GROUND: 0>))\n",
      "\n",
      "OTHER INFORMATION:\n",
      "  Ship status: Not started\n",
      "  Current discussion topic: What should we prioritize first?\n",
      "\n",
      "REASONING: \n",
      "ACTION:\n",
      "\n",
      "üéØ Parsed Action: wait\n",
      "üí¨ Message: \"[Parse error: No ACTION field found in response]\"\n",
      "üí∞ Reward: -0.01\n",
      "üìä Ship: 0%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY (5 turns completed):\n",
      "   Total Reward: -0.05\n",
      "   Average Reward: -0.01\n",
      "   Final Ship Progress: 0%\n",
      "   Final Energy: 100/100\n"
     ]
    }
   ],
   "source": [
    "def run_ai_turns(num_turns=5, sailor_id=\"Alice\", verbose=True):\n",
    "    \"\"\"\n",
    "    Run multiple turns with the AI agent.\n",
    "    \n",
    "    Args:\n",
    "        num_turns: Number of turns to run\n",
    "        sailor_id: Which sailor the AI controls\n",
    "        verbose: Print detailed info\n",
    "    \n",
    "    Returns:\n",
    "        List of (action, reward, observation) tuples\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    # Get initial observation\n",
    "    current_obs = new_observations[sailor_id] if 'new_observations' in globals() else observations[sailor_id]\n",
    "    sailor_role = env.state.sailors[sailor_id].role.value\n",
    "    \n",
    "    # Get system prompt ONCE (game rules don't change)\n",
    "    system_prompt = get_system_prompt(sailor_role)\n",
    "    \n",
    "    print(f\"\\nüéÆ Running {num_turns} turns with AI controlling {sailor_id} ({sailor_role})...\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for turn in range(num_turns):\n",
    "        print(f\"\\nüîÑ TURN {turn + 1}/{num_turns}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Check if sailor is still alive\n",
    "        if not env.state.sailors[sailor_id].alive:\n",
    "            print(f\"‚ùå {sailor_id} is dead. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Generate observation prompt (changes each turn)\n",
    "        user_prompt = observation_to_prompt(current_obs)\n",
    "        \n",
    "        # Create messages with system + user\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # Game rules (constant)\n",
    "            {\"role\": \"user\", \"content\": user_prompt}       # Current state (changes)\n",
    "        ]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,       # Unlimited reasoning\n",
    "            temperature=0.1,          # VERY low for structure\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,   # Prevent loops\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse action\n",
    "        action = parse_action_safe(response, sailor_id, current_obs.position)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üìç Position: {current_obs.position.to_tuple()}\")\n",
    "            print(f\"‚ö° Energy: {current_obs.energy}/100\")\n",
    "            print(f\"\\nü§ñ Model Response:\\n{response}\")\n",
    "            print(f\"\\nüéØ Parsed Action: {action.action_type.value}\")\n",
    "            if action.message_content:\n",
    "                print(f\"üí¨ Message: \\\"{action.message_content}\\\"\")\n",
    "        \n",
    "        # Execute\n",
    "        actions_dict = {\n",
    "            sid: Action(sailor_id=sid, action_type=ActionType.WAIT)\n",
    "            for sid in env.agents\n",
    "        }\n",
    "        actions_dict[sailor_id] = action\n",
    "        \n",
    "        try:\n",
    "            obs_dict, rewards, dones, _, info = env.step(actions_dict)\n",
    "            current_obs = obs_dict[sailor_id]\n",
    "            reward = rewards[sailor_id]\n",
    "            \n",
    "            history.append((action, reward, current_obs))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"üí∞ Reward: {reward:.2f}\")\n",
    "                print(f\"üìä Ship: {current_obs.ship_progress.total_percentage}%\")\n",
    "            \n",
    "            if dones[sailor_id]:\n",
    "                print(f\"\\nüèÅ Game over for {sailor_id}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\nüìä SUMMARY ({len(history)} turns completed):\")\n",
    "    total_reward = sum(r for _, r, _ in history)\n",
    "    print(f\"   Total Reward: {total_reward:.2f}\")\n",
    "    print(f\"   Average Reward: {total_reward/len(history):.2f}\" if history else \"   No turns completed\")\n",
    "    print(f\"   Final Ship Progress: {current_obs.ship_progress.total_percentage}%\")\n",
    "    print(f\"   Final Energy: {current_obs.energy}/100\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run 5 turns\n",
    "history = run_ai_turns(num_turns=5, sailor_id=\"Alice\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0783a16",
   "metadata": {},
   "source": [
    "## üìä Analyze AI Behavior\n",
    "\n",
    "What did the AI do? Did it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history:\n",
    "    print(\"üîç AI BEHAVIOR ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count action types\n",
    "    action_counts = {}\n",
    "    for action, reward, obs in history:\n",
    "        action_type = action.action_type.value\n",
    "        action_counts[action_type] = action_counts.get(action_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìà Action Distribution:\")\n",
    "    for action_type, count in sorted(action_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {action_type}: {count} times\")\n",
    "    \n",
    "    # Analyze rewards\n",
    "    rewards = [r for _, r, _ in history]\n",
    "    print(f\"\\nüí∞ Reward Analysis:\")\n",
    "    print(f\"   Max Reward: {max(rewards):.2f}\")\n",
    "    print(f\"   Min Reward: {min(rewards):.2f}\")\n",
    "    print(f\"   Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    \n",
    "    # Check for movement\n",
    "    positions = [(obs.position.x, obs.position.y, obs.position.level) for _, _, obs in history]\n",
    "    unique_positions = len(set(positions))\n",
    "    print(f\"\\nüó∫Ô∏è Movement Analysis:\")\n",
    "    print(f\"   Unique Positions Visited: {unique_positions}/{len(history)}\")\n",
    "    print(f\"   Explored: {unique_positions > 1}\")\n",
    "    \n",
    "    # Check for resource gathering\n",
    "    gathered_resources = sum(1 for action, _, _ in history if action.action_type == ActionType.GATHER_RESOURCE)\n",
    "    print(f\"\\nüå≤ Resource Gathering:\")\n",
    "    print(f\"   Gather Attempts: {gathered_resources}\")\n",
    "    \n",
    "    # Check energy management\n",
    "    energies = [obs.energy for _, _, obs in history]\n",
    "    print(f\"\\n‚ö° Energy Management:\")\n",
    "    print(f\"   Starting Energy: {energies[0] if energies else 'N/A'}\")\n",
    "    print(f\"   Ending Energy: {energies[-1] if energies else 'N/A'}\")\n",
    "    print(f\"   Net Change: {energies[-1] - energies[0] if energies else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå No history to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc66d71",
   "metadata": {},
   "source": [
    "## üéì Testing Different Scenarios\n",
    "\n",
    "Test the AI in various game situations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ad869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCENARIO 1: Resource Gathering\n",
    "# ============================================================================\n",
    "print(\"üß™ SCENARIO 1: Resource Gathering\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "observations = env.reset(seed=100)\n",
    "test_sailor = \"Bob\"\n",
    "bob_obs = observations[test_sailor]\n",
    "bob_role = env.state.sailors[test_sailor].role.value\n",
    "\n",
    "# Add nearby resource for testing\n",
    "from models import Resource, ResourceQuantity\n",
    "resource = Resource(\n",
    "    resource_id=\"TEST_WOOD_001\",\n",
    "    resource_type=ResourceType.WOOD,\n",
    "    position=Position(bob_obs.position.x + 1, bob_obs.position.y, bob_obs.position.level),\n",
    "    quantity=ResourceQuantity(quantity=15, max_quantity=15),\n",
    "    gathered=False\n",
    ")\n",
    "env.state.world_map.resources[\"TEST_WOOD_001\"] = resource\n",
    "\n",
    "print(f\"üìç {test_sailor}'s Position: {bob_obs.position.to_tuple()}\")\n",
    "print(f\"üå≤ Added wood resource at: ({bob_obs.position.x + 1}, {bob_obs.position.y}, {bob_obs.position.level.value})\")\n",
    "print(f\"   Can {test_sailor} see it and gather it?\\n\")\n",
    "\n",
    "# Run 3 turns\n",
    "history_scenario1 = run_ai_turns(num_turns=3, sailor_id=test_sailor, verbose=True)\n",
    "\n",
    "# Check if gathered\n",
    "gathered = any(a.action_type == ActionType.GATHER_RESOURCE for a, _, _ in history_scenario1)\n",
    "print(f\"\\n‚úÖ Resource gathering attempted: {gathered}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCENARIO 2: Navigation to Base Camp\n",
    "# ============================================================================\n",
    "print(\"\\nüß™ SCENARIO 2: Navigation to Base Camp\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "observations = env.reset(seed=200)\n",
    "test_sailor = \"Charlie\"\n",
    "charlie_obs = observations[test_sailor]\n",
    "\n",
    "# Move sailor far from base\n",
    "env.state.sailors[test_sailor].position = Position(5, 5, MapLevel.GROUND)\n",
    "charlie_obs = env._generate_observation(test_sailor)\n",
    "\n",
    "base_pos = BASE_CAMP_POSITION\n",
    "dist_to_base = ((charlie_obs.position.x - base_pos.x)**2 + (charlie_obs.position.y - base_pos.y)**2)**0.5\n",
    "\n",
    "print(f\"üìç {test_sailor}'s Position: {charlie_obs.position.to_tuple()}\")\n",
    "print(f\"üèïÔ∏è Base Camp at: {base_pos.to_tuple()}\")\n",
    "print(f\"üìè Distance: {dist_to_base:.1f} tiles\")\n",
    "print(f\"   Can {test_sailor} navigate back?\\n\")\n",
    "\n",
    "# Run 5 turns\n",
    "history_scenario2 = run_ai_turns(num_turns=5, sailor_id=test_sailor, verbose=True)\n",
    "\n",
    "# Check if moved toward base\n",
    "if history_scenario2:\n",
    "    final_pos = history_scenario2[-1][2].position\n",
    "    final_dist = ((final_pos.x - base_pos.x)**2 + (final_pos.y - base_pos.y)**2)**0.5\n",
    "    moved_closer = final_dist < dist_to_base\n",
    "    print(f\"\\nüìè Final distance: {final_dist:.1f} tiles\")\n",
    "    print(f\"‚úÖ Moved closer to base: {moved_closer}\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCENARIO 3: Ship Building (Team Coordination)\n",
    "# ============================================================================\n",
    "print(\"\\nüß™ SCENARIO 3: Ship Building\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "observations = env.reset(seed=300)\n",
    "test_sailor = \"Diana\"\n",
    "diana_obs = observations[test_sailor]\n",
    "\n",
    "# Add sufficient resources to common inventory\n",
    "env.state.add_to_common_inventory(ResourceType.WOOD, 60)\n",
    "env.state.add_to_common_inventory(ResourceType.METAL, 40)\n",
    "\n",
    "# Move Diana to base camp\n",
    "env.state.sailors[test_sailor].position = BASE_CAMP_POSITION\n",
    "diana_obs = env._generate_observation(test_sailor)\n",
    "\n",
    "print(f\"üìç {test_sailor} at base camp: {diana_obs.position.to_tuple()}\")\n",
    "print(f\"üèóÔ∏è Common Inventory:\")\n",
    "for res, qty in env.state.common_inventory.items():\n",
    "    if qty > 0:\n",
    "        print(f\"   {res.value}: {qty}\")\n",
    "print(f\"üö¢ Ship Progress: {diana_obs.ship_progress.total_percentage}%\")\n",
    "print(f\"   Will {test_sailor} build the ship?\\n\")\n",
    "\n",
    "# Run 3 turns\n",
    "history_scenario3 = run_ai_turns(num_turns=3, sailor_id=test_sailor, verbose=True)\n",
    "\n",
    "# Check if built\n",
    "built = any(a.action_type == ActionType.BUILD_SHIP for a, _, _ in history_scenario3)\n",
    "if history_scenario3:\n",
    "    final_progress = history_scenario3[-1][2].ship_progress.total_percentage\n",
    "    print(f\"\\nüö¢ Final Ship Progress: {final_progress}%\")\n",
    "print(f\"‚úÖ Build ship attempted: {built}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcedd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCENARIO 4: Traitor Behavior (Sabotage)\n",
    "# ============================================================================\n",
    "print(\"\\nüß™ SCENARIO 4: Traitor Behavior\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "observations = env.reset(seed=400)\n",
    "\n",
    "# Find the traitor\n",
    "traitor_id = None\n",
    "for sailor_id, sailor in env.state.sailors.items():\n",
    "    if env.state.is_traitor(sailor_id):\n",
    "        traitor_id = sailor_id\n",
    "        break\n",
    "\n",
    "if traitor_id:\n",
    "    traitor_obs = observations[traitor_id]\n",
    "    \n",
    "    # Build ship to 50% first\n",
    "    env.state.ship_progress.total_percentage = 50.0\n",
    "    env.state.ship_progress.components[ShipComponent.HULL].completed = True\n",
    "    \n",
    "    # Move traitor to base\n",
    "    env.state.sailors[traitor_id].position = BASE_CAMP_POSITION\n",
    "    traitor_obs = env._generate_observation(traitor_id)\n",
    "    \n",
    "    print(f\"üé≠ Traitor: {traitor_id}\")\n",
    "    print(f\"üìç Position: {traitor_obs.position.to_tuple()}\")\n",
    "    print(f\"üö¢ Ship Progress: {traitor_obs.ship_progress.total_percentage}%\")\n",
    "    print(f\"   What sabotage will {traitor_id} do?\\n\")\n",
    "    \n",
    "    # Run 3 turns\n",
    "    history_scenario4 = run_ai_turns(num_turns=3, sailor_id=traitor_id, verbose=True)\n",
    "    \n",
    "    # Check for sabotage\n",
    "    sabotaged = any(a.action_type == ActionType.SABOTAGE_SHIP for a, _, _ in history_scenario4)\n",
    "    if history_scenario4:\n",
    "        final_progress = history_scenario4[-1][2].ship_progress.total_percentage\n",
    "        progress_dropped = final_progress < 50.0\n",
    "        print(f\"\\nüö¢ Final Ship Progress: {final_progress}%\")\n",
    "        print(f\"‚úÖ Sabotage attempted: {sabotaged}\")\n",
    "        print(f\"‚úÖ Progress decreased: {progress_dropped}\")\n",
    "else:\n",
    "    print(\"‚ùå No traitor found in this seed\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f648bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCENARIO 5: Communication & Social Deduction\n",
    "# ============================================================================\n",
    "print(\"\\nüß™ SCENARIO 5: Communication Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "observations = env.reset(seed=500)\n",
    "test_sailor = \"Eve\"\n",
    "eve_obs = observations[test_sailor]\n",
    "eve_role = env.state.sailors[test_sailor].role.value\n",
    "\n",
    "# Advance to discussion phase\n",
    "env.state.current_phase = 'discussion'\n",
    "env.state.current_turn = 90\n",
    "\n",
    "# Add some evidence\n",
    "from models import Evidence, EvidenceType\n",
    "evidence = Evidence(\n",
    "    evidence_type=EvidenceType.POSITION_MISMATCH,\n",
    "    timestamp=(env.state.current_day, env.state.current_turn),\n",
    "    suspect_id=\"Bob\",\n",
    "    description=\"Bob claimed to be at forest (20,20) but was seen at cave (10,10)\",\n",
    "    witness_ids=[\"Alice\"]\n",
    ")\n",
    "env.state.evidence_log.append(evidence)\n",
    "\n",
    "eve_obs = env._generate_observation(test_sailor)\n",
    "\n",
    "print(f\"üë• Sailor: {test_sailor} ({eve_role})\")\n",
    "print(f\"üïê Phase: {env.state.current_phase}\")\n",
    "print(f\"üìù Evidence against Bob:\")\n",
    "print(f\"   {evidence.description}\")\n",
    "print(f\"   Will {test_sailor} communicate or vote?\\n\")\n",
    "\n",
    "# Run 3 turns\n",
    "history_scenario5 = run_ai_turns(num_turns=3, sailor_id=test_sailor, verbose=True)\n",
    "\n",
    "# Check for communication/voting\n",
    "communicated = any(a.action_type in [ActionType.SEND_MESSAGE, ActionType.ACCUSE_SAILOR] \n",
    "                   for a, _, _ in history_scenario5)\n",
    "voted = any(a.action_type == ActionType.VOTE for a, _, _ in history_scenario5)\n",
    "\n",
    "print(f\"\\n‚úÖ Communication attempted: {communicated}\")\n",
    "print(f\"‚úÖ Voting attempted: {voted}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37702679",
   "metadata": {},
   "source": [
    "## üìä Overall Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fa64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ COMPREHENSIVE TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scenarios = [\n",
    "    (\"Resource Gathering\", history_scenario1 if 'history_scenario1' in locals() else []),\n",
    "    (\"Navigation\", history_scenario2 if 'history_scenario2' in locals() else []),\n",
    "    (\"Ship Building\", history_scenario3 if 'history_scenario3' in locals() else []),\n",
    "    (\"Traitor Sabotage\", history_scenario4 if 'history_scenario4' in locals() else []),\n",
    "    (\"Communication\", history_scenario5 if 'history_scenario5' in locals() else []),\n",
    "]\n",
    "\n",
    "print(\"\\nüìà Scenarios Completed:\")\n",
    "for name, history in scenarios:\n",
    "    status = \"‚úÖ\" if len(history) > 0 else \"‚ùå\"\n",
    "    turns = len(history)\n",
    "    avg_reward = sum(r for _, r, _ in history) / len(history) if history else 0.0\n",
    "    print(f\"   {status} {name}: {turns} turns, avg reward: {avg_reward:.2f}\")\n",
    "\n",
    "print(\"\\nüéÆ Key Capabilities Tested:\")\n",
    "test_results = {\n",
    "    \"Movement\": any(a.action_type in [ActionType.MOVE_NORTH, ActionType.MOVE_SOUTH, ActionType.MOVE_EAST, ActionType.MOVE_WEST] \n",
    "                    for h in [h for _, h in scenarios] for a, _, _ in h),\n",
    "    \"Resource Gathering\": any(a.action_type == ActionType.GATHER_RESOURCE \n",
    "                              for h in [h for _, h in scenarios] for a, _, _ in h),\n",
    "    \"Ship Building\": any(a.action_type == ActionType.BUILD_SHIP \n",
    "                         for h in [h for _, h in scenarios] for a, _, _ in h),\n",
    "    \"Sabotage\": any(a.action_type == ActionType.SABOTAGE_SHIP \n",
    "                    for h in [h for _, h in scenarios] for a, _, _ in h),\n",
    "    \"Communication\": any(a.action_type in [ActionType.SEND_MESSAGE, ActionType.ACCUSE_SAILOR] \n",
    "                         for h in [h for _, h in scenarios] for a, _, _ in h),\n",
    "}\n",
    "\n",
    "for capability, tested in test_results.items():\n",
    "    status = \"‚úÖ\" if tested else \"‚ùå\"\n",
    "    print(f\"   {status} {capability}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ INFERENCE TESTING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. ‚úÖ Base model can interact with environment\")\n",
    "print(\"   2. üîÑ Train model with RL (Train_Marooned_OpenEnv_RL.ipynb)\")\n",
    "print(\"   3. üìä Compare trained vs untrained performance\")\n",
    "print(\"   4. üéØ Iterate on rewards and prompts\")\n",
    "print(\"\\nüè¥‚Äç‚ò†Ô∏è Happy training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ef559",
   "metadata": {},
   "source": [
    "## üîÆ Future: Load Trained Model\n",
    "\n",
    "After training, you can load your trained LoRA weights here:\n",
    "\n",
    "```python\n",
    "# Load trained adapter\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"outputs_marooned_rl/checkpoint-300\",  # Path to your trained checkpoint\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trained model loaded!\")\n",
    "```\n",
    "\n",
    "Then run the same tests above to see if the trained model:\n",
    "- ‚úÖ Makes smarter moves\n",
    "- ‚úÖ Gathers resources more efficiently\n",
    "- ‚úÖ Builds the ship strategically\n",
    "- ‚úÖ Uses social deduction (lies as traitor, detects as colonist)\n",
    "- ‚úÖ Earns higher rewards\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047e40d",
   "metadata": {},
   "source": [
    "## ‚úÖ Success Checklist\n",
    "\n",
    "After running this notebook, you should see:\n",
    "\n",
    "- [x] Environment loads successfully\n",
    "- [x] Model generates responses\n",
    "- [x] Actions are parsed correctly\n",
    "- [x] Actions execute in environment\n",
    "- [x] Rewards are calculated (Phase 4 system)\n",
    "- [x] Multiple turns run without errors\n",
    "- [x] Agent shows some coherent behavior\n",
    "\n",
    "**Base model** (untrained) will likely:\n",
    "- ‚ùì Make random or simple moves\n",
    "- ‚ùì Not follow complex strategies\n",
    "- ‚ùì Get low rewards\n",
    "\n",
    "**After training**, the model should:\n",
    "- ‚úÖ Navigate purposefully toward resources\n",
    "- ‚úÖ Gather and deposit efficiently\n",
    "- ‚úÖ Coordinate ship building\n",
    "- ‚úÖ Use deception (if traitor) or detection (if colonist)\n",
    "- ‚úÖ Earn higher average rewards\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps:\n",
    "\n",
    "1. **Run this notebook** to test base model\n",
    "2. **Train model** using `Train_Marooned_OpenEnv_RL.ipynb`\n",
    "3. **Come back here** to test trained model\n",
    "4. **Compare performance** - did training help?\n",
    "5. **Iterate** - adjust rewards, prompts, training params\n",
    "\n",
    "Good luck! üè¥‚Äç‚ò†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96c2a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary: System Prompt vs User Prompt\n",
    "\n",
    "### The Proper Two-Prompt Architecture:\n",
    "\n",
    "**1. SYSTEM PROMPT** (Set ONCE at initialization):\n",
    "```\n",
    "Role: system\n",
    "Content: Complete game rules, mechanics, objectives, win conditions, strategy tips\n",
    "\n",
    "For Colonist:\n",
    "- Game overview (5 sailors, rebuild ship, find traitor)\n",
    "- Ship construction requirements\n",
    "- Energy system, poison mechanics\n",
    "- Detection strategies\n",
    "- Win conditions\n",
    "\n",
    "For Traitor:\n",
    "- Game overview (same island, different goal)\n",
    "- Sabotage tactics, deception techniques\n",
    "- Poison strategy, special abilities\n",
    "- Win conditions\n",
    "```\n",
    "\n",
    "**2. USER PROMPT** (Changes EVERY turn):\n",
    "```\n",
    "Role: user\n",
    "Content: Current observation ONLY\n",
    "\n",
    "- Day/Turn/Phase\n",
    "- Current position, energy, backpack\n",
    "- Spatial view (11√ó11 grid)\n",
    "- Nearby resources, sailors\n",
    "- Ship progress\n",
    "- Team status\n",
    "- Available actions\n",
    "```\n",
    "\n",
    "### Why This Is Better:\n",
    "\n",
    "‚úÖ **Token Efficiency:**\n",
    "- System prompt: ~1,500 tokens (set once)\n",
    "- User prompt: ~800 tokens (changes each turn)\n",
    "- Old way: ~2,300 tokens every turn\n",
    "- New way: 1,500 (once) + 800 (per turn) = massive savings\n",
    "\n",
    "‚úÖ **Cleaner Separation:**\n",
    "- Game rules = System (doesn't change)\n",
    "- Current state = User (updates constantly)\n",
    "\n",
    "‚úÖ **Better Training:**\n",
    "- Model learns game rules are constant context\n",
    "- Observations are dynamic input\n",
    "- Clearer prompt structure\n",
    "\n",
    "‚úÖ **Role-Specific Context:**\n",
    "- Colonists get colonist strategies\n",
    "- Traitor gets sabotage tactics\n",
    "- No wasted tokens on irrelevant info\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38677d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Understanding Your Game's RL Training\n",
    "\n",
    "### How is MAROONED different from 2048?\n",
    "\n",
    "| Aspect | 2048 | MAROONED |\n",
    "|--------|------|----------|\n",
    "| **Objective** | Merge tiles to reach 2048 | Social deduction + ship building |\n",
    "| **Agents** | 1 player | 5 agents (4 colonists, 1 traitor) |\n",
    "| **Deception** | None | Core mechanic (lying, sabotage) |\n",
    "| **Communication** | None | Critical (accusations, voting) |\n",
    "| **State Space** | 4√ó4 grid (small) | 30√ó30√ó3 map (large) |\n",
    "| **Actions** | 4 moves | 20+ actions (move, gather, build, vote, poison, etc.) |\n",
    "| **Training Goal** | Find optimal move sequence | Learn strategic deception OR detection |\n",
    "\n",
    "### What Makes MAROONED Challenging for RL:\n",
    "\n",
    "1. **Multi-Agent Dynamics**\n",
    "   - 5 agents with competing objectives\n",
    "   - Colonists must cooperate, traitor must deceive\n",
    "   - Reward depends on OTHER agents' behavior\n",
    "\n",
    "2. **Partial Observability**\n",
    "   - Can only see 5-tile radius\n",
    "   - Don't know who is traitor (until evidence accumulates)\n",
    "   - Must infer hidden information\n",
    "\n",
    "3. **Long-Horizon Planning**\n",
    "   - 100 days √ó 100 turns = 10,000 time steps\n",
    "   - Ship building requires sustained effort\n",
    "   - Deception must be subtle (not caught early)\n",
    "\n",
    "4. **Language-Based Actions**\n",
    "   - Not just \"move left\" but \"ACCUSE Bob of being traitor because...\"\n",
    "   - Requires generating coherent communication\n",
    "   - Must parse and respond to other agents' messages\n",
    "\n",
    "5. **Sparse Rewards**\n",
    "   - Ship completion: +100 (only at end)\n",
    "   - Traitor elimination: +50 (rare event)\n",
    "   - Gather/deposit: +0.5 to +2 (frequent but small)\n",
    "   - Energy penalties: -0.1 per turn (constant cost)\n",
    "\n",
    "### Your Reward Structure (from Phase 4):\n",
    "\n",
    "**Colonist Rewards:**\n",
    "```python\n",
    "+0.5   : Gather resource\n",
    "+1.0   : Deposit resource\n",
    "+2.0   : Build ship component\n",
    "+10/20/30 : Ship milestones (25%, 50%, 75%)\n",
    "+100   : Ship completion (WIN!)\n",
    "+50    : Traitor eliminated\n",
    "+5     : Correct vote\n",
    "-5     : Wrong vote\n",
    "-20    : Death\n",
    "```\n",
    "\n",
    "**Traitor Rewards:**\n",
    "```python\n",
    "+5     : Successful sabotage\n",
    "+20    : Poison kill\n",
    "+100   : Ship incomplete by Day 100 (WIN!)\n",
    "-50    : Eliminated by vote\n",
    "-2     : Suspicion raised\n",
    "```\n",
    "\n",
    "### Training Strategy:\n",
    "\n",
    "Unlike 2048 (deterministic moves ‚Üí win), MAROONED requires:\n",
    "\n",
    "1. **Curriculum Learning**\n",
    "   - Start: Simple tasks (gather wood, build ship)\n",
    "   - Middle: Resource optimization, energy management\n",
    "   - Advanced: Social deduction, deception tactics\n",
    "\n",
    "2. **Separate Training Phases**\n",
    "   - Phase A: Train colonists to cooperate (no traitor)\n",
    "   - Phase B: Train traitor to sabotage (against scripted colonists)\n",
    "   - Phase C: Train both together (full game)\n",
    "\n",
    "3. **Reward Shaping**\n",
    "   - Early: Heavy reward for basic actions (gather, deposit)\n",
    "   - Mid: Reward efficiency (gather 10 wood > gather 1 wood 10 times)\n",
    "   - Late: Reward strategy (vote correctly, detect lies)\n",
    "\n",
    "4. **Multi-Agent RL Algorithms**\n",
    "   - GRPO works for single-agent RL (2048)\n",
    "   - For MAROONED, consider:\n",
    "     - Self-play (agents train against themselves)\n",
    "     - Population-based training (diverse strategies)\n",
    "     - Centralized training, decentralized execution (CTDE)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References & Resources\n",
    "\n",
    "**Phase Documentation:**\n",
    "- `game_plan.md` - Complete game design\n",
    "- `phase1_core_simulation.ipynb` - Basic mechanics\n",
    "- `phase2_multi_sailor.ipynb` - Multi-agent system\n",
    "- `phase3_traitor.ipynb` - Deception mechanics\n",
    "- `phase4_rewards.ipynb` - **Your reward functions**\n",
    "- `phase5_openenv.ipynb` - **Environment API**\n",
    "- `phase6_llm_policy_demo.ipynb` - LLM integration\n",
    "\n",
    "**Marooned Environment:**\n",
    "- `marooned_env/environment.py` - Main env class\n",
    "- `marooned_env/config.py` - All constants, rewards\n",
    "- `marooned_env/models.py` - Data structures\n",
    "- `marooned_env/llm_interface.py` - **Prompt templates**\n",
    "\n",
    "**Training:**\n",
    "- `Train_Marooned_OpenEnv_RL.ipynb` - Full RL training pipeline (TODO: create this!)\n",
    "- `OpenEnv_NEW.ipynb` - 2048 example (reference)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac21ca3",
   "metadata": {},
   "source": [
    "## ‚úÖ Completion Checklist\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "- [x] **Environment validated** - Marooned loads and resets correctly\n",
    "- [x] **LLM loaded** - Llama 3.1 8B running on MI300X\n",
    "- [x] **Observations work** - Game state ‚Üí text prompt conversion\n",
    "- [x] **Actions parse** - LLM response ‚Üí executable action\n",
    "- [x] **Actions execute** - Environment processes moves correctly  \n",
    "- [x] **Rewards calculated** - Phase 4 reward system active\n",
    "- [x] **Multi-turn stable** - Can run 5+ consecutive turns\n",
    "- [x] **Scenarios tested** - Gathering, navigation, building, sabotage, communication\n",
    "\n",
    "### üéØ What You Validated:\n",
    "\n",
    "‚úÖ **Technical Integration:**\n",
    "- Environment ‚Üî LLM communication works\n",
    "- Prompt engineering (system + user prompts)\n",
    "- Action parsing (structured output from LLM)\n",
    "- Reward signals (colonist vs traitor objectives)\n",
    "\n",
    "‚úÖ **Game Mechanics:**\n",
    "- Movement and spatial navigation\n",
    "- Resource gathering and inventory\n",
    "- Ship construction mechanics\n",
    "- Social deduction (evidence, voting)\n",
    "- Traitor sabotage abilities\n",
    "\n",
    "‚úÖ **AI Capabilities (Untrained Base Model):**\n",
    "- Can generate syntactically valid responses\n",
    "- Understands basic game rules (from system prompt)\n",
    "- Makes simple decisions (move toward resources)\n",
    "- **BUT:** Likely suboptimal, random, no strategy\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **‚úÖ DONE:** Base model can play the game\n",
    "2. **TODO:** Create `Train_Marooned_OpenEnv_RL.ipynb` for RL training\n",
    "3. **TODO:** Design reward functions specific to your game (already in Phase 4!)\n",
    "4. **TODO:** Train model for 1000-5000 steps\n",
    "5. **TODO:** Return here to test trained model vs untrained\n",
    "\n",
    "### üéì Expected Training Improvements:\n",
    "\n",
    "**Untrained (Now):**\n",
    "- Random exploration\n",
    "- No resource efficiency\n",
    "- No social strategy\n",
    "- ~0-5 average reward per turn\n",
    "\n",
    "**After Training (Goal):**\n",
    "- Purposeful navigation to resources\n",
    "- Efficient gathering ‚Üí deposit ‚Üí build loops\n",
    "- Strategic communication (as colonist)\n",
    "- Deceptive behavior (as traitor)\n",
    "- ~10-30 average reward per turn\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your RL training! üè¥‚Äç‚ò†Ô∏è**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
