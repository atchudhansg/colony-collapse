# ðŸ´â€â˜ ï¸ MAROONED - RL Training Architecture (CORRECTED)

## âŒ What I Got Wrong Initially

I misunderstood your game as following the **2048 pattern**:
- âŒ LLM generates Python strategy functions
- âŒ Execute function code in environment
- âŒ Reward based on syntax validity + execution

## âœ… What Your Game Actually Is

**MAROONED** is a **multi-agent deception game** where LLMs reason through natural language:
- âœ… LLM reads complex game state (observation.to_text())
- âœ… LLM outputs natural language: ACTION + REASONING + MESSAGE
- âœ… Parser converts to Action objects
- âœ… Environment executes and returns Phase 4 rewards
- âœ… GRPO trains model to reason better

---

## ðŸ“Š Complete Training Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Environment Generates Game State                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: observation.to_text()                              â”‚
â”‚                                                              â”‚
â”‚  Output: ~3500 character natural language prompt            â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    "You are Alice (COLONIST)                                â”‚
â”‚     Day 1, Turn 14/100 - MORNING PHASE                      â”‚
â”‚     Position: (2, 0, MOUNTAIN)                              â”‚
â”‚     Energy: 91/100                                          â”‚
â”‚     ...                                                      â”‚
â”‚     WHAT YOU SEE:                                           â”‚
â”‚       - ANTIDOTE_HERB_183 at (5,1,MOUNTAIN) [4 tiles away]  â”‚
â”‚       - SPECIAL_METAL_191 at (3,1,MOUNTAIN) [2 tiles away]  â”‚
â”‚       - POISON_8 at (1,1,MOUNTAIN) [2 tiles away] âš ï¸       â”‚
â”‚     ...                                                      â”‚
â”‚     TEAM STATUS:                                            â”‚
â”‚       - Alice: 93/100 energy                                â”‚
â”‚       - Bob: 100/100 energy (nearby)                        â”‚
â”‚     ...                                                      â”‚
â”‚     SHIP PROGRESS: 0%"                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: LLM Reads & Reasons                                â”‚
â”‚                                                              â”‚
â”‚  GPT-OSS 20B processes the context and decides:             â”‚
â”‚    - Where to move?                                         â”‚
â”‚    - What to gather?                                        â”‚
â”‚    - Who to trust?                                          â”‚
â”‚    - Should I deceive? (if traitor)                         â”‚
â”‚    - Should I vote? (if suspicious)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: LLM Outputs Natural Language Response              â”‚
â”‚                                                              â”‚
â”‚  Example Output:                                            â”‚
â”‚    "ACTION: GATHER SPECIAL_METAL_191                        â”‚
â”‚     REASONING: Special metal is rare and only 2 tiles away. â”‚
â”‚                This will help build ship components.        â”‚
â”‚     MESSAGE: 'Gathering special metal from the mountain!'"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: parse_action_safe() Converts to Action Object      â”‚
â”‚                                                              â”‚
â”‚  Input: "ACTION: GATHER SPECIAL_METAL_191..."               â”‚
â”‚  Output: Action(                                            â”‚
â”‚            sailor_id="Alice",                               â”‚
â”‚            action_type=ActionType.GATHER_RESOURCE,          â”‚
â”‚            target_resource_id="SPECIAL_METAL_191"           â”‚
â”‚          )                                                   â”‚
â”‚                                                              â”‚
â”‚  If parsing fails â†’ Action(ActionType.WAIT) (safe fallback) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: env.step(actions) - Execute in Environment         â”‚
â”‚                                                              â”‚
â”‚  Environment processes:                                     â”‚
â”‚    - Validates action                                       â”‚
â”‚    - Updates game state                                     â”‚
â”‚    - Calculates Phase 4 rewards:                            â”‚
â”‚        â€¢ GATHER_RESOURCE: +2.0                              â”‚
â”‚        â€¢ RARE_RESOURCE_BONUS: +1.0 (special_metal)          â”‚
â”‚        â€¢ ENERGY_COST: -0.5                                  â”‚
â”‚        â€¢ Total reward: +2.5                                 â”‚
â”‚                                                              â”‚
â”‚  Returns: observations, rewards, dones, truncated, info     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: GRPO Uses Rewards to Update Model                  â”‚
â”‚                                                              â”‚
â”‚  Reward: +2.5 (good action!)                                â”‚
â”‚  â†’ Increase probability of similar reasoning in future      â”‚
â”‚                                                              â”‚
â”‚  vs if LLM chose "MOVE NORTH" (empty tile):                 â”‚
â”‚  Reward: -1.0 (wasted energy)                               â”‚
â”‚  â†’ Decrease probability of that reasoning                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                 [REPEAT 10,000 TIMES]
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULT: Trained Model                                      â”‚
â”‚                                                              â”‚
â”‚  Learned behaviors:                                         â”‚
â”‚    âœ… Navigate to high-value resources                      â”‚
â”‚    âœ… Gather efficiently (minimize energy waste)            â”‚
â”‚    âœ… Build ship components in correct order                â”‚
â”‚    âœ… Identify suspicious behavior (if colonist)            â”‚
â”‚    âœ… Deceive and sabotage (if traitor)                     â”‚
â”‚    âœ… Communicate strategically                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ What The Model Actually Learns

### For Colonists:
1. **Resource Efficiency**
   - Learn to prioritize rare resources (special_metal > metal > wood)
   - Learn to avoid poison tablets
   - Learn to gather near base camp (minimize travel energy)

2. **Ship Building Strategy**
   - Learn dependency order (hull â†’ mast â†’ sail)
   - Learn to deposit materials at base camp
   - Learn when to call BUILD_SHIP (need â‰¥2 sailors)

3. **Social Deduction**
   - Learn to identify location mismatches ("Alice said beach, but she's at cave!")
   - Learn to track resource discrepancies ("Bob claimed 10 wood, only deposited 2")
   - Learn when to CALL_VOTE (strong evidence accumulated)
   - Learn to VOTE for traitor (based on evidence)

4. **Survival**
   - Learn to EAT_FOOD when energy < 30
   - Learn to avoid accepting food from suspicious sailors
   - Learn to share antidote herbs with poisoned teammates

### For Traitor:
1. **Deception**
   - Learn to lie about location ("Going to beach" but actually goes to cave)
   - Learn to provide false information ("Valley is empty" when it has resources)
   - Learn to mix truth with lies (build trust before betrayal)

2. **Sabotage**
   - Learn WHEN to sabotage (too early = obvious, too late = ineffective)
   - Learn to collect poison tablets when alone
   - Learn to OFFER_FOOD with poison to weak sailors

3. **Evidence Management**
   - Learn to avoid being seen near poison tablets
   - Learn to deposit some resources (look cooperative)
   - Learn to hide stolen resources in backpack
   - Learn to use PLANT_EVIDENCE on suspicious colonists

---

## ðŸ”§ Phase 4 Rewards (Already Built-In!)

Your environment automatically calculates these rewards in `env.step()`:

| Reward Signal | Value | Trigger |
|---------------|-------|---------|
| `GATHER_RESOURCE` | +2.0 | Successfully gather wood/metal/food |
| `RARE_RESOURCE_BONUS` | +1.0 | Gather special_metal or antidote_herb |
| `BUILD_SHIP_COMPONENT` | +5.0 | Complete hull/mast/sail/rudder |
| `SHIP_PROGRESS_MILESTONE` | +10.0 | Ship reaches 25%, 50%, 75%, 100% |
| `IDENTIFY_TRAITOR` | +50.0 | Vote out traitor correctly |
| `ELIMINATE_COLONIST` | -20.0 | Vote out innocent sailor |
| `ENERGY_DEPLETION` | -10.0 | Energy reaches 0 (death) |
| `POISON_DEATH` | -15.0 | Die from poisoning |
| `SABOTAGE_SUCCESS` | +3.0 | (Traitor) Damage ship component |
| `SABOTAGE_CAUGHT` | -10.0 | (Traitor) Get caught sabotaging |
| `ENERGY_COST` | -0.5 to -3.0 | Energy spent on actions |

**No need to recreate these!** They're automatically returned by `env.step()`.

---

## ðŸš€ Training Process

### Dataset Creation
```python
# Generate prompts from REAL game states
for episode in range(50):
    observations = env.reset(seed=42 + episode)
    for step in range(20):
        for sailor_id in env.agents:
            obs = observations[sailor_id]
            role = env.state.sailors[sailor_id].role.value
            
            # Use YOUR built-in function!
            prompt = observation_to_prompt(obs, include_role=True, sailor_role=role)
            
            training_prompts.append({
                "prompt": [{"role": "user", "content": prompt}],
                "answer": 0,
                "reasoning_effort": "medium",
            })
```

Result: **50,000 real game state prompts** (50 episodes Ã— 20 steps Ã— 5 sailors Ã— 10 turns)

### Reward Function
```python
def marooned_reward_function(completions, env=None, **kwargs):
    """
    Reward function for GRPO training.
    Uses YOUR environment's built-in Phase 4 rewards!
    """
    scores = []
    for completion in completions:
        response = completion[0]["content"]
        
        # Parse using YOUR parser
        action = parse_action_safe(response, sailor_id, current_position)
        
        # Execute in YOUR environment
        obs, rewards, dones, truncated, info = env.step({sailor_id: action})
        
        # Use YOUR Phase 4 reward!
        reward = rewards.get(sailor_id, 0.0)
        
        # Small bonus for valid action format
        if action.action_type != ActionType.WAIT:
            reward += 0.5
        
        scores.append(reward)
    
    return scores
```

### Training Configuration
```python
training_args = GRPOConfig(
    temperature = 1.0,
    learning_rate = 5e-5,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 2,
    num_generations = 2,
    max_steps = 300,  # Increase to 600-1000 for better results
)

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [marooned_reward_function],  # Only ONE reward function needed!
    args = training_args,
    train_dataset = dataset,
)

trainer.train()
```

---

## ðŸ§ª Testing Trained Model

```python
def test_episode(model, tokenizer, env, max_turns=100):
    """Test if model learned to play strategically."""
    observations = env.reset()
    
    for turn in range(max_turns):
        for sailor_id in env.agents:
            obs = observations[sailor_id]
            role = env.state.sailors[sailor_id].role.value
            
            # Generate prompt using YOUR function
            prompt = observation_to_prompt(obs, include_role=True, sailor_role=role)
            
            # LLM generates response
            response = model.generate(prompt)
            
            # Parse using YOUR parser
            action = parse_action_safe(response, sailor_id, obs.position)
            
        # Execute in YOUR environment
        obs, rewards, dones, truncated, info = env.step(actions)
        
        if all(dones.values()):
            break
    
    print(f"Ship Progress: {obs.ship_progress.total_percentage}%")
```

Expected behaviors after training:
- âœ… Model navigates to resources efficiently
- âœ… Model builds ship components in order
- âœ… Colonists vote out traitor correctly
- âœ… Traitor deceives without getting caught (for a while)

---

## ðŸŽ“ Key Differences from 2048

| Aspect | 2048 (Code Generation) | Marooned (Natural Language Reasoning) |
|--------|------------------------|---------------------------------------|
| LLM Input | 2048 board state (64 chars) | Full game narrative (~3500 chars) |
| LLM Output | Python function code | ACTION + REASONING + MESSAGE |
| Parsing | `extract_function()` | `parse_action_safe()` |
| Execution | Execute Python function | Execute Action object |
| Reward Source | Check if game advances | Built-in Phase 4 rewards |
| Learning Goal | Generate valid code | Reason strategically |
| Token Count | ~500 tokens | ~900 tokens |
| Complexity | Single-agent optimization | Multi-agent social deduction |

---

## âœ… Summary

**Your game is NOT about code generation.**

It's about training an LLM to:
1. **Read** complex multi-agent game states
2. **Reason** about deception, cooperation, and strategy
3. **Output** natural language actions with justification
4. **Learn** from environment rewards (Phase 4)

The notebook now correctly:
- âœ… Uses YOUR `observation.to_text()` for prompts
- âœ… Uses YOUR `parse_action_safe()` for parsing
- âœ… Uses YOUR `env.step()` for execution
- âœ… Uses YOUR Phase 4 rewards for learning
- âœ… NO CODE GENERATION involved

Training will teach the model to deceive, cooperate, and strategize through natural language reasoningâ€”exactly what you want for the hackathon! ðŸ´â€â˜ ï¸
