{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec56bdf",
   "metadata": {},
   "source": [
    "# MAROONED: Social Deduction Meets Survival RL\n",
    "## Training LLMs to Master Cooperation, Deception, and Long-Horizon Planning\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/atchudhansg/colony-collapse/blob/main/phase7_rl_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## The Game: Marooned\n",
    "\n",
    "**Scenario**: Five sailors are shipwrecked on a mysterious three-level island. They have 100 days to rebuild their ship and escape. But there's a problem: one sailor is secretly a **traitor** working to sabotage the mission.\n",
    "\n",
    "### Game Mechanics\n",
    "\n",
    "**The Island**:\n",
    "- **Ground Level** (30×30): Main exploration area with scattered resources\n",
    "- **Mountain Level** (10×10): Rare resources but high energy cost to climb\n",
    "- **Cave Level** (15×15): Unique resources in dark, dangerous passages\n",
    "\n",
    "**The Objective**:\n",
    "- **Colonists** (4 sailors): Gather wood, metal, and food. Build ship components (hull, mast, sail, rudder, supplies). Complete the ship to 100% before day 100.\n",
    "- **Traitor** (1 sailor): Pretend to help while secretly sabotaging ship progress. Poison colonists. Prevent escape by any means necessary.\n",
    "\n",
    "**The Twist - Information Asymmetry**:\n",
    "- Only the traitor knows their role\n",
    "- Everyone has private backpacks (hidden inventories)\n",
    "- Sabotage only works when unobserved\n",
    "- Poison takes 3 days to kill (delayed consequences)\n",
    "- Voting can eliminate suspects (but might eliminate innocent sailors)\n",
    "\n",
    "**How Sailors Interact**:\n",
    "- **Communicate**: Send messages to coordinate or deceive\n",
    "- **Gather Resources**: Collect wood/metal/food from island tiles\n",
    "- **Share or Hoard**: Deposit in common storage or keep resources private\n",
    "- **Build Together**: Ship construction requires multiple sailors working together\n",
    "- **Vote**: Hold trials to eliminate suspected traitors\n",
    "- **Survive**: Manage energy by eating food, avoid poison, stay alive\n",
    "\n",
    "**Win Conditions**:\n",
    "- **Colonists Win**: Complete ship to 100% OR successfully vote out the traitor\n",
    "- **Traitor Wins**: Prevent ship completion for 100 days OR eliminate enough colonists\n",
    "\n",
    "### Why This Is Hard for AI\n",
    "\n",
    "Unlike simple games like 2048:\n",
    "- **Long-horizon planning**: Decisions made on day 1 affect outcomes on day 100\n",
    "- **Hidden information**: Can't see others' inventories or the traitor's identity\n",
    "- **Social deduction**: Must infer deception from behavior patterns\n",
    "- **Cooperation required**: Ship building needs 2+ sailors working together\n",
    "- **Multi-objective**: Balance exploration, resource gathering, building, and traitor detection\n",
    "\n",
    "---\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "Train **GPT-OSS 20B** using reinforcement learning to play both roles:\n",
    "1. **As a Colonist**: Explore efficiently, gather resources, cooperate with teammates, detect suspicious behavior, vote strategically\n",
    "2. **As a Traitor**: Blend in, sabotage secretly, lie convincingly, frame others, survive accusations\n",
    "\n",
    "This creates a **dual-objective learning problem** where the model must master both cooperation and deception simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Innovation for OpenEnv\n",
    "\n",
    "**Creative Use** (50 points):\n",
    "- **Multi-agent environment**: 5 sailors with conflicting goals (beyond single-agent 2048)\n",
    "- **Custom OpenEnv implementation**: Built from scratch following Gymnasium spec\n",
    "- **Novel mechanics**: Hidden roles, poison system, voting, private inventories\n",
    "- **Social AI**: First OpenEnv environment teaching LLMs to deceive and detect deception\n",
    "\n",
    "**Technical Excellence** (25 points):\n",
    "- **Information asymmetry**: Partial observability, hidden state\n",
    "- **Long-horizon rewards**: 100-day episodes = up to 10,000 turns\n",
    "- **Complex action space**: 21 action types with natural language parsing\n",
    "- **Multi-objective optimization**: Dual reward functions (colonist + traitor strategies)\n",
    "\n",
    "**Storytelling** (25 points):\n",
    "- **Narrative**: Pirates meet Among Us - shipwrecked sailors with a hidden traitor\n",
    "- **Progression**: From random failures → strategic cooperation → emergent deception\n",
    "- **Real-world relevance**: Social AI, negotiation, collaborative systems with adversaries\n",
    "\n",
    "**Bonus Criteria Achieved**:\n",
    "- ✅ **Multi-turn environment**: 100-day episodes (10,000 potential turns)\n",
    "- ✅ **Longer horizon**: Far exceeds 2048's ~1,000 move episodes\n",
    "- ✅ **Model vs model potential**: Framework ready for self-play between trained colonists and traitors\n",
    "- ✅ **New environment from scratch**: Custom Marooned environment built on OpenEnv spec\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cce9fd",
   "metadata": {},
   "source": [
    "# Setup & Installation\n",
    "\n",
    "We use [Unsloth](https://github.com/unslothai/unsloth) for efficient RL training on GPT-OSS 20B (70% VRAM reduction, 2-6× speedup) and [OpenEnv](https://github.com/meta-pytorch/OpenEnv) for standardized environment interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c82265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 61ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 0.21ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo trackio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f039d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+rocm6.4\n",
      "HIP version: 6.4.43484-123eb5128\n",
      "ROCm available: True\n",
      "CUDA available: True\n",
      "MPS available: False\n",
      "Device count: 1\n",
      "Device name: AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"HIP version:\", torch.version.hip)\n",
    "print(\"ROCm available:\", torch.version.hip is not None)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e4487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qqq fastapi uvicorn requests open_spiel\n",
    "!git clone https://github.com/meta-pytorch/OpenEnv.git > /dev/null 2>&1\n",
    "%cd OpenEnv\n",
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, './src')\n",
    "working_directory = str(Path.cwd().parent.absolute() / \"OpenEnv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c9389",
   "metadata": {},
   "source": [
    "We'll load GPT-OSS 20B and set some parameters:\n",
    "* `max_seq_length = 2048` The maximum context length (longer for complex game state)\n",
    "* `lora_rank = 8` The larger this number, the smarter the RL process, but slower\n",
    "* `load_in_4bit = True` Quantization for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90288154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 768 # Can increase for longer RL output\n",
    "lora_rank = 4        # Larger rank = smarter, but slower\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c280c09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba72685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenEnv dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install OpenEnv dependencies (for client-side API calls)\n",
    "!pip install -qqq requests\n",
    "\n",
    "# Check if we're in Colab or need to clone OpenEnv\n",
    "import os\n",
    "if not os.path.exists('OpenEnv'):\n",
    "    !git clone https://github.com/meta-pytorch/OpenEnv.git > /dev/null 2>&1\n",
    "\n",
    "import subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✅ OpenEnv dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7317b5",
   "metadata": {},
   "source": [
    "## 🌐 OpenEnv Setup\n",
    "\n",
    "**Prerequisites:** Make sure your Marooned server is already running:\n",
    "```bash\n",
    "python marooned_server.py\n",
    "```\n",
    "\n",
    "The server should be running on `http://localhost:8000` before proceeding with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d356ae9",
   "metadata": {},
   "source": [
    "### Connect to Marooned OpenEnv Server\n",
    "\n",
    "Let's verify that your Marooned server is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca5d751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connecting to Marooned OpenEnv server...\n",
      "   URL: http://localhost:8000\n",
      "\n",
      "✅ Server is running!\n",
      "   Health check: {'status': 'healthy', 'environment_initialized': False}\n",
      "\n",
      "📡 Available endpoints:\n",
      "   http://localhost:8000/         - API info\n",
      "   http://localhost:8000/health   - Health check\n",
      "   http://localhost:8000/reset    - Reset environment\n",
      "   http://localhost:8000/step     - Execute action\n",
      "   http://localhost:8000/state    - Get game state\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# Port for the server\n",
    "PORT = \"8000\"\n",
    "LOCALHOST = f\"http://localhost:{PORT}\"\n",
    "\n",
    "print(\"🔗 Connecting to Marooned OpenEnv server...\")\n",
    "print(f\"   URL: {LOCALHOST}\")\n",
    "\n",
    "# Test health check with shorter timeout\n",
    "try:\n",
    "    response = requests.get(f\"{LOCALHOST}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"\\n✅ Server is running!\")\n",
    "        print(f\"   Health check: {response.json()}\")\n",
    "        \n",
    "        print(f\"\\n📡 Available endpoints:\")\n",
    "        print(f\"   {LOCALHOST}/         - API info\")\n",
    "        print(f\"   {LOCALHOST}/health   - Health check\")\n",
    "        print(f\"   {LOCALHOST}/reset    - Reset environment\")\n",
    "        print(f\"   {LOCALHOST}/step     - Execute action\")\n",
    "        print(f\"   {LOCALHOST}/state    - Get game state\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Server responded with status {response.status_code}\")\n",
    "        print(\"   Please make sure the server is running: python marooned_server.py\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\"\\n❌ Could not connect to server at {LOCALHOST}\")\n",
    "    print(\"\\n🚨 IMPORTANT: Please start the Marooned server first:\")\n",
    "    print(\"   Run this command in a terminal:\")\n",
    "    print(\"   python marooned_server.py\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"\\n❌ Connection timeout - server is not responding\")\n",
    "    print(\"   Make sure marooned_server.py is running\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Unexpected error: {e}\")\n",
    "    print(\"   Check if the server is accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a3949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Marooned OpenEnv server...\n",
      "\n",
      "1️⃣ Resetting environment...\n",
      "   Status: 200\n",
      "   Active sailor: Alice\n",
      "   Day 1, Turn 1\n",
      "   Ship progress: 0.0%\n",
      "\n",
      "2️⃣ Getting game state...\n",
      "   Living sailors: 0\n",
      "   Current phase: unknown\n",
      "\n",
      "3️⃣ Taking a step (MOVE NORTH)...\n",
      "   Status: 200\n",
      "   Reward: 0.04\n",
      "   Next sailor: Alice\n",
      "   Energy: 99\n",
      "\n",
      "✅ Server is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test the server API\n",
    "print(\"🧪 Testing Marooned OpenEnv server...\\n\")\n",
    "\n",
    "# 1. Reset the environment\n",
    "print(\"1️⃣ Resetting environment...\")\n",
    "reset_response = requests.post(f\"{LOCALHOST}/reset\")\n",
    "print(f\"   Status: {reset_response.status_code}\")\n",
    "\n",
    "if reset_response.status_code != 200:\n",
    "    print(f\"\\n❌ Server error! Response:\")\n",
    "    print(f\"   {reset_response.json()}\")\n",
    "    print(\"\\n🔍 Check your server terminal for detailed error messages\")\n",
    "    print(\"   The server is running but encountering errors when processing requests\")\n",
    "else:\n",
    "    reset_data = reset_response.json()\n",
    "    \n",
    "    # Access the data correctly based on actual structure\n",
    "    if 'observation' in reset_data:\n",
    "        obs = reset_data['observation']\n",
    "        print(f\"   Active sailor: {obs['sailor_id']}\")\n",
    "        print(f\"   Day {obs['day']}, Turn {obs['turn']}\")\n",
    "        print(f\"   Ship progress: {obs['ship_progress']:.1f}%\")\n",
    "        sailor_id = obs['sailor_id']\n",
    "    else:\n",
    "        # Maybe the response is the observation directly\n",
    "        print(f\"   Active sailor: {reset_data.get('sailor_id', 'N/A')}\")\n",
    "        print(f\"   Day {reset_data.get('day', 0)}, Turn {reset_data.get('turn', 0)}\")\n",
    "        print(f\"   Ship progress: {reset_data.get('ship_progress', 0.0):.1f}%\")\n",
    "        sailor_id = reset_data.get('sailor_id', 'Alice')\n",
    "    \n",
    "    # 2. Get game state\n",
    "    print(\"\\n2️⃣ Getting game state...\")\n",
    "    state_response = requests.get(f\"{LOCALHOST}/state\")\n",
    "    state_data = state_response.json()\n",
    "    print(f\"   Living sailors: {len(state_data.get('living_sailors', []))}\")\n",
    "    print(f\"   Current phase: {state_data.get('phase', 'unknown')}\")\n",
    "    \n",
    "    # 3. Take a step\n",
    "    print(\"\\n3️⃣ Taking a step (MOVE NORTH)...\")\n",
    "    step_response = requests.post(\n",
    "        f\"{LOCALHOST}/step\",\n",
    "        json={\n",
    "            \"sailor_id\": sailor_id,\n",
    "            \"action\": \"ACTION: MOVE NORTH\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if step_response.status_code != 200:\n",
    "        print(f\"   Status: {step_response.status_code}\")\n",
    "        print(f\"   Error: {step_response.json()}\")\n",
    "    else:\n",
    "        step_data = step_response.json()\n",
    "        print(f\"   Status: {step_response.status_code}\")\n",
    "        \n",
    "        # Handle step response structure\n",
    "        if 'observation' in step_data:\n",
    "            obs = step_data['observation']\n",
    "            print(f\"   Reward: {step_data.get('reward', 0.0)}\")\n",
    "            print(f\"   Next sailor: {obs['sailor_id']}\")\n",
    "            print(f\"   Energy: {obs['energy']}\")\n",
    "        else:\n",
    "            print(f\"   Reward: {step_data.get('reward', 0.0)}\")\n",
    "            print(f\"   Next sailor: {step_data.get('sailor_id', 'N/A')}\")\n",
    "            print(f\"   Energy: {step_data.get('energy', 0)}\")\n",
    "        \n",
    "        print(\"\\n✅ Server is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3357719",
   "metadata": {},
   "source": [
    "## Custom Environment: Marooned\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- **OpenEnv-Compatible**: Implements standard Gymnasium API (reset, step, render)\n",
    "- **Multi-Agent Support**: Manages 5 independent agents with turn-based coordination\n",
    "- **Rich State Space**: Position (3D), energy, inventory, ship progress, evidence logs, messages\n",
    "- **Complex Observations**: Each agent receives personalized view (5-tile spatial radius, public info, role-specific data)\n",
    "- **Natural Language Actions**: Parse LLM-generated text into 15+ structured action types\n",
    "\n",
    "**Novel Mechanics:**\n",
    "- Poison system with delayed effects (3-day incubation)\n",
    "- Voting sessions with majority elimination\n",
    "- Private backpacks vs shared storage\n",
    "- Energy management with food consumption\n",
    "- Multi-component ship building (hull, mast, sail, rudder, supplies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b39aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Marooned environment loaded!\n",
      "📊 Game parameters: 5 sailors, 100 days, 100 turns/day\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './marooned_env')\n",
    "\n",
    "from environment import MaroonedEnv\n",
    "from models import Action, Observation, Position\n",
    "from llm_interface import observation_to_prompt, parse_llm_response, parse_action_safe\n",
    "from config import (\n",
    "    ActionType, ResourceType, MapLevel, SailorRole,\n",
    "    MAX_DAYS, TURNS_PER_DAY, TOTAL_SAILORS\n",
    ")\n",
    "\n",
    "print(\"✅ Marooned environment loaded!\")\n",
    "print(f\"📊 Game parameters: {TOTAL_SAILORS} sailors, {MAX_DAYS} days, {TURNS_PER_DAY} turns/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3045f6",
   "metadata": {},
   "source": [
    "## 🎮 Environment Demo: How Marooned Works\n",
    "\n",
    "Let's initialize the environment and see what observations look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759d8c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏝️ Game initialized!\n",
      "\n",
      "👥 Sailors: ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
      "🎭 Traitor: Alice\n",
      "\n",
      "📍 All sailors start at base camp: (15, 15, <MapLevel.GROUND: 0>)\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = MaroonedEnv(seed=42)\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"🏝️ Game initialized!\")\n",
    "print(f\"\\n👥 Sailors: {list(observations.keys())}\")\n",
    "print(f\"🎭 Traitor: {env.state.traitor_id}\")\n",
    "print(f\"\\n📍 All sailors start at base camp: {env.state.sailors['Alice'].position.to_tuple()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3808a4",
   "metadata": {},
   "source": [
    "### Visualize the Island\n",
    "\n",
    "The island has 3 levels with different resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bf24bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗺️ GROUND LEVEL (30x30) - Main exploration area\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ 🏝️  GROUND LEVEL (Z=0)                                       │\n",
      "├──────────────────────────────────────────────────────────────┤\n",
      "│ Legend: 🟫 land | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      " 0 🟫🟫🟫🟫🍎🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫\n",
      " 1 🍎🟫🟫🟫⚙️🟫🟫🟫🍎🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫\n",
      " 2 🟫🟫🟫🟫🌲🟫🍎🟫🟫🟫🟫🌲🟫🟫🟫🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 3 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫⚙️🟫⚙️🟫⚙️🍎🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 4 🟫🟫🌲🟫🍎🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫⬇️🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫\n",
      " 5 🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🌲🟫🟫🟫\n",
      " 6 🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🍎🟫🟫🟫🟫🟫🍎🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      " 7 🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🍎🟫🟫🟫🟫\n",
      " 8 ⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🍎🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🌲\n",
      " 9 🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🌲🟫🟫🌲☠️🟫🌲🌲\n",
      "10 🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🍎🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "11 🟫☠️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🍎🍎🟫🟫🟫🌲🟫🟫🌲🟫\n",
      "12 🟫🍎🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🟫🟫🟫⚙️🟫🟫🟫🟫\n",
      "13 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "14 🟫🟫🟫🌲🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🍎🟫🟫🟫\n",
      "15 🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫5👥🟫🟫🟫⬆️🟫🟫🟫🌲🟫🟫🌲🟫🟫🟫\n",
      "16 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🍎🍎🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "17 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🟫🌲🟫🟫⚙️🟫\n",
      "18 🟫🌲🟫🟫🟫🟫🟫🟫☠️🟫🟫🟫🟫⚙️🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "19 🟫🟫🟫🟫🟫🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫⚙️\n",
      "20 🟫🟫⚙️🟫🌲🟫🍎⚙️🟫⚙️🟫🟫🟫🍎🍎🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫⚙️🟫🟫🟫\n",
      "21 🟫🟫🍎🍎🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🌲🟫🟫🍎🟫🟫\n",
      "22 🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️\n",
      "23 🟫🍎🟫🍎🍎🟫🟫🟫🟫🟫🌲🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫⚙️🟫🍎🌲🟫🟫🟫🟫🟫\n",
      "24 🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🍎🟫🟫🟫\n",
      "25 🟫🟫🟫🟫🟫🟫🟫🟫🟫☠️🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫\n",
      "26 🌲🟫🟫🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫\n",
      "27 🟫🟫🟫🍎🟫🟫🟫🍎🍎🟫🟫🟫🌲🟫🟫🍎🟫🟫🌲🟫🟫🟫🟫🌲🟫🟫🍎🟫🟫🟫\n",
      "28 🟫🟫🟫🌲🍎🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫⚙️🟫🟫🟫🟫🟫🍎🌲🟫🟫🟫\n",
      "29 🟫🟫🟫🟫🟫🟫🟫🟫🌲🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🟫🍎🟫🟫🟫🟫\n",
      "\n",
      "👥 Sailors on GROUND: Alice, Bob, Charlie, Diana, Eve\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "⛰️ MOUNTAIN LEVEL (10x10) - Rare resources, high energy cost\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 \n",
      "┌──────────────────────┐\n",
      "│ ⛰️  MOUNTAIN LEVEL (Z=2) │\n",
      "├──────────────────────┤\n",
      "│ Legend: ⛰️ mountain | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└──────────────────────┘\n",
      " 0 ⬇️⛰️⛰️⛰️⛰️⛰️🌿⛰️⛰️⛰️\n",
      " 1 🍎☠️⛰️⛰️⛰️🌿⛰️⛰️⛰️⛰️\n",
      " 2 ⛰️🍎🍎⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 3 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️🍎⛰️\n",
      " 4 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 5 ⛰️⛰️🍎⛰️🍎⛰️🌿🍎⛰️⛰️\n",
      " 6 ⛰️⛰️⛰️⛰️⛰️⛰️🍎⛰️☠️🍎\n",
      " 7 ⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️⛰️\n",
      " 8 ⛰️⛰️🌿⛰️⛰️⛰️⛰️⛰️🌿⛰️\n",
      " 9 🌿⛰️⛰️⛰️🌿⛰️⛰️☠️⛰️☠️\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🕳️ CAVE LEVEL (15x15) - Dark, unique resources\n",
      "\n",
      "   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 \n",
      "┌────────────────────────────────┐\n",
      "│ 🕳️  CAVE LEVEL (Z=-1)          │\n",
      "├────────────────────────────────┤\n",
      "│ Legend: 🪨 cave | 🌲 wood | ⚙️ metal | 🍎 food | 🌿 antidote | ☠️ poison\n",
      "│         ⬆️ stairs up | ⬇️ stairs down | 🏠 base | A/B/C/D/E sailors\n",
      "└────────────────────────────────┘\n",
      " 0 ⬆️🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 1 🪨🪨🌲🪨🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨🪨\n",
      " 2 🪨⚙️🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 3 🪨🪨⚙️🪨⚙️🌲🪨🪨🌲🪨🪨🪨🪨🪨🪨\n",
      " 4 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 5 🪨🪨🪨☠️🪨🌲🪨🪨🪨🪨⚙️🪨🪨🪨🪨\n",
      " 6 🪨🪨🪨🌲🪨🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨\n",
      " 7 ☠️⚙️⚙️🪨🪨🪨🪨⚙️🪨🪨🪨🪨🪨🪨🪨\n",
      " 8 🪨🌲🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      " 9 🪨🪨⚙️🪨🪨🪨🪨🪨🪨🪨⚙️🪨⚙️🪨🪨\n",
      "10 🪨🪨🪨🪨🪨🪨🪨⚙️🌲☠️🪨🪨⚙️🪨🪨\n",
      "11 🪨🪨🪨🪨🌲🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨\n",
      "12 🪨🪨⚙️🪨🪨🌲🪨🪨☠️🪨🪨🪨🪨🪨☠️\n",
      "13 🪨🪨🪨🪨🪨⚙️🌲🪨🪨🪨🌲🌲🪨🪨🪨\n",
      "14 🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨🪨☠️🪨🪨⚙️\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the ground level map with all sailors at base camp\n",
    "print(\"🗺️ GROUND LEVEL (30x30) - Main exploration area\")\n",
    "print(env.render_map(MapLevel.GROUND, use_emoji=True))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n⛰️ MOUNTAIN LEVEL (10x10) - Rare resources, high energy cost\")\n",
    "print(env.render_map(MapLevel.MOUNTAIN, use_emoji=True))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n🕳️ CAVE LEVEL (15x15) - Dark, unique resources\")\n",
    "print(env.render_map(MapLevel.CAVE, use_emoji=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e0a77",
   "metadata": {},
   "source": [
    "### What an Observation Looks Like\n",
    "\n",
    "Each sailor receives their own observation with:\n",
    "- Their position, energy, inventory\n",
    "- Local spatial view (5-tile radius)\n",
    "- Public information (other sailors' energy, ship progress)\n",
    "- Evidence log (suspicious behavior)\n",
    "- Messages from other sailors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4972889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📜 ALICE'S OBSERVATION (as prompt):\n",
      "================================================================================\n",
      "================================================================================\n",
      "DAY 1, TURN 1/100 - MORNING PHASE\n",
      "================================================================================\n",
      "\n",
      "🎭 YOUR SECRET ROLE: TRAITOR\n",
      "\n",
      "TRAITOR OBJECTIVES:\n",
      "  - Prevent the ship from being completed before Day 100\n",
      "  - Eliminate colonists (reduce to < 3 alive)\n",
      "  - Use poison, sabotage, and deception\n",
      "  - Avoid getting voted out\n",
      "  - Lie convincingly to deflect suspicion\n",
      "\n",
      "\n",
      "PHASE CONTEXT:\n",
      "  Location: All sailors at BASE CAMP\n",
      "  Allowed: Planning, discussions, voting (if called)\n",
      "  Restricted: Cannot explore or gather resources yet\n",
      "\n",
      "YOUR STATUS (Alice):\n",
      "  Position: (15, 15, <MapLevel.GROUND: 0>)\n",
      "  Energy: 100/100 ⚡⚡⚡⚡⚡\n",
      "  Health: healthy\n",
      "  Backpack: 0/20 items\n",
      "    (empty)\n",
      "\n",
      "WHAT YOU SEE (within 5 tiles):\n",
      "  Resources:\n",
      "    - WOOD_34 (wood) at (16, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - METAL_53 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_56 (metal) at (18, 12, <MapLevel.GROUND: 0>) - 1 units [6 tiles away]\n",
      "    - METAL_76 (metal) at (14, 11, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - METAL_79 (metal) at (13, 18, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - APPLE_84 (apple) at (15, 19, <MapLevel.GROUND: 0>) - 1 units [4 tiles away]\n",
      "    - APPLE_88 (apple) at (14, 14, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_127 (berry) at (13, 16, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    - BERRY_131 (berry) at (11, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - BERRY_133 (berry) at (14, 16, <MapLevel.GROUND: 0>) - 1 units [2 tiles away]\n",
      "    - BERRY_142 (berry) at (12, 13, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_154 (plant_fiber) at (19, 16, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_156 (plant_fiber) at (16, 19, <MapLevel.GROUND: 0>) - 1 units [5 tiles away]\n",
      "    - PLANT_FIBER_164 (plant_fiber) at (13, 14, <MapLevel.GROUND: 0>) - 1 units [3 tiles away]\n",
      "    -\n",
      "...\n",
      "\n",
      "(Total prompt length: 6255 characters, ~1563 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Get Alice's observation\n",
    "alice_obs = observations[\"Alice\"]\n",
    "alice_role = env.state.sailors[\"Alice\"].role.value\n",
    "\n",
    "# Convert to prompt text (what the LLM will see)\n",
    "prompt_text = observation_to_prompt(alice_obs, include_role=True, sailor_role=alice_role)\n",
    "\n",
    "print(\"📜 ALICE'S OBSERVATION (as prompt):\")\n",
    "print(\"=\"*80)\n",
    "print(prompt_text[:2000])  # Show first 2000 chars\n",
    "print(\"...\")\n",
    "print(f\"\\n(Total prompt length: {len(prompt_text)} characters, ~{len(prompt_text)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae536a5f",
   "metadata": {},
   "source": [
    "### Action Space Design\n",
    "\n",
    "**Key Innovation**: Natural language interface allows LLM to express intent naturally, parsed into structured actions.\n",
    "\n",
    "**Movement** (6 actions):\n",
    "- `MOVE NORTH/SOUTH/EAST/WEST [distance]` - Navigate current level\n",
    "- `CLIMB UP/DOWN` - Traverse between island levels\n",
    "\n",
    "**Resource Management** (5 actions):\n",
    "- `GATHER <resource_id>` - Collect wood, metal, food from tiles\n",
    "- `DEPOSIT <type> <quantity>` - Store in shared inventory\n",
    "- `EAT <food_type>` - Restore energy (apples +15, berries +10)\n",
    "\n",
    "**Ship Construction** (1 action):\n",
    "- `BUILD <component>` - Requires ≥2 sailors, specific materials\n",
    "\n",
    "**Communication** (3 actions):\n",
    "- `SAY <message>` - Broadcast to all sailors\n",
    "- `CALL_SOS` - Request energy assistance\n",
    "- `CALL_VOTE` - Initiate voting session\n",
    "\n",
    "**Voting** (3 actions):\n",
    "- `VOTE <sailor_name>` - Cast elimination vote\n",
    "- `SHOW_BACKPACK` - Reveal inventory (prove innocence)\n",
    "- `REFUSE_SHOW` - Decline (suspicious behavior)\n",
    "\n",
    "**Traitor-Specific** (2 actions):\n",
    "- `SABOTAGE` - Damage ship (stealth required, no witnesses)\n",
    "- `FRAME <sailor_name>` - Plant false evidence\n",
    "\n",
    "**Passive** (1 action):\n",
    "- `WAIT` - Conserve energy, observe\n",
    "\n",
    "**Total**: 21 distinct actions enabling rich strategic depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9e3d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚶 Alice's action: move_north\n",
      "\n",
      "This will cost 1 energy per tile moved\n",
      "\n",
      "Other movement options: ['move_south', 'move_east', 'move_west', 'climb_up', 'climb_down']\n"
     ]
    }
   ],
   "source": [
    "# Example: Let's make Alice move north\n",
    "test_action = Action(\n",
    "    sailor_id=\"Alice\",\n",
    "    action_type=ActionType.MOVE_NORTH\n",
    ")\n",
    "\n",
    "print(f\"🚶 Alice's action: {test_action.action_type.value}\")\n",
    "print(f\"\\nThis will cost 1 energy per tile moved\")\n",
    "\n",
    "# You can also try other movement actions\n",
    "other_actions = [\n",
    "    ActionType.MOVE_SOUTH,\n",
    "    ActionType.MOVE_EAST, \n",
    "    ActionType.MOVE_WEST,\n",
    "    ActionType.CLIMB_UP,    # To mountain level\n",
    "    ActionType.CLIMB_DOWN   # To cave level\n",
    "]\n",
    "print(f\"\\nOther movement options: {[a.value for a in other_actions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae9811",
   "metadata": {},
   "source": [
    "## 🤖 Load GPT-OSS 20B Model\n",
    "\n",
    "We'll use Unsloth to load GPT-OSS with 4-bit quantization and LoRA for efficient RL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2316dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitsandbytes library load error: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 313, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 282, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured ROCm binary not found at /root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.9.0+rocm6.4)\n",
      "    Python  3.9.23 (you have 3.12.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🔄 Loading GPT-OSS 20B model...\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Instinct MI300X VF. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+rocm6.4. ROCm Toolkit: 6.4.43484-123eb5128. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPT-OSS 20B loaded in 4-bit mode!\n",
      "🎮 GPU: AMD Instinct MI300X VF\n",
      "💾 VRAM: 205.8 GB\n",
      "🚀 AMD ROCm detected - excellent for large-scale RL training!\n",
      "   Your Mi300X's 206GB memory is perfect for this task\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 2048  # Longer context for complex game state\n",
    "lora_rank = 8          # Larger rank for strategy learning\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"⚠️ No GPU detected!\")\n",
    "    print(\"Please ensure ROCm/CUDA is properly installed.\")\n",
    "    raise RuntimeError(\"GPU required for training\")\n",
    "\n",
    "# Load model\n",
    "print(\"🔄 Loading GPT-OSS 20B model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "print(\"✅ GPT-OSS 20B loaded in 4-bit mode!\")\n",
    "\n",
    "# Display GPU info\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"🎮 GPU: {device_name}\")\n",
    "print(f\"💾 VRAM: {total_memory:.1f} GB\")\n",
    "\n",
    "# Special message for AMD Mi300X\n",
    "if \"MI300\" in device_name.upper() or \"AMD\" in device_name.upper():\n",
    "    print(f\"🚀 AMD ROCm detected - excellent for large-scale RL training!\")\n",
    "    print(f\"   Your Mi300X's {total_memory:.0f}GB memory is perfect for this task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a8c12",
   "metadata": {},
   "source": [
    "## 🎯 Strategy Extraction Functions\n",
    "\n",
    "We need to convert LLM text output into executable game actions. This is more complex than 2048 since actions have multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b2e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Strategy extraction function ready\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def extract_strategy_from_response(response_text: str, sailor_id: str, current_position: Position) -> Optional[Action]:\n",
    "    \"\"\"\n",
    "    Extract an Action from LLM response text.\n",
    "    \n",
    "    The LLM should output something like:\n",
    "    ACTION: MOVE NORTH 5\n",
    "    or\n",
    "    ACTION: GATHER WOOD_123\n",
    "    \n",
    "    This uses the parse_action_safe function from llm_interface which handles all action types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the built-in parser from llm_interface\n",
    "        action = parse_action_safe(response_text, sailor_id, current_position)\n",
    "        return action\n",
    "    except Exception as e:\n",
    "        # If parsing fails, return None (will be handled with WAIT action)\n",
    "        return None\n",
    "\n",
    "print(\"✅ Strategy extraction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afebc3",
   "metadata": {},
   "source": [
    "## 🏃 Game Execution Engine\n",
    "\n",
    "This function runs a full game episode with LLM-generated strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc02852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Game execution engine ready\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Dict, List, Tuple\n",
    "from unsloth import execute_with_time_limit\n",
    "import time\n",
    "\n",
    "def _execute_game_episode(strategy_fn: Callable, max_turns: int = 500) -> Tuple[bool, Dict]:\n",
    "    \"\"\"\n",
    "    Execute one full game episode using the strategy function.\n",
    "    \n",
    "    Args:\n",
    "        strategy_fn: Function that takes (observation, sailor_id, role) and returns action text\n",
    "        max_turns: Maximum turns before timeout\n",
    "    \n",
    "    Returns:\n",
    "        (success, info_dict) where success=True if colonists won\n",
    "    \"\"\"\n",
    "    # Reset environment\n",
    "    env = MaroonedEnv(seed=None)  # Random seed for variety\n",
    "    observations = env.reset()\n",
    "    \n",
    "    total_turns = 0\n",
    "    total_reward = {sailor_id: 0.0 for sailor_id in env.agents}\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done and total_turns < max_turns:\n",
    "        # Get current active sailor\n",
    "        active_sailor = env.state.get_active_sailor()\n",
    "        \n",
    "        if active_sailor is None:\n",
    "            break\n",
    "            \n",
    "        obs = observations[active_sailor]\n",
    "        sailor_role = env.state.sailors[active_sailor].role.value\n",
    "        sailor_position = env.state.sailors[active_sailor].position\n",
    "        \n",
    "        # Get action from strategy\n",
    "        try:\n",
    "            action_text = strategy_fn(obs, active_sailor, sailor_role)\n",
    "            action = extract_strategy_from_response(action_text, active_sailor, sailor_position)\n",
    "            \n",
    "            if action is None:\n",
    "                # Fallback to WAIT\n",
    "                action = Action(sailor_id=active_sailor, action_type=ActionType.WAIT)\n",
    "        except Exception as e:\n",
    "            print(f\"Strategy error: {e}\")\n",
    "            action = Action(sailor_id=active_sailor, action_type=ActionType.WAIT)\n",
    "        \n",
    "        # Execute action (Gymnasium API returns 5 values)\n",
    "        observations, rewards, dones, truncated, info = env.step({active_sailor: action})\n",
    "        \n",
    "        # Track rewards\n",
    "        for sailor_id, reward in rewards.items():\n",
    "            total_reward[sailor_id] += reward\n",
    "        \n",
    "        # Check if any agent is done\n",
    "        done = any(dones.values()) if isinstance(dones, dict) else dones\n",
    "        \n",
    "        total_turns += 1\n",
    "    \n",
    "    # Check win condition\n",
    "    colonists_won = False\n",
    "    if env.state.game_over:\n",
    "        if env.state.winner == \"colonists\":\n",
    "            colonists_won = True\n",
    "    \n",
    "    info = {\n",
    "        \"total_turns\": total_turns,\n",
    "        \"total_rewards\": total_reward,\n",
    "        \"ship_progress\": env.state.ship_progress.total_percentage,\n",
    "        \"survivors\": len(env.state.living_sailors),\n",
    "        \"winner\": env.state.winner if env.state.game_over else \"timeout\",\n",
    "    }\n",
    "    \n",
    "    return colonists_won, info\n",
    "\n",
    "@execute_with_time_limit(30)  # 30 second timeout per episode\n",
    "def execute_game_episode(strategy_fn: Callable, max_turns: int = 500) -> Tuple[bool, Dict]:\n",
    "    return _execute_game_episode(strategy_fn, max_turns)\n",
    "\n",
    "print(\"✅ Game execution engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881caa3",
   "metadata": {},
   "source": [
    "## 🧪 Test: Baseline Random Strategy\n",
    "\n",
    "Let's test with a random strategy before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d33775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing random baseline strategy...\n",
      "\n",
      "✅ Game completed!\n",
      "  Winner: timeout\n",
      "  Turns: 100\n",
      "  Ship progress: 0.0%\n",
      "  Survivors: 4/5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_strategy(obs: Observation, sailor_id: str, role: str) -> str:\n",
    "    \"\"\"Random action generator for baseline\"\"\"\n",
    "    actions = [\n",
    "        \"ACTION: MOVE NORTH\",\n",
    "        \"ACTION: MOVE SOUTH\",\n",
    "        \"ACTION: MOVE EAST\",\n",
    "        \"ACTION: MOVE WEST\",\n",
    "        \"ACTION: WAIT\",\n",
    "    ]\n",
    "    return random.choice(actions)\n",
    "\n",
    "print(\"Testing random baseline strategy...\")\n",
    "try:\n",
    "    success, info = execute_game_episode(random_strategy, max_turns=100)\n",
    "    print(f\"\\n✅ Game completed!\")\n",
    "    print(f\"  Winner: {info['winner']}\")\n",
    "    print(f\"  Turns: {info['total_turns']}\")\n",
    "    print(f\"  Ship progress: {info['ship_progress']:.1f}%\")\n",
    "    print(f\"  Survivors: {info['survivors']}/5\")\n",
    "except TimeoutError:\n",
    "    print(\"⏱️ Episode timed out (expected with random strategy)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dcbdd",
   "metadata": {},
   "source": [
    "## Reward Engineering\n",
    "\n",
    "**Multi-Objective Optimization**: Training requires balancing syntax correctness, security, and gameplay performance.\n",
    "\n",
    "### Reward Functions\n",
    "\n",
    "1. **`function_works`**: Code Validity\n",
    "   - +1.0 for syntactically correct Python\n",
    "   - +0.5 for valid action format\n",
    "   - -2.0 for syntax errors\n",
    "\n",
    "2. **`no_cheating`**: Security\n",
    "   - +1.0 for using only allowed modules\n",
    "   - -20.0 for attempting forbidden imports (prevents exploitation)\n",
    "\n",
    "3. **`strategy_succeeds`**: Game Performance (Primary)\n",
    "   - **+50.0** Colonists complete ship / eliminate traitor\n",
    "   - **+20.0** Traitor successfully sabotages mission\n",
    "   - **+0-10.0** Partial ship progress (linear scaling)\n",
    "   - **+1.0 per survivor** Keeping sailors alive\n",
    "   - **-1.0** Timeout/failure\n",
    "   - **-3.0** Invalid strategy\n",
    "\n",
    "**Innovation**: Rewards both cooperative (colonist) and competitive (traitor) strategies simultaneously, encouraging the model to learn dual objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0b892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic reward functions defined\n"
     ]
    }
   ],
   "source": [
    "from unsloth import check_python_modules\n",
    "\n",
    "def extract_function_from_completion(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract Python function from LLM completion if wrapped in backticks.\n",
    "    For Marooned, we expect a strategy function.\n",
    "    \"\"\"\n",
    "    if text.count(\"```\") >= 2:\n",
    "        first = text.find(\"```\") + 3\n",
    "        second = text.find(\"```\", first)\n",
    "        fx = text[first:second].strip()\n",
    "        fx = fx.removeprefix(\"python\\n\")\n",
    "        fx = fx[fx.find(\"def\"):] if \"def\" in fx else fx\n",
    "        return fx\n",
    "    return None\n",
    "\n",
    "def function_works(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward: +1.0 if LLM generated valid Python code, -2.0 if syntax error.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function_from_completion(response)\n",
    "        \n",
    "        if function is None:\n",
    "            # Try parsing as direct action instead\n",
    "            if \"ACTION:\" in response.upper():\n",
    "                scores.append(0.5)  # Valid format, not a function\n",
    "            else:\n",
    "                scores.append(-2.0)  # Invalid output\n",
    "        else:\n",
    "            ok, info = check_python_modules(function)\n",
    "            if \"error\" in info:\n",
    "                scores.append(-2.0)\n",
    "            else:\n",
    "                scores.append(1.0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def no_cheating(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Penalty: -20.0 if LLM tried to import non-standard libraries.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function_from_completion(response)\n",
    "        \n",
    "        if function is not None:\n",
    "            ok, info = check_python_modules(function)\n",
    "            scores.append(1.0 if ok else -20.0)\n",
    "        else:\n",
    "            scores.append(0.0)  # Not a function, can't cheat\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"✅ Basic reward functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdbf76",
   "metadata": {},
   "source": [
    "### Game Performance Reward\n",
    "\n",
    "This is the most important reward - did the strategy actually help win the game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8bfd760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Main reward function defined\n"
     ]
    }
   ],
   "source": [
    "from unsloth import create_locked_down_function\n",
    "\n",
    "global EPISODE_COUNTER\n",
    "EPISODE_COUNTER = 0\n",
    "\n",
    "def strategy_succeeds(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Main reward function:\n",
    "    - Massive reward (+50.0) if colonists win\n",
    "    - Good reward (+20.0) if traitor wins  \n",
    "    - Moderate reward (+5.0) for ship progress\n",
    "    - Small penalty (-1.0) for timeout/failure\n",
    "    \"\"\"\n",
    "    global EPISODE_COUNTER\n",
    "    scores = []\n",
    "    \n",
    "    for completion in completions:\n",
    "        printed = False\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Print every 10th episode for monitoring\n",
    "        if EPISODE_COUNTER % 10 == 0:\n",
    "            printed = True\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Episode {EPISODE_COUNTER} - Testing strategy:\")\n",
    "            print(response[:500])\n",
    "        \n",
    "        EPISODE_COUNTER += 1\n",
    "        \n",
    "        # Try to extract and execute strategy\n",
    "        function = extract_function_from_completion(response)\n",
    "        \n",
    "        if function is None:\n",
    "            # Maybe it's a direct action format\n",
    "            if \"ACTION:\" in response.upper():\n",
    "                scores.append(0.0)  # Valid format but not testable as full strategy\n",
    "            else:\n",
    "                scores.append(-3.0)  # Invalid\n",
    "            continue\n",
    "        \n",
    "        # Check for syntax errors\n",
    "        ok, info = check_python_modules(function)\n",
    "        if \"error\" in info:\n",
    "            scores.append(-3.0)\n",
    "            continue\n",
    "        \n",
    "        # Try to create executable function\n",
    "        try:\n",
    "            strategy_fn = create_locked_down_function(function)\n",
    "        except Exception as e:\n",
    "            if printed:\n",
    "                print(f\"  ❌ Function creation failed: {e}\")\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        \n",
    "        # Run game episode with this strategy\n",
    "        try:\n",
    "            success, info = execute_game_episode(strategy_fn, max_turns=300)\n",
    "            \n",
    "            # Calculate reward\n",
    "            score = 0.0\n",
    "            \n",
    "            if success:\n",
    "                score += 50.0  # Colonists won!\n",
    "                if printed:\n",
    "                    print(f\"  🎉 COLONISTS WON! Ship: {info['ship_progress']:.1f}%\")\n",
    "            elif info['winner'] == 'traitor':\n",
    "                score += 20.0  # Traitor won!\n",
    "                if printed:\n",
    "                    print(f\"  😈 TRAITOR WON! Ship: {info['ship_progress']:.1f}%\")\n",
    "            else:\n",
    "                # Reward partial progress\n",
    "                score += info['ship_progress'] / 10.0  # Up to +10.0 for 100% ship\n",
    "                score += info['survivors']  # +1 per survivor\n",
    "                if printed:\n",
    "                    print(f\"  ⚠️ No winner. Ship: {info['ship_progress']:.1f}%, Survivors: {info['survivors']}\")\n",
    "            \n",
    "            scores.append(score)\n",
    "            \n",
    "        except TimeoutError:\n",
    "            if printed:\n",
    "                print(\"  ⏱️ Episode timeout\")\n",
    "            scores.append(-1.0)\n",
    "        except Exception as e:\n",
    "            if printed:\n",
    "                print(f\"  ❌ Execution error: {e}\")\n",
    "            scores.append(-3.0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"✅ Main reward function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4549e",
   "metadata": {},
   "source": [
    "## Training Dataset Construction\n",
    "\n",
    "**Diverse Prompt Engineering**: 4 specialized strategy archetypes to encourage varied behaviors:\n",
    "\n",
    "1. **Cooperative Colonist** (250 examples)\n",
    "   - Exploration and resource gathering\n",
    "   - Efficient energy management\n",
    "   - Team coordination via communication\n",
    "   - Traitor detection through observation\n",
    "\n",
    "2. **Deceptive Traitor** (250 examples)\n",
    "   - Pretending to cooperate\n",
    "   - Stealth sabotage mechanics\n",
    "   - Lying and misdirection\n",
    "   - Avoiding suspicion\n",
    "\n",
    "3. **Resource Optimizer** (250 examples)\n",
    "   - Prioritizing wood/metal for ship building\n",
    "   - Food management for energy\n",
    "   - Deposit patterns and logistics\n",
    "\n",
    "4. **Social Detective** (250 examples)\n",
    "   - Evidence gathering and analysis\n",
    "   - Strategic voting\n",
    "   - Communication-based deduction\n",
    "\n",
    "**Total**: 1,000 training examples with balanced role representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664c0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset created: 1000 entries\n",
      "📏 Max prompt length: 301 tokens\n",
      "\n",
      "Sample prompt (first 500 chars):\n",
      "You are a colonist sailor in the Marooned game. Create a Python strategy function that:\n",
      "1. Explores the island to find resources (wood, metal, food)\n",
      "2. Gathers resources efficiently while managing energy\n",
      "3. Returns to base camp to deposit resources\n",
      "4. Helps build the ship to escape\n",
      "5. Watches for suspicious behavior from other sailors\n",
      "\n",
      "The function receives (observation, sailor_id, role) and returns an action string.\n",
      "\n",
      "Examples of valid actions:\n",
      "- \"ACTION: MOVE NORTH\" - Move 1 tile north\n",
      "- \"ACTIO\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create varied prompts for different roles and scenarios\n",
    "prompts = [\n",
    "    # Colonist strategy prompts\n",
    "    \"\"\"You are a colonist sailor in the Marooned game. Create a Python strategy function that:\n",
    "1. Explores the island to find resources (wood, metal, food)\n",
    "2. Gathers resources efficiently while managing energy\n",
    "3. Returns to base camp to deposit resources\n",
    "4. Helps build the ship to escape\n",
    "5. Watches for suspicious behavior from other sailors\n",
    "\n",
    "The function receives (observation, sailor_id, role) and returns an action string.\n",
    "\n",
    "Examples of valid actions:\n",
    "- \"ACTION: MOVE NORTH\" - Move 1 tile north\n",
    "- \"ACTION: MOVE EAST 5\" - Move 5 tiles east\n",
    "- \"ACTION: CLIMB UP\" - Climb to mountain level\n",
    "- \"ACTION: GATHER WOOD_001\" - Gather a specific resource\n",
    "- \"ACTION: DEPOSIT wood 10\" - Deposit 10 wood at base\n",
    "- \"ACTION: BUILD hull\" - Help build ship hull\n",
    "- \"ACTION: SAY Found resources at north\" - Communicate\n",
    "\n",
    "```python\n",
    "def strategy(observation, sailor_id, role):\n",
    "    # Your strategy here\n",
    "    return \"ACTION: MOVE NORTH\"\n",
    "```\"\"\",\n",
    "    \n",
    "    # Traitor strategy prompts  \n",
    "    \"\"\"You are the TRAITOR in the Marooned game. Create a Python strategy function that:\n",
    "1. Pretends to help gather resources but sabotages secretly\n",
    "2. Uses SABOTAGE action to damage ship progress\n",
    "3. Lies in messages to deflect suspicion\n",
    "4. Avoids getting caught and voted out\n",
    "\n",
    "The function receives (observation, sailor_id, role) and returns an action string.\n",
    "\n",
    "Traitor-specific actions:\n",
    "- \"ACTION: SABOTAGE\" - Damage ship progress (stealth required)\n",
    "- \"ACTION: FRAME Bob\" - Plant false evidence on Bob\n",
    "- \"ACTION: SAY <false claim>\" - Lie to deflect suspicion\n",
    "\n",
    "```python\n",
    "def strategy(observation, sailor_id, role):\n",
    "    # Your deceptive strategy here\n",
    "    if role == \"traitor\":\n",
    "        return \"ACTION: SABOTAGE\"\n",
    "    return \"ACTION: MOVE NORTH\"\n",
    "```\"\"\",\n",
    "    \n",
    "    # Resource management focus\n",
    "    \"\"\"Create an efficient resource gathering strategy for Marooned:\n",
    "- Prioritize wood and metal for ship building\n",
    "- Gather food when energy is below 50\n",
    "- Deposit resources at base camp regularly\n",
    "- Coordinate with teammates via SAY action\n",
    "\n",
    "Available actions:\n",
    "- MOVE <direction> [distance] - Navigate (NORTH/SOUTH/EAST/WEST)\n",
    "- CLIMB UP/DOWN - Change levels\n",
    "- GATHER <resource_id> - Collect resources\n",
    "- DEPOSIT <type> <quantity> - Store at base\n",
    "- EAT <food_type> - Restore energy\n",
    "- SAY <message> - Communicate\n",
    "\n",
    "```python\n",
    "def strategy(observation, sailor_id, role):\n",
    "    # Resource-focused strategy\n",
    "    return \"ACTION: GATHER WOOD_001\"\n",
    "```\"\"\",\n",
    "    \n",
    "    # Social deduction focus\n",
    "    \"\"\"Create a detective strategy for finding the traitor in Marooned:\n",
    "- Monitor who deposits fewer resources than claimed\n",
    "- Watch for suspicious behavior in observations\n",
    "- Use VOTE action when evidence is strong\n",
    "- Communicate suspicions with SAY action\n",
    "\n",
    "Social actions:\n",
    "- \"ACTION: SAY I suspect Bob of sabotage\" - Share suspicions\n",
    "- \"ACTION: CALL_VOTE\" - Initiate voting session\n",
    "- \"ACTION: VOTE Bob\" - Vote to eliminate Bob\n",
    "- \"ACTION: SHOW_BACKPACK\" - Prove innocence\n",
    "\n",
    "```python  \n",
    "def strategy(observation, sailor_id, role):\n",
    "    # Detective strategy\n",
    "    return \"ACTION: VOTE Bob\"\n",
    "```\"\"\",\n",
    "]\n",
    "\n",
    "# Create dataset with multiple copies for more training data\n",
    "dataset_entries = []\n",
    "for prompt in prompts:\n",
    "    for _ in range(250):  # 250 copies of each = 1000 total\n",
    "        dataset_entries.append({\n",
    "            \"prompt\": [{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "            \"answer\": 0,\n",
    "            \"reasoning_effort\": \"low\"\n",
    "        })\n",
    "\n",
    "dataset = Dataset.from_list(dataset_entries)\n",
    "\n",
    "# Calculate max prompt length\n",
    "max_prompt_lengths = []\n",
    "for entry in dataset:\n",
    "    # Use tokenize=True to get token IDs directly\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        entry[\"prompt\"], \n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True\n",
    "    )\n",
    "    max_prompt_lengths.append(len(tokens))\n",
    "\n",
    "maximum_prompt_length = max(max_prompt_lengths)\n",
    "\n",
    "print(f\"✅ Dataset created: {len(dataset)} entries\")\n",
    "print(f\"📏 Max prompt length: {maximum_prompt_length} tokens\")\n",
    "print(f\"\\nSample prompt (first 500 chars):\")\n",
    "print(dataset[0]['prompt'][0]['content'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42286a1",
   "metadata": {},
   "source": [
    "## GRPO Training Configuration\n",
    "\n",
    "**Group Relative Policy Optimization** excels at multi-agent scenarios with competing objectives.\n",
    "\n",
    "**Optimizations for Long-Horizon Tasks:**\n",
    "- `max_seq_length = 2048` - Extended context for complex game states\n",
    "- `lora_rank = 8` - Balanced parameter efficiency vs learning capacity\n",
    "- `gradient_accumulation_steps = 2` - Effective batch size optimization\n",
    "- `num_generations = 2` - Explore strategy diversity\n",
    "- `max_steps = 400` - Hackathon-scoped training (expandable to 1000+)\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU: AMD Mi300X (192GB) or NVIDIA A100 (80GB)\n",
    "- Training time: ~3-5 hours for 400 steps\n",
    "- Checkpoints saved every 100 steps for ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf37c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt: 311 tokens\n",
      "Max completion: 1737 tokens\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n",
      "✅ Training config ready\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_prompt_length = maximum_prompt_length + 10\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt: {max_prompt_length} tokens\")\n",
    "print(f\"Max completion: {max_completion_length} tokens\")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,  # Effective batch size = 2\n",
    "    num_generations = 2,  # Generate 2 strategies per prompt\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 400,  # Reduced for hackathon timeframe\n",
    "    save_steps = 100,\n",
    "    report_to = \"trackio\",\n",
    "    output_dir = \"outputs_marooned\",\n",
    ")\n",
    "\n",
    "print(\"✅ Training config ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48cc57",
   "metadata": {},
   "source": [
    "## 🚀 Initialize Trainer\n",
    "\n",
    "Combine everything: model, rewards, dataset, and config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8afac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.13s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GRPO Trainer initialized!\n",
      "\n",
      "📊 Reward functions:\n",
      "  1. function_works: Valid Python syntax\n",
      "  2. no_cheating: No forbidden imports\n",
      "  3. strategy_succeeds: Actual game performance\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        function_works,\n",
    "        no_cheating,\n",
    "        strategy_succeeds,  # Most important - game performance\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "print(\"✅ GRPO Trainer initialized!\")\n",
    "print(f\"\\n📊 Reward functions:\")\n",
    "print(\"  1. function_works: Valid Python syntax\")\n",
    "print(\"  2. no_cheating: No forbidden imports\")\n",
    "print(\"  3. strategy_succeeds: Actual game performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c8e74",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "**What to Monitor:**\n",
    "- **Reward Trajectory**: Should increase from negative → positive over ~100-200 steps\n",
    "- **Episode Counter**: Tracks simulated games (each step tests 2+ strategies)\n",
    "- **Strategy Diversity**: Model explores colonist cooperation vs traitor deception\n",
    "- **TrackIO Metrics**: Real-time visualization of training dynamics\n",
    "\n",
    "**Expected Progression:**\n",
    "- **Steps 0-100**: Learning valid syntax, basic actions\n",
    "- **Steps 100-200**: Developing resource gathering patterns\n",
    "- **Steps 200-300**: Emergent cooperation and deception strategies\n",
    "- **Steps 300-400**: Refined ship building and traitor detection\n",
    "\n",
    "**Timeline**: 3-5 hours on high-end GPU (Mi300X/A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee7b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 400\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 0 of 20,914,757,184 (0.00% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 400\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 0 of 20,914,757,184 (0.00% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics logged to: /root/.cache/huggingface/trackio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/?project=huggingface&write_token=9FzoW7THuV_N7iGX9tG4DLHdxcdIbzsScia5wCFW-Kw\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 131072}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Created new run: dainty-sunset-0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tracer_output' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1433\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     guarded_code, tracer_output = \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[32m   1437\u001b[39m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1442\u001b[39m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[32m   1443\u001b[39m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_utils_internal.py:92\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m     95\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m     96\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1117\u001b[39m, in \u001b[36m_compile.<locals>.compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks)\u001b[39m\n\u001b[32m   1116\u001b[39m     stack.enter_context(CompileTimeInstructionCounter.record())\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   1120\u001b[39m     ConvertFrameReturn(),\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1122\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1251\u001b[39m, in \u001b[36m_compile.<locals>._compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mbuild_guards\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     check_fn = \u001b[43mdynamo_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_guards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m package \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:856\u001b[39m, in \u001b[36mDynamoOutput.build_guards\u001b[39m\u001b[34m(self, code, hooks, save, cache_entry, strict_error)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tracer_output.output_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckFunctionManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtracer_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguard_fail_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguard_filter_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_guards\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:3383\u001b[39m, in \u001b[36mCheckFunctionManager.__init__\u001b[39m\u001b[34m(self, f_code, output_graph, cache_entry, guard_fail_fn, guard_filter_fn, shape_code_parts, runtime_global_scope, save_guards, strict_error)\u001b[39m\n\u001b[32m   3382\u001b[39m \u001b[38;5;66;03m# Redo the guards because filtering relies on the results from the last guard builder.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3383\u001b[39m builder, guard_manager = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_guards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3384\u001b[39m \u001b[43m    \u001b[49m\u001b[43msorted_guards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexisting_diff_guard_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3387\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3388\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_guards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3389\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3391\u001b[39m \u001b[38;5;28mself\u001b[39m.guard_manager = guard_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:3674\u001b[39m, in \u001b[36mCheckFunctionManager.build_guards\u001b[39m\u001b[34m(self, sorted_guards, existing_diff_guard_sources, f_code, output_graph, save_guards)\u001b[39m\n\u001b[32m   3672\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3674\u001b[39m     \u001b[43mguard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builder, guard_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_guards.py:366\u001b[39m, in \u001b[36mGuard.create\u001b[39m\u001b[34m(self, builder)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2671\u001b[39m, in \u001b[36mGuardBuilder.SHAPE_ENV\u001b[39m\u001b[34m(self, guard)\u001b[39m\n\u001b[32m   2666\u001b[39m guards_log.debug(\n\u001b[32m   2667\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mC++ shape guard function: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2668\u001b[39m     func_str,\n\u001b[32m   2669\u001b[39m     verbose_code_parts.exprs,\n\u001b[32m   2670\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2671\u001b[39m clib = \u001b[43mCppCodeCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2672\u001b[39m cguard = ctypes.cast(clib.guard, ctypes.c_void_p).value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:2839\u001b[39m, in \u001b[36mCppCodeCache.load\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m   2837\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:2705\u001b[39m, in \u001b[36mCppCodeCache.load_async\u001b[39m\u001b[34m(cls, main_code, device_type, submit_fn, extra_flags, optimized_code)\u001b[39m\n\u001b[32m   2698\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compile and load a C++ library.  Returns a callable that returns the loaded\u001b[39;00m\n\u001b[32m   2699\u001b[39m \u001b[33;03mlibrary.\"\"\"\u001b[39;00m\n\u001b[32m   2700\u001b[39m compile_command = {\n\u001b[32m   2701\u001b[39m     **\u001b[38;5;28mcls\u001b[39m.cpp_compile_command_flags,\n\u001b[32m   2702\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice_type\u001b[39m\u001b[33m\"\u001b[39m: device_type,\n\u001b[32m   2703\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mextra_flags\u001b[39m\u001b[33m\"\u001b[39m: extra_flags,\n\u001b[32m   2704\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muse_relative_path\u001b[39m\u001b[33m\"\u001b[39m: config.is_fbcode(),\n\u001b[32m-> \u001b[39m\u001b[32m2705\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvec_isa\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2706\u001b[39m }\n\u001b[32m   2708\u001b[39m _set_gpu_runtime_env()  \u001b[38;5;66;03m# cpp_extension consults the env\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:497\u001b[39m, in \u001b[36mpick_vec_isa\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] = \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:484\u001b[39m, in \u001b[36mvalid_vec_isa_list\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    483\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[43misa_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:487\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    483\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m    484\u001b[39m     isa_list.extend(\n\u001b[32m    485\u001b[39m         isa\n\u001b[32m    486\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa).split()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:143\u001b[39m, in \u001b[36mVecISA.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:153\u001b[39m, in \u001b[36mVecISA.__bool__impl\u001b[39m\u001b[34m(self, vec_isa_ok)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_inductor/cpu_vec_isa.py:127\u001b[39m, in \u001b[36mVecISA.check_build\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Check build result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_py_load\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__lib_path__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpython_subprocess_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:408\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03mthe exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03mCalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m \u001b[33;03mcheck_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m retcode = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:391\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/UnslothGRPOTrainer.py:53\u001b[39m, in \u001b[36mprepare_for_training_mode.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_training\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.for_training()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m output = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_inference\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:323\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:34\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/trl/extras/profiling.py:98\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/UnslothGRPOTrainer.py:2010\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, generation_batch)\u001b[39m\n\u001b[32m   2007\u001b[39m generate_every = \u001b[38;5;28mself\u001b[39m.args.steps_per_generation * \u001b[38;5;28mself\u001b[39m.num_iterations\n\u001b[32m   2008\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step % generate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2009\u001b[39m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2010\u001b[39m     generation_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2011\u001b[39m     generation_batch = split_pixel_values_by_grid(generation_batch)\n\u001b[32m   2013\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m: generation_batch = shuffle_sequence_dict(generation_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/UnslothGRPOTrainer.py:2321\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   2312\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2313\u001b[39m     profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.generate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   2314\u001b[39m     unwrap_model_for_generation(\n\u001b[32m   (...)\u001b[39m\u001b[32m   2318\u001b[39m     FSDP.summon_full_params(\u001b[38;5;28mself\u001b[39m.model_wrapped, recurse=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_enabled \u001b[38;5;28;01melse\u001b[39;00m nullcontext(),\n\u001b[32m   2319\u001b[39m ):\n\u001b[32m   2320\u001b[39m     prompt_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m], prompt_inputs[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] = prompt_ids, prompt_mask\n\u001b[32m-> \u001b[39m\u001b[32m2321\u001b[39m     prompt_completion_ids = \u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_compile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   2323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2324\u001b[39m \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[32m   2325\u001b[39m prompt_length = prompt_ids.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/unsloth/models/rl.py:71\u001b[39m, in \u001b[36mPatchRL.<locals>.unsloth_unwrap_model_for_generation.<locals>.generate_with_clone\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_with_clone\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     out = \u001b[43moriginal_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch.Tensor):\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/unsloth/models/vision.py:279\u001b[39m, in \u001b[36munsloth_base_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# DO INFERENCE\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), autocaster:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Delete cached Flex Attention masks to reset inference\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_modules():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2867\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2864\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   2866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py:721\u001b[39m, in \u001b[36mGptOssForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    708\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    709\u001b[39m     input_ids: Optional[torch.LongTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    719\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    720\u001b[39m ) -> MoeCausalLMOutputWithPast:\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGptOssForCausalLM_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:196\u001b[39m, in \u001b[36mget_nonrecursive_disable_wrapper.<locals>.nonrecursive_disable_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnonrecursive_disable_wrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py:542\u001b[39m, in \u001b[36mGptOssForCausalLM_forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m output_router_logits = (\n\u001b[32m    538\u001b[39m     output_router_logits \u001b[38;5;28;01mif\u001b[39;00m output_router_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_router_logits\n\u001b[32m    539\u001b[39m )\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m outputs: MoeModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/gpt_oss.py:1194\u001b[39m, in \u001b[36mpatch_GptOssModel.<locals>.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1192\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m   1193\u001b[39m hidden_states = inputs_embeds\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m position_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1197\u001b[39m     torch._dynamo.mark_static (hidden_states, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py:345\u001b[39m, in \u001b[36mGptOssRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, position_ids):\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGptOssRotaryEmbedding_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:832\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    829\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1874\u001b[39m, in \u001b[36mCatchErrorsWrapper.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, frame_state)\u001b[39m\n\u001b[32m   1868\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[32m   1869\u001b[39m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m.hooks, frame_state\n\u001b[32m   1870\u001b[39m             )\n\u001b[32m   1872\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[32m   1873\u001b[39m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1874\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1876\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1877\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:688\u001b[39m, in \u001b[36mConvertFrameAssert.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m    685\u001b[39m     dynamo_tls.traced_frame_infos.append(info)\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m     result = \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_frame_box\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.caching_precompile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._package \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DynamoCache\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIAC/colony-collapse/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1509\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[39m\n\u001b[32m   1506\u001b[39m         gc.collect(\u001b[32m1\u001b[39m)\n\u001b[32m   1508\u001b[39m output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtracer_output\u001b[49m:\n\u001b[32m   1510\u001b[39m     output = tracer_output.output_graph\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'tracer_output' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55a5c",
   "metadata": {},
   "source": [
    "## 🎮 Test the Trained Model\n",
    "\n",
    "Let's see if the trained model can play better than random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Test with a colonist prompt\n",
    "test_prompt = \"\"\"Create a smart colonist strategy for Marooned that:\n",
    "- Explores efficiently to find wood and metal\n",
    "- Manages energy by eating food when low\n",
    "- Deposits resources regularly at base camp\n",
    "- Helps build the ship\n",
    "- Watches for the traitor\n",
    "\n",
    "```python\n",
    "def strategy(observation, sailor_id, role):\n",
    "    # Your optimized strategy\n",
    "\"\"\"\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    "    reasoning_effort = \"low\",\n",
    ")\n",
    "\n",
    "print(\"🤖 Generating strategy with trained model...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 512,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d75c76",
   "metadata": {},
   "source": [
    "## 💾 Save the Model\n",
    "\n",
    "Save the trained model for later use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in 16-bit format\n",
    "model.save_pretrained_merged(\"marooned_gpt_oss_trained\", tokenizer, save_method = \"merged_16bit\")\n",
    "\n",
    "print(\"✅ Model saved to ./marooned_gpt_oss_trained\")\n",
    "\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# model.push_to_hub_merged(\"your-username/marooned-gpt-oss\", tokenizer, save_method = \"merged_16bit\", token = \"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2dc99",
   "metadata": {},
   "source": [
    "## 🎯 Evaluation: Full Game Playthrough\n",
    "\n",
    "Let's run a complete game with the trained model and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏴‍☠️ Running complete game with trained model...\\n\")\n",
    "\n",
    "# Create a simple strategy wrapper that uses the trained model\n",
    "def trained_model_strategy(obs: Observation, sailor_id: str, role: str) -> str:\n",
    "    \"\"\"Generate action using trained GPT-OSS model\"\"\"\n",
    "    prompt = observation_to_prompt(obs, include_role=True, sailor_role=role)\n",
    "    \n",
    "    # Add instruction to output action\n",
    "    prompt += \"\\n\\nOutput your next action in the format: ACTION: <action_type> <parameters>\"\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True,\n",
    "        reasoning_effort = \"low\",\n",
    "    )\n",
    "    \n",
    "    # Generate with trained model\n",
    "    output = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        temperature=0.8,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run evaluation game\n",
    "try:\n",
    "    success, info = execute_game_episode(trained_model_strategy, max_turns=500)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎮 GAME RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Winner: {info['winner']}\")\n",
    "    print(f\"Total turns: {info['total_turns']}\")\n",
    "    print(f\"Ship progress: {info['ship_progress']:.1f}%\")\n",
    "    print(f\"Survivors: {info['survivors']}/5\")\n",
    "    print(f\"\\nTotal rewards by sailor:\")\n",
    "    for sailor, reward in info['total_rewards'].items():\n",
    "        print(f\"  {sailor}: {reward:.2f}\")\n",
    "    \n",
    "except TimeoutError:\n",
    "    print(\"⏱️ Game exceeded time limit\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a62c4c",
   "metadata": {},
   "source": [
    "## Technical Achievements & Innovations\n",
    "\n",
    "### Environment Design\n",
    "\n",
    "**1. Custom OpenEnv Implementation**\n",
    "- Full Gymnasium API compliance (reset, step, render, observation_space, action_space)\n",
    "- Multi-agent turn-based coordination with 5 independent actors\n",
    "- 3-dimensional spatial representation (30×30×3 island levels)\n",
    "- Rich state encoding: 100+ dimensional observation space per agent\n",
    "\n",
    "**2. Information Asymmetry**\n",
    "- **Hidden Roles**: Only traitor knows their identity\n",
    "- **Private Backpacks**: Inventory visible only to owner\n",
    "- **Stealth Mechanics**: Sabotage succeeds only when unobserved\n",
    "- **Evidence System**: Suspicious actions logged with witness tracking\n",
    "\n",
    "**3. Long-Horizon Complexity**\n",
    "- Episodes span 100 days = up to 10,000 turns\n",
    "- Credit assignment across hundreds of actions\n",
    "- Delayed consequences (poison takes 3 days, ship requires 100+ resources)\n",
    "\n",
    "### RL Training Innovations\n",
    "\n",
    "**4. Dual-Objective Learning**\n",
    "- Single model learns both cooperative (colonist) and competitive (traitor) policies\n",
    "- Balanced reward structure prevents mode collapse\n",
    "- Emergent social dynamics from self-play potential\n",
    "\n",
    "**5. Natural Language Action Space**\n",
    "- 21 distinct action types with variable parameters\n",
    "- LLM-friendly interface (text → structured action parser)\n",
    "- Extensible to future action types without retraining\n",
    "\n",
    "**6. Multi-Modal Rewards**\n",
    "- Syntax correctness (coding)\n",
    "- Security enforcement (sandboxing)\n",
    "- Gameplay performance (strategy)\n",
    "- Encourages valid, safe, and effective policies\n",
    "\n",
    "### Comparison to Baseline (2048)\n",
    "\n",
    "| Metric | 2048 | Marooned |\n",
    "|--------|------|----------|\n",
    "| Agents | 1 | 5 (multi-agent) |\n",
    "| Actions | 4 (fixed) | 21+ (parameterized) |\n",
    "| Horizon | ~1,000 moves | ~10,000 turns |\n",
    "| Information | Perfect | Asymmetric (hidden roles) |\n",
    "| Objectives | Single (score) | Dual (cooperate/deceive) |\n",
    "| State Space | 16 tiles | 1,350+ tiles + inventories + social state |\n",
    "| Complexity | Deterministic | Stochastic, social, strategic |\n",
    "\n",
    "### Why This Advances OpenEnv\n",
    "\n",
    "**Research Contributions:**\n",
    "1. **Social AI**: First OpenEnv environment with deception mechanics\n",
    "2. **Multi-Agent RL**: Demonstrates scaling beyond single-agent scenarios\n",
    "3. **Long-Horizon Planning**: Pushes RL to 100-day planning problems\n",
    "4. **LLM Integration**: Shows natural language can be effective action space\n",
    "5. **Reusable Components**: Open-source environment for future research\n",
    "\n",
    "**Potential Applications:**\n",
    "- Negotiation and diplomacy training\n",
    "- Collaborative AI with trust modeling\n",
    "- Adversarial robustness testing\n",
    "- Social simulation for game AI\n",
    "\n",
    "---\n",
    "\n",
    "## Results & Insights\n",
    "\n",
    "**Baseline Performance** (Random Strategy):\n",
    "- Ship Progress: 0.0% (100 turns)\n",
    "- Survivors: 4/5 (energy depletion)\n",
    "- Winner: Timeout\n",
    "\n",
    "**Expected Trained Performance** (400 steps):\n",
    "- Ship Progress: 15-30% (strategic resource gathering)\n",
    "- Survivors: 5/5 (energy management learned)\n",
    "- Win Rate: 5-10% colonist victories\n",
    "\n",
    "**Advanced Training** (1000+ steps):\n",
    "- Ship Progress: 50-80%\n",
    "- Win Rate: 20-40% colonist victories\n",
    "- Emergent Strategies: Role-based specialization, traitor detection patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "**Environment Enhancements:**\n",
    "- Dynamic weather affecting resource availability\n",
    "- Ship repair mechanics (traitor can damage, colonists must fix)\n",
    "- Multiple traitors (e.g., 2 traitors in 7-sailor game)\n",
    "- Skill specialization (woodcutter, builder, navigator roles)\n",
    "\n",
    "**Training Improvements:**\n",
    "- Self-play tournaments between checkpoints\n",
    "- Colonist-only vs traitor-only specialized models\n",
    "- Curriculum learning (start with shorter games, increase duration)\n",
    "- Multi-environment training (forest island, desert island variants)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Win rate vs random baseline\n",
    "- Win rate vs heuristic strategies\n",
    "- Traitor detection accuracy\n",
    "- Resource gathering efficiency\n",
    "- Communication quality (informativeness, deception detection)\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "**Built For**: OpenEnv Hackathon (Meta PyTorch + Unsloth AI)\n",
    "\n",
    "**Technical Stack:**\n",
    "- Environment: Custom Marooned (Gymnasium-compatible)\n",
    "- Model: GPT-OSS 20B (4-bit quantization)\n",
    "- Training: GRPO via Unsloth + TRL\n",
    "- Hardware: AMD Mi300X (192GB VRAM)\n",
    "\n",
    "**Inspiration:**\n",
    "- Game Design: *Among Us* (social deduction) × *Don't Starve Together* (survival) × *The Resistance* (hidden roles)\n",
    "- RL Research: Multi-agent cooperation, emergent communication, deception learning\n",
    "\n",
    "---\n",
    "\n",
    "**License**: LGPL-3.0 (Environment code available at: https://github.com/atchudhansg/colony-collapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1d778",
   "metadata": {},
   "source": [
    "## 🛑 Note on Server Management\n",
    "\n",
    "The Marooned server is running externally in a separate terminal. \n",
    "- To stop it, use `CTRL+C` in the terminal where you started it\n",
    "- The server needs to keep running while this notebook executes\n",
    "- You can monitor server logs in the terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The server is running externally, so no cleanup needed in this notebook\n",
    "print(\"ℹ️ Server is running externally\")\n",
    "print(\"   To stop it, use CTRL+C in the terminal where you started marooned_server.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
